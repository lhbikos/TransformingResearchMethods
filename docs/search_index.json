[["index.html", "Transforming Research Methods in Health Services Psychology: Applications for the Advocate ~ Practitioner ~ Scientist BOOK COVER", " Transforming Research Methods in Health Services Psychology: Applications for the Advocate ~ Practitioner ~ Scientist Lynette H. Bikos, PhD, ABPP &amp; Cirleen DeBlaere, PhD, Editors Kiana Clay, Editorial Assistant 20 Jan 2025 BOOK COVER An image of the book cover. It includes three overlapping, pastel-colored, circles representing advocacy, practice, and science. These are surrounded by a handful of statistical symbols and formulae. methods. This open education resource is available in the following formats, all available in the docs folder at the GitHub repository: Formatted as an html book via GitHub Pages available As a PDF As an ebook As a Word Document All materials used in creating this OER are available at its GitHub repo. "],["preface.html", "PREFACE Strategies for a Social Responsivity Perpetually in Progress Under Construction Acknowledgements Copyright with Open Access", " PREFACE If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the Transforming Counseling Psychology Curriculum Showcase at APA 2022, for peer review, and for generating interest in collaboration. The document was last updated on 20 Jan 2025. In her 2021-2022 term as President of the Society of Counseling Psychology, one of Dr. Amy Reynolds’ Presidential Initiatives was Transforming Counseling Psychology Curriculum and Praxis. Dr. Reynolds invited counseling psychology faculty, practitioners, and doctoral students “to critically examine and deconstruct how various competencies, courses, and content are taught; how we socialize our students; and then re-imagine, dream, and reconstruct new and transformative ways to teach and train.” Strategies for a Social Responsivity This open education resource (OER) is a product of the group devoted to research. There are a number of strategies we used to ensure that the OER moves in the direction of being socially and culturally responsive. Our authors committed to using the guidelines for a liberated syllabus found in the CCTC: Social Responsiveness in Health Services Psychology Education &amp; Training Toolkit. We chose the format of OER because provides a zero-cost textbook to faculty and students. We sought authors and co-author teams that represent the diversity of health services psychology including discipline (counseling, clinical, educational),stage in career (students, early career professionals, mid- and late- career professionals), and identities that have been marginalized in higher education and our discipline. Each chapter works its way through an open peer review process where the chapter (with authors clearly identified)is hosted in a shared drive. At least two reviewers can mark up the same document and contribute to the same rubric. At any time the author(s) can see the review and, if desired, dialogue with the reviewers. At the outset, we specified the tone to be “formative not summative.” Although we are still learning, we have attempted to use tools and techniques that are consistent with universal design. For example, we hope that the image captions and headers are marked such that text readers will identify them as such. Perpetually in Progress This book is being formatted in R Markdown, rendered into its “book” format with Bookdown, hosted on GitHub, and pushed to the internet (in its html format) through GitHub Pages. This set of tools allows the book to be perpetually-in-progress. This means that our authors can update their chapters at-any-time. It also means that we can add chapters at-any-time. If you are interested in contributing to the book, please contact us. It is one of our greatest hopes that this flexibility contributes to the socially and culturally responsive pedagogy that we intend. Under Construction At this stage in the OER’s development, authors are still writing and revising chapters. The following designations will identify the chapters that have not been through the review process: In-progress means that the chapter is partially written (or perhaps outlined) and that the author(s) are continuing to work on the chapter. Under review means that the chapter is being (or has been) peer-reviewed. Acknowledgements Financial support, supporting the copy editing and desktop publishing for this project was provided by the Office of Education, Technology, &amp; Media, Seattle Pacific University (Summer 2022). The book cover was designed by Dominic Williamson, Senior Instructional Designer in Graphics &amp; Illustrations, in the Office of Education, Technology, &amp; Media at Seattle Pacific University. Copyright with Open Access This book is published under a a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA. A GitHub open-source repository contains all of the text and source code for the book, including data and images. "],["InExVal.html", "Chapter 1 Internal and External Validity in Health Service Psychology 1.1 Learning Objectives 1.2 Recommended Readings 1.3 Internal Validity 1.4 External Validity 1.5 Current Issues, Trends, and Considerations 1.6 A Consideration for Practice 1.7 Activity 1.8 References", " Chapter 1 Internal and External Validity in Health Service Psychology Franco Dispenza, PhD (he/him/his) &amp; Alec Prince (he/him), MPA Georgia State University Georgia State University is located on the traditional homelands of the Muscogee Creek and Cherokee Nations The focus of this lesson is to provide a review of internal validity, external validity, threats to validity, and current trends and considerations in relation to validity. Internal and external validity are foundational to experimental and quasi-experimental research. In experimental and quasi-experimental research designs, health service psychologists work thoughtfully and diligently to ensure that a line of systematic inquiry demonstrates some degree of internal and external validity. This is because psychologists and behavioral health researchers are concerned with making reasonable epistemological claims that could directly impact the lives of diverse communities and populations, especially in research studies attempting to show if particular interventions, treatments, or programs have a true effect on specific outcomes. Whereas researchers utilizing qualitative frameworks may be more interested in methodological integrity (e.g., credibility and transferability; Levitt et al., 2017), researchers employing quantitative-based paradigms are especially interested in internal and external validity. You will notice that the term “validity” is used in many concepts and research frameworks, including construct validity, content validity, predictive validity, criterion validity, and statistical conclusion validity. These are all specific types of validity attempting to establish a degree of accuracy or truthfulness in research. Further, the aforementioned forms of validity often have statistical computations and procedures that accompany them (e.g., bivariate correlations, beta weights, etc.). This chapter will not address those forms of validity. Instead, we will focus on internal and external validity, conceptual constructs that rely on methodological procedures and considerations versus statistical calculations. 1.1 Learning Objectives Learning objectives for this chapter include the following: Explain the dimensional features of internal and external validity in experimental and quasi-experimental research. List and discuss threats associated with internal and external validity in experimental and quasi-experimental research. Discuss and apply established methods for controlling the various threats associated with internal and external validity in experimental and quasi-experimental research. Identify and apply their knowledge of internal validity, external validity, and their associated threats to current trends and issues in psychological research. Critique and distinguish the strengths and limitations of internal and external validity when applied to socially responsive research. 1.2 Recommended Readings The following served as critical references in the development of this chapter. We encourage you to review them. Campbell, D. T., &amp; Stanley, J. C. (1963). Experimental and quasi- experimental designs for research. Chicago, IL: Rand McNally. This classic and foundational text introduces readers to internal validity, external validity, and threats to validity as originally proposed by Campbell and Stanley. It also provides readers with an overview of the various experimental and quasi-experimental methodological designs, and how various methodological designs could be used to minimize threats to validity. Ferguson L. (2004). External validity, generalizability, and knowledge utilization. Journal of Nursing Scholarship, 36(1), 16-22. https://doi.org/10.1111/j.1547-5069.2004.04006.x Although aimed for a nursing audience, there is much to be gained from Ferguson’s (2004) review of external validity, generalizability, and evidence-based practice. Ferguson reviews much of the major conceptual tenets of external validity, including its threats and control strategies. Ferguson also discusses ways in which researchers and practitioners could enhance external validity of research. Schmuckler, M.A. (2001).What Is Ecological Validity? A Dimensional Analysis. Infancy, 2(4), 419-436. https://doi.org/10.1207/S15327078IN0204_02 Schmuckler (2001) introduces readers to a subset of external validity, namely ecological validity. Schmuckler provides an historical review of ecological validity, and a discussion of the various dimensions, advantages, and criticisms of ecological validity. 1.3 Internal Validity Health service psychologists are committed to accuracy, honesty, and truthfulness when engaging in the research process. Whether representing research findings fairly, capturing the essence of people’s lived experiences with precision, or using research to advocate against harmful psychological practices, researchers are compelled to uphold the integrity of the knowledge claims they make through their scholarship. Researchers are committed to ensuring that their research is justifiable, well-grounded, and internally valid. Internal validity is the extent to which a researcher can infer cause-effect relationships between a set of variables, while simultaneously excluding the influence of confounding or extraneous variables (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979). In an attempt to establish internal validity, it is important to rule out the effects of confounding or extraneous variables. Confounding or extraneous variables can serve as potential rivals to inferred relationships, leading a researcher to be less confident about the conclusions and implications that can be made between an independent (or treatment) variable and a dependent (or outcome) variable. Methodological control is also paramount in experimental and quasi-experimental research. In an ideal research scenario, a researcher will have identified and controlled for all possible confounds and extraneous variables that may be inherent in the methodological design of a study. Since determining causality is the goal in experimental and quasi-experimental research, such a scenario would render a study to be high in internal validity. However, we know in practice there is no such thing as a perfect study, especially when there exist numerous threats to internal validity. Threats consist of factors or conditions that endanger a researcher’s capacity to amplify a study’s level of internal validity (Onwuegbuzie &amp; McLean, 2003). 1.3.1 Threats to Internal Validity Researchers often execute considerable forethought when identifying and eliminating potential threats to internal validity. Psychological researchers are encouraged to use a variety of logic-informed heuristics, “what if” sensitivity analyses, or consider any new data or findings to rule out rival hypotheses (Kenny, 2019). Researchers are also encouraged to consider more classically identified threats to their research design (Campbell &amp; Stanley, 1963). Some of the most common threats to internal validity consist of history, maturation, instrumentation, statistical regression, selection, attrition, and experimenter bias (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979; Shadish et al., 2002). Threats to internal validity can occur, or be discovered, at any point during the research process, including design, implementation, data collection, and analyses. There can exist multiple threats in a given study, and these threats can also intersect with one another. Keep in mind that threats are never something a researcher just “checks off” in a box, but rather a researcher continuously monitors their methods to ensure the credibility of the concluded findings (Onwuegbuzie &amp; McLean, 2003). Each threat to internal validity is discussed in more detail below. 1.3.1.1 History No researcher can control the potential of geological, sociopolitical, financial, cultural, climate-related, pandemic, and other national and global events from impacting individuals in a research study. But inevitably such events happen, and unfortunately with what seems to be at higher frequencies these days. Historical events can influence the manner in which participants respond to a study’s dependent variable, making it difficult for researchers to determine whether an observed outcome was the result of a study’s independent variable or the historical event. In some instances, the historical event could directly intersect with objectives of the study itself making it even more difficult for researchers to make valid conclusions. Imagine you are evaluating the effectiveness of a new stress-reduction intervention for survivors of catastrophic hurricanes in a low-income, rural community in the southeast United States. In the middle of implementing the intervention, a tornado hits the same community, leading to severe damage and loss of life. What impact will this historical event have on the research study? 1.3.1.2 Maturation Development and growth are natural processes that occur for individuals during their lifespan. These processes could be physiological, cognitive, social, or emotional. Given the length of a particular study, sometimes results from a study may be more indicative of naturally occurring growth and developmental factors, versus any manipulated independent variable. 1.3.1.3 Instrumentation No tool, test, or measure used in research can be entirely valid and reliable one hundred percent of the time. Instruments can produce low reliability scores or even produce inadequate psychometric properties (e.g., construct and predictive validity) with a study’s particular sample. This is an important consideration as some tests and measures were never validated or normed with diverse samples. Consider a measure of marital satisfaction that was created using heterosexual couples in the Netherlands. The measure may have great face validity, and even demonstrate adequate levels of content, criterion, and construct validity. But it may not produce the same psychometrics if used in married couples in the United States. It may be entirely inappropriate to use this measure if the United States sample also includes same-sex couples. Lastly, human beings administering instruments could contribute additional error, making instrumentation a serious threat to research studies. For example, some researchers may become fatigued when administering some measures, may not be consistent with measurement, or may not accurately observe a phenomenon during a research study. 1.3.1.4 Testing It is common to test participants multiple times throughout the course of a research study. Researchers often use the same test, measure, scale, or inventory when testing participants, but it begs the question as to whether changes in a test—that had been given multiple times—reflect true change? Sometimes referred to either as a practice effect or fatigue effect, changes in scores on the same test may be the result of familiarity with the test or becoming tired with the test. 1.3.1.5 Statistical Regression, or Regression to the Mean Sometimes researchers select participants based on high or low scores on a particular test or measure (Campbell &amp; Kenny, 1999). For example, researchers may be interested in examining people who score high on measures of academic or cognitive functioning. If retested, those same participants may continue to score high or low. However, not all would score as high or low because of statistical regression, or regression to the mean. When participants are retested, researchers find that scores are less extreme and center toward an average score. Imagine a researcher was interested in testing a career counseling intervention with first generation college students who reported high scores on career indecision and anxiety in a career battery of questionnaires. After the intervention is complete, the researchers issue a final battery of questionnaires. These same first generation college students may appear less anxious and indecisive due to statistical regression to the mean and not necessarily the intervention. 1.3.1.6 Attrition, or Differential Mortality Ethically, participants have the right to withdraw from participating in a research study at any given time during the research process. When participants withdraw, or drop from a study, researchers refer to this as attrition. A major concern of attrition is that it leads to potential biases in scores between groups (or observations in the case of longitudinal studies) that may not reflect whether an independent variable had any effect on the dependent variable. If there is substantial attrition in one group, or across the entire study, it leads a researcher to question why participants are withdrawing. Researchers may further wonder if participants dropping from the study are characteristically different from those who are remaining in the study. Attrition limits the type of conclusions that can be made because observations made across time or between groups may not reflect true differences as a function of the independent variable. Rather, there may be some other underlying issue with the research study. Imagine a researcher has been evaluating the effects of a six-session mindfulness cognitive behavioral group intervention for the reduction of race related stress among Black and African American women employed in a large healthcare setting. The researcher used a longitudinal, between group experimental design with an intervention group (treatment) and a control group (education), but found that attrition rates were higher for those in the control than for the intervention group. Equivalent and adequate comparisons could not be reliably made between the two groups at completion of study, so the researcher decided to send surveys to those dropped from the study and inquire why they dropped. The surveys return and the researcher finds that a significant portion of the control group were made up of on-call, ambulatory nurses who were unable to commit to the scheduled sessions. Therefore, the attrition was the result of some characteristic that differentiated the two groups. 1.3.1.7 Experimenter Bias Researchers need to ensure they do not engage in verbal or nonverbal behaviors that inadvertently alter the results of a study. Sometimes this is referred to as an experimenter expectancy, and it is even known as the Rosenthal Effect. This becomes a threat when a participant’s response in a study is the result of the experimenter’s expectations versus the manipulated or independent variable. Examples include emphasizing particular words when reading prompts or scripts, or excessive nodding and smiling when certain favorable responses are solicited by study participants. 1.3.1.8 Selection Bias Differences between groups in experimental studies can sometimes be the result of characteristics of the participants themselves, versus the manipulated or independent variable. For instance, a researcher may have accidentally grouped people along the same characteristic, such as sex, gender, sexual orientation, race, ethnicity, or age group. This potentially sets up nonequivalent groups in experimental or quasi-experimental research, and makes it difficult for researchers to determine whether any changes in dependent or outcome variables were the result of an independent variable or the characteristic itself. This also pertains to self-selection bias commonly seen in survey and questionnaire research, in which participants self-select to participate in a study. 1.3.2 Controlling for Threats to Internal Validity Researchers have identified a number of methodological procedures that could be used to control for threats to internal validity (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979; Fabrigar et al., 2020; Onwuegbuzie &amp; McLean, 2003; Shadish et al., 2002). Below we discuss the importance of considering control groups, random assignment, matching, blocking and holding variables constant. 1.3.2.1 Control Groups Control groups are commonly used in experimental and quasi-experimental human subjects research, and there is an important logic to its use. Individuals are assigned to a group (or condition) in which they do not receive the treatment or manipulated variable. However, participants do partake in similar tasks and conditions (e.g., complete surveys, questionnaires, physiological markers, etc.) as those in the experimental condition. Upon completion of the study, researchers then compare how the intervention or experimental condition performed alongside the control condition or group. Control groups are particularly effective at controlling for the effects of history, maturation, testing, instrumentation, and regression toward the mean (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979). 1.3.2.2 Random Assignment Considered one of the most robust methods to control against threats to internal validity, researchers randomly place volunteer participants in various study conditions at the very beginning of a research study. This assures the researcher that each participant had the same or equally probable chance of being placed in either an experimental condition (or group) or a control condition (or group), while helping to decrease any unknown or intentional influence on assignment of participants to different groups. It further assures the researcher that the groups are equitable in terms of various characteristics of the participants (e.g., demographics, temperament, etc.; Fabrigar et al., 2020). Any observed differences or changes seen among the groups or conditions could then be accounted for by the manipulated independent variable or the applied intervention. More importantly, any observed difference between the groups is not the result of any sort of systematic bias that might have occurred during the initial phases of the research study. Take a hypothetical scenario in which a researcher is evaluating the ways in which implicit sexist messaging influences women’s responses to a cognitive motor task. Women are recruited from the community to participate in the study. One group receives the implicit sexist messaging while the other group does not. However, nearly all members in one group happened to be women between the ages of 21 and 29, while the majority of those in another group were women between the age of 43 and 56 This could constitute a systematic bias since there are generational differences between members in one group versus the other. Random assignment is incredibly important when controlling for the effects of selection (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979; Shadish et al., 2002). It is important to keep in mind that random assignment and random sampling are not synonymous with one another. Random sampling is when researchers utilize a variety of probable sampling techniques (e.g., simple, systematic, stratified, or cluster) to recruit participants who approximate the general population. It also means that all members of a given population have an equal chance of being recruited to participate. 1.3.2.3 Matching To further avoid the threat of selection bias, researchers may engage in the process of participant matching. This is especially helpful if a researcher cannot guarantee equivalent groups through randomization, or when sample size may be too small. Participants are matched on a variety of characteristics (e.g., cognitive or intelligence pre-test scores, gender, age, etc.), by placing members of similar characteristics in either an experimental/intervention condition or in the control condition. This helps a researcher establish some degree of equivalence between groups within a particular sample. 1.3.2.4 Blocking and Holding Variables Constant Researchers may also choose to use some characteristic in the study’s sample (e.g., cognitive or intelligence scores, ethnicity or race) as an additional independent variable. This is referred to as blocking. Unlike matching, researchers may employ this strategy to see if a particular characteristic of the sample has an effect on the dependent variable. Alternatively, some researchers may choose to hold a particular characteristic constant or homogenize some sample characteristic so as to eliminate any undue influence from extraneous characteristics of a sample. For instance, a researcher may choose to only recruit high school aged adolescent boys between the ages of 14 and 15 who score above a certain threshold on an anxiety and depression measure to participate in a short-term emotional regulation treatment study. 1.4 External Validity Many researchers invest time and resources with the hopes of expanding their research findings to larger communities and contexts, especially if the research is aimed at alleviating any suffering or influence larger systemic change efforts. Thus, researchers are not only concerned with internal validity, but also the external validity of their study. External validity is the degree towhich research findings can be generalized to the population that approximate the original context of the study (Campbell &amp; Stanley, 1963; Cook &amp; Campbell, 1979; Shadish et al., 2002). External validity is also concerned with the degree in which a study can be generalized across broader populations, treatments, settings, and conditions (Ferguson, 2004). Researchers wish to move beyond the controlled setting in which the study originally took place, and further consider ways that the findings may apply in other diverse applicational settings, time, persons, or slightly different variables or targets (Kenny, 2019; Shadish et al., 2002). For instance, a researcher who evaluated the effectiveness of a minority stress reduction intervention for transgender and gender nonbinary individuals in a controlled laboratory setting at a university, might have interest in seeing the intervention applied or replicated with transgender and gender nonbinary individuals in community based clinics, private practices, college counseling centers, and other agencies across the United States. That same researcher may have further interest in having their research used to inform policy on affirming psychological care for transgender and gender nonbinary individuals for all mental health practitioners. Researchers always have the hope that the finding of their research will have some degree of relevance and importance in “real-world” settings, particularly if study findings are replicated in other contexts. Replicated studies that support original study findings are considered to demonstrate high levels of external validity, as well as other forms of validity (e.g., internal validity, construct validity; Fabrigar et al., 2020). As a variation, or subcategory of external validity, ecological validity is concerned with whether a study’s results can be applied to naturalistic or representative settings in every-day life (Andrade, 2018; Schmuckler, 2001). With this in mind, it is important to consider that there also exist threats that could interfere with a researcher’s confidence in the external validity of their study’s findings. 1.4.1 Threats to External Validity Campbell and Stanley (1963) identified four particular threats, including reactive or interaction effects of testing, interaction effects of selection bias, reactive effects of arrangement, and multiple treatment interference. 1.4.1.1 Reactive or Interaction Effect of Testing Sometimes researchers need to be cognizant that research studies, experiments, and testing procedures—in and of themselves—may be the catalyst producing some of the findings we see from research. In many “real-world” conditions, people are not tested or observed as much as they are in research. In particular, Campbell and Stanley (1963) discussed how exposure to a pre-test condition, or multiple testing conditions, may influence a study participant’s degree of sensitivity to the experimental variable. Consider an example in which a researcher is interested in examining a clinical supervisor’s attitudes toward racial and ethnic microaggressions in counseling. The clinical supervisor is asked to view a fictitious counseling session of a supervisee, and then asked to identify any subtle instances of discrimination from a 10 minute clip of a counseling session. Afterwards, the researcher follows up with another post-test to see if there have been any changes in attitudes toward racial and ethnic microaggressions in counseling. The potential threat to external validity in this example is that the clinical supervisors in the study have been sensitized by the pre-test condition (i.e., the fictitious counseling session), increasing their potential chances of identifying microaggressions in a counseling session. If generalized out, clinical supervisors may not respond the same way since they’ve not been pre-tested and sensitized. 1.4.1.2 Interaction Effects of Selection Bias Health service psychologists pay close attention to samples, and work diligently to recruit adequate samples to participate. However, there are situations in which a particular sample in a research study would not generalize to the entire population. This could be the result of selection bias. For instance, many university researchers utilize an undergraduate psychology research pool to recruit participants for their research. But undergraduate students represent a biased sample, and do not reflect the larger population in terms of representative demographics. Thus, researchers replicating a study with a different sample may not obtain the same findings. 1.4.1.3 Reactive Effects of Experimental Arrangements Research conducted in highly controlled settings (e.g., sterile laboratories) run the risk of not generalizing well in “real world” diverse settings or populations. This is mainly to do with the fact that research participants are willing volunteers who understand they are fully participating in experimental or study-related activities. In some instances, research participants may respond or behave a certain way because they are being observed. Frey (2018) reports that a participant may even have the desire to please a researcher by altering their performance on a particular outcome. This may sound familiar because you may understand this to be the Hawthorne effect, a phenomenon in which human beings change their behavior as a result of being observed. 1.4.1.4 Multiple Treatment Interference Depending on the sequencing of a particular study, researchers may provide the same subject different treatments or interventions at different intervals. For example, a researcher may be testing multiple formats to examine the combination of psychotropic medication along with some type of psychotherapy. However, this makes it difficult for researchers to determine if the sequencing of the differing treatments played any role in any of the observed outcomes. Because of this type of sequencing, we would argue that there has been some level of treatment contamination, because it is difficult to control effects from previous treatments or studies. 1.4.2 Controlling for Threats to External Validity It is important to note that external validity can never be assured, even when a researcher stringently addresses and controls for threats to internal validity (Ferguson, 2004). However, there are some threats to external validity that can be managed through some methodological considerations. Below we discuss only a few, including random selection, concealed research, as well as counterbalancing and strategies to control for pre-testing effects. #### Random Selection Sometimes confused with random assignment (already discussed above as a means of controlling threats to internal validity), random selection is about accessing and including a representative sample of the target population in a research study. Random sampling procedures, such as simple or stratified sampling, play a significant role when it comes to random selection. You may recall that random sampling is concerned with the notion that every individual (or observation) has an equal probability of being selected for a study. The equal probability of being selected then increases the probable chances that research findings can be generalized back to the target population (Ferguson, 2004). This form of control is particularly beneficial when considering the interaction effects of selection bias. 1.4.2.1 Single, Double, and Triple-Concealed Research It is important to preface that the term often used in research texts is single, double, and triple “blind” research. However, we believe “blind” is often misused in a variety of contexts, and in this context we believe it perpetuates ableist ideologies. And so, we offer a slight modification by using the term “concealed.” In order to reduce overt and covert forms of researcher bias in a study, researchers attempt to conceal as much as possible from participants and other members of a research team. In a single-concealed research study, only the researcher knows if participants are in a control or experimental group. Participants do not know what condition they are in. In a double-concealed research study, neither the researcher or participant know which is the control or experimental group. In a triple-concealed research study, consistent with the double-concealed design, neither the researcher nor participant know if they are in the control or experimental group. Additionally, those responsible for analyzing or examining outcomes do not know which set of variables were the control or experimental condition. This form of control is especially helpful when addressing reactive effects of experimental arrangements. 1.4.2.2 Counterbalancing and Controlling for Pre-Test Effects In order to address the effects of multiple treatment interference, or carryover effects, a researcher may consider the use of counterbalancing. A researcher must decide a priori all the possible sequences for a treatment, implement those varying permutations, and evaluate study participants in those different orders in order minimize carryover effects. If pre-testing effects is a concern, a researcher may decide not to include a pre-test at all, and compare groups at post-test only. Relatedly, a researcher may want to consider the use of the Solomon Four-Group Design as a means of countering the effects of a pre-test (Allen, 2017). In a Solomon Four-Group Design, a researcher will have four groups, in which some groups receive a pre-test and other groups do not. 1.5 Current Issues, Trends, and Considerations We will briefly review current issues, trends, and consideration of internal and external validity. It is vital that health service psychology researchers attend to matters of multiculturalism and diversity, since this has direct implications on the external and ecological validity of study findings. It is also incredibly important to consider evidence based practices with culturally diverse populations. Replication of research is another pressing trend in the field of psychology that requires researchers to consider how they present and disseminate their research findings to broader communities. Relatedly, Internet-based collection procedures require new concerted efforts in our conceptualization of internal control and generalizability of results. We end with a recommendation that researchers could consider when addressing internal and external validity of research. 1.5.1 Validity Issues Related to Cultural Diversity In a critique of psychological research, Sue (1999) wrote that researchers have overemphasized internal validity over external validity. As a result, the overemphasis on internal validity has hindered the development of research on ethnic and racial minority groups, as well as other marginalized groups. Unfortunately, this has further perpetuated psychology’s own history of reinforcing oppression and inequality (Lewis, 2021). This has also led to a tension between “basic” researchers who privilege statistical power and high degrees of internal validity and “applied” researchers who privilege the nuance of intersectional research and external or ecological validity (Lewis, 2021). As an issue of validity, psychological based research has often failed to include diverse participants, or it has failed to report on the diverse identities within samples. A 36-year review of randomized clinical trials of depression (from 1981-2016) found that less than half of the studies reported on the sample’s race/ethnicity, about one in six trials had a predominantly ethnic minority sample, and one in seven studies had a predominantly low socioeconomic (SES) sample (Polo et al., 2019). Similarly, researchers have found inconsistency in demographic data collection and reporting. Racial and ethnic minority groups are often underrepresented, along with disability, and diverse sexual orientations (Greenwell &amp; Hough, 2008). Within research on the lesbian, gay, bisexual, transgender, queer (LGBTQ+) communities, the needs of cisgender white gay men have been privileged, and the experiences of LGBTQ+ individuals who represent women, people of color, transgender, or bisexual communities have largely been omitted or excessively medicalized (American Psychological Association, 2015; American Psychological Association, 2021; Hegarty &amp; Rutherford, 2019). Researchers have noted the importance of consistently collecting and reporting demographic data on race, ethnicity, sexual orientation, gender identity, SES, disability, and other identities (dickey, Hendricks, &amp; Bockting, 2016; Greenwell &amp; Hough, 2008; Polo et al., 2019). For instance, dickey and colleagues (2016) stressed the importance of collecting and analyzing gender identity and sexual orientation data separately in population surveys, especially since gender identity is often conflated with sexual orientation. Parent and colleagues (2013) encourage researchers to focus on the context of intersecting oppressions in addition to intersecting identities, to ensure that diverse populations are included in psychological research. To redress the exclusion of marginalized identities from psychological research and scholarship, researchers recommend going beyond including marginalized individuals in samples to including them in creating the studies themselves. Participatory research is an umbrella term for research that engages those being studied in the production of knowledge to promote education and change (Cargo &amp; Mercer, 2008). Participatory research in its many forms has been implemented with gender and sexual minority populations, refugees, individuals with disabilities, and racial and ethnic minority communities around the globe (Cargo &amp; Mercer, 2008; Fine et al., 2021; Jacquez et al., 2021). Fine et al. (2021) argue that “the move to include and privilege those most impacted by injustice as co-researchers is not simply an act of empathy or decolonizing; it is a commitment to good science” (p.346). 1.5.2 Validity Issues Related to Replication Studies In recent years, psychologists have argued that the discipline of psychology suffers from a replication crisis (Fabrigar et al., 2020). This calls into question the validity of psychological research and the findings that have been reported over the years. Some notable cases of outright fraud, questionable research practices (Pashler &amp; Wagenmakers, 2012), flawed methodologies, and incorrect analysis of data (Fabrigar et al., 2020) have led to some of these replication issues. Others have argued that failures to replicate result from small sample sizes, subsequent low statistical power (Maxwell, Lau, &amp; Howard, 2015; Schmidt &amp; Oh, 2016), as well poor statistical conclusion validity (Fabrigar et al., 2020). Schmidt and Oh (2016) argue that “the real problem is not a lack of replication; it is the distortion of our research literatures caused by publication bias and questionable research practices” (p.32). Addressing the replication crisis is no easy task as it requires addressing journal review processes, research practices, and reward structures in academia (Pashler &amp; Wagenmakers, 2012). The Reproducibility Project, created by the Open Science Collaboration, aimed to address the replication crisis by replicating 100 experimental and correlational studies from key psychology journals (Open Science Collaboration, 2015). The Reproducibility Project found that while 97% of all the original studies had significant results, only 36% of replicated studies demonstrated significant results (Open Science Collaboration, 2015). Schmidt and Oh (2016) note the importance of replicating studies with nonsignificant results and recommend meta-analysis as a solution to this issue, provided publishing bias and questionable research practices are addressed. Some researchers argue that simply replicating studies is not enough to safeguard the validity of psychological research. In an analysis of the Reproducibility Project, Sabik and colleagues (2021) found that the studies reproduced by the Project seldom considered context and identity, even when it was central to the study’s design, and that study samples were predominantly WEIRD (people from Western, educated, industrialized, rich, and democratic countries). Further, intersectionality, power, discrimination, and historical contexts were rarely considered in the Project’s reports (Sabik et al., 2021). Sabik and colleagues argued that the Reproducibility Project and the discourse surrounding the replication crisis are more concerned with data transparency and methods than with the inclusion of historically oppressed and marginalized groups. To truly move the discipline forward, some argue it is necessary to set aside the emphasis on traditional research methods and reproducibility in favor of methods that center on the co-creation of knowledge (Grzanka &amp; Cole, 2021). 1.5.3 Internet Research and the use of Crowdsourcing Platforms Many social science researchers utilize the Internet (e.g., social media, list servs, emails) for purposes of recruitment and data collection (e.g., Survey Monkey, Qualtrics, etc.). Although this has considerable implications for internal and external validity that go beyond the scope of this chapter, it is essential that we discuss some of the implications of crowdsourcing platforms. Crowdsourcing platforms are online websites that can be used by researchers to recruit potential participants who have access to Internet and an electronic device (Peer et al., 2017). And although we cannot review all of the available crowdsourcing platforms, researchers have several platforms to choose from, including CrowdFlower and Prolific Academic (see Peer et al., 2017 for a review of the various strengths and limitations of these crowdsourcing platforms). However, Amazon’s Mechanical Turk (MTurk) has garnered some of the most attention in recent years by methodologists and scholars. It is likely that many of the issues surrounding internal and external validity with MTurk are also applicable to other crowdsourcing platforms, but we will focus mostly on MTurk for this chapter. Created in 2005 by Amazon, MTurk is an online marketplace where workers (Turkers) complete Human Intelligence Tasks (HITs) for MTurk requesters for pay. This is equivalent to a psychology undergraduate pool, but using a larger sample of the population. Typical HITs are transcribing movies, copying text from images, and participating in surveys. In addition to regular MTurk workers, there are MTurk “masters” whose accuracy are validated by previous MTurk requesters. MTurk is used extensively by businesses, academic researchers, and nonprofits (Pew Research Center, 2016). A review of key journals in psychology, psychiatry, and other social sciences found that fewer than 50 studies using MTurk data were published in 2011; in 2015, over 500 studies using MTurk data were published (Chandler &amp; Shapiro, 2016). Although MTurk is a cost-effective, efficient method to collect large amounts of data, there are concerns about the reliability and validity of MTurk data. For instance, several researchers have called the external validity of MTurk data into question. Turkers are relatively young and well-educated compared to national averages (Hitlin, 2016; Walters et al., 2018). Walters et al. (2018) found that MTurk workers’ health status and behaviors were not comparable to a nationally representative sample. Compared to a national sample, MTurk users were over twice as likely to screen positive for depression, but they were less likely to exercise, smoke, have asthma, or have health insurance (Walters et al., 2018). Another concern regarding MTurk data is the overall decrease in data quality resulting from an influx of computer programs or “bots” that complete HITs and individual users bypassing location restrictions using server farms (“farmers”). Chmielewski and Kucker (2019) conducted the same study over four years and found a substantial increase in low-quality data from MTurkers, including failures to replicate well-established findings, decreases in the reliability and validity of the Big Five Inventory, a widely used personality measure, and increases in participants failing response validity indicators (Chmielewski &amp; Kucker, 2019). MTurk remains a valuable resource for collecting data, provided the necessary steps are taken to ensure data quality. Buhrmester et al. (2018) recommend that researchers take the time to work on MTurk themselves to understand the Turker experience. Other recommendations include screening responses before approving HITs, including validity indicators, and comprehensive reporting on screening and study designs (Buhrmester et al., 2018; Cheung et al., 2017; Chmielewski &amp; Kucker, 2019; Mason &amp; Suri, 2011). MTurk is particularly useful for researching hard-to-reach populations such as individuals with disabilities, LGBTQ+ individuals, and those with low socioeconomic status (Smith et al., 2015). To safeguard against Turkers lying about being part of the target group, Smith et al. (2015) recommend providing monetary incentives that are not overly attractive and asking participants to self-identify prior to sharing the purpose of the research. Walters at al. (2018) suggested that researchers would benefit from using MTurk workers over masters because the two groups were comparable in demographics and health characteristics; however, workers are a larger sample and more cost-effective. With adequate measures to ensure data quality, MTurk remains an efficient and cost-effective option for researchers, especially those studying hard-to-reach populations. 1.6 A Consideration for Practice First introduced in 1999, the RE-AIM framework is a tool that can help researchers balance internal and external validity when planning, designing, and evaluating health-related interventions (Dzewaltowski et al., 2004). Originally intended as guidelines for reporting research results, the framework is now also used to organize literature reviews and to translate research into practice. This has incredible implications for both internal and external validity, as it attempts to take a study beyond just epistemology and into direct practice with populations. The RE-AIM framework’s five dimensions include: Reach, Efficacy/Effectiveness, Adoption, Implementation, and Maintenance (Dzewaltowski et al., 2004; Glasgow, Vogt, &amp; Boles, 1999). Reach and Efficacy/Effectiveness are both individual-level dimensions. Reach considers the percentage of the population of interest included in the intervention and how representative they are, whereas Efficacy/Effectiveness considers the impacts (both positive and negative) on participants (Dzewaltowski et al., 2004; Glasgow, Vogt, &amp; Boles, 1999). Adoption and Implementation are organizational-level dimensions that consider the type and proportions of settings that will adopt the intervention and the extent to which the intervention is implemented faithfully in the real world (Dzewaltowski et al., 2004; Glasgow, Vogt, &amp; Boles, 1999). Finally, Maintenance examines the continuity of the program over time at both the individual and the organizational levels (Glasgow, Vogt, &amp; Boles, 1999). RE-AIM is used in a variety of fields and settings, such as chronic illness management, mental health, smoking cessation, health policy, and diabetes prevention (Kwan et al., 2019). Although the RE-AIM framework is becoming more widely used, a systematic review noted that the framework is often used inconsistently (Gaglio, Shoup, &amp; Glasgow, 2013; Glasgow et al., 2019). Several adaptations and clarifications have been offered to mitigate confusion and inconsistency using the RE-AIM framework. Holtrop and colleagues (2018) offered guidance on integrating qualitative methods into the RE-AIM framework. Holtrop et al. (2021) offered clarifications on common misconceptions about the framework. The Practical, Robust, Implementation, and Sustainability (PRISM) model is an emerging complement to the RE-AIM framework that focuses on contextual factors (Glasgow et al., 2019). With the original goals of producing valid and relevant research and translating research into practice, RE-AIM and PRISM will continue to grow and evolve as researchers apply these frameworks to new populations and settings. Researchers and students can directly go to the website to learn more about how to implement these principles in their research, as well as access various resources, tools, and checklists (http://www.re-aim.org). 1.7 Activity Take a moment to locate the latest issue of a peer-refereed journal in your respective field. Some example psychology journals published by the American Psychological Association include the Journal of Counseling Psychology, School Psychology, Journal of Consulting and Clinical Psychology, Health Psychology, Developmental Psychology, or Professional Psychology: Research and Practice. Once you have located a recent issue, browse through the table of contents and select a quantitative article that may be of interest to you. Read the article and then consider the following prompts: Which threats of internal validity were identified and controlled for in the study? Were any explicitly identified and addressed by the authors the article? Were there any that you noticed that were not addressed or controlled for in the study? How were study participants recruited or sampled for the study? In what ways were study participants diverse? What limitations were mentioned in the discussion section? Were issues of internal and external validity explicitly named? If so, which ones? If not, which internal and external validity issues were implied? How would you replicate the study you read? What additional validity factors would you consider to improve the new proposed study’s internal and external validity? 1.8 References "],["TrainMod.html", "Chapter 2 Understanding Training Models and the Factors that Transcend Them 2.1 Learning Objectives 2.2 Recommended Readings and Resources 2.3 Training in Professional Psychology: An Overview 2.4 Identifying Training Models in Professional Psychology 2.5 The Value of Research and Quantitative/Qualitative Literacy as a Competency 2.6 Embedding Social Justice Regardless of Training Model 2.7 What Do We Do? 2.8 Summary, Conclusions, and/or Recommendations 2.9 Suggestions for Practice, Further Learning, and/or Conversation", " Chapter 2 Understanding Training Models and the Factors that Transcend Them Rachel M. Chickerella (she/her) PhD, Antioch University New England Karen Meteyer (she/her), PhD, Antioch University New England Placeholder for land acknowledgement.  Research methods are the tools used by psychologists and others to collect and analyze data to understand new information or further understand a topic (University of Newcastle Libguides, 2023). Psychologists may employ a range of research designs including quantitative approaches that use numbers to capture information about variables and qualitative designs in which phenomena are represented with words. Though competency in research methods is considered essential to becoming a professional psychologist, or a psychologist engages in clinical work (Rutgers University Catalogs, 2023), there are different approaches to imparting the skills and knowledge to students which reflect distinct training models. Although each degree program in psychology will offer its own unique identity in how it trains students, there are commonalities among models of training. The emphasis placed on certain types of research (qualitative vs. quantitative), the specific techniques that are taught (e.g., statistics) and the level of proficiency expected of students will all vary as a function of the training model. Understanding the different training models and how they approach teaching research methods will help students select the type of degree and program that will best prepare them to achieve their career goals. 2.1 Learning Objectives Learning objectives for this chapter include the following: Define training models in professional psychology for programs and professors. Identify the value of research and quantitative/ qualitative literacy as a competency for programs and professors. Explore ways for professors and programs to integrate social justice regardless of training model. Describe the importance of qualitative literacy along with quantitative literacy for professional psychology programs and professors. List ways to motivate students to engage in course content despite an often perceived boredom related to research methods content. 2.2 Recommended Readings and Resources The following are three works to consider. American Psychological Association. (2012). A practical guidebook for the competency benchmarks. Washington DC: APA. DeAngelis, T. (2003). Three Programs: Three Different Training Models. American Psychological Association. https://www.apa.org/gradpsych/2003/09/three-programs Okun, T. (2021). White supremacy culture–Still here. Dismantling Racism. Dismantalingracism.org 2.3 Training in Professional Psychology: An Overview In pursuing a degree in psychology, there are a multitude of paths students may take. A prospective student may first decide if they are going to pursue a professional psychology degree or a non-professional degree. Pursuing a professional degree means that following the degree and licensure, the individual would be eligible to practice as a psychologist. Health service psychology is another term used to describe programs that use a competency based model to promote the development of practicing psychologists (“What Is a Health Service Psychologist and Why Join,” n.d.). Some pursuits of psychology do not fall under the professional psychology realm, including social, cognitive and experimental psychology. These degrees focus more on research and teaching pursuits. Professional psychology includes multiple disciplines including counseling, clinical and school psychology (“Types of Programs,” n.d.). Clinical psychology programs tend to be housed within the psychology department of educational institutions. Students in these programs typically receive training in understanding and diagnosing pathology (“Counseling Versus Clinical Psychology,” n.d.). Clinical psychology is the most common degree conferred (“Types of Programs,” n.d.). Counseling psychology is another form of professional psychology. Counseling Psychology programs may be housed in the psychology department, but may also be placed in the education department (“Types of Programs,” n.d.). Counseling psychologists tend to put less emphasis on psychopathology and more focus on treating the whole person (“Counseling Versus Clinical Psychology,” n.d.). Finally, School psychologists may be housed in the school of education or psychology, and tend to focus heavily on working with children in school systems (“Types of Programs,” n.d.). Most individuals who complete a degree in professional psychology will either receive a PhD (doctor of philosophy) or PsyD (doctor of psychology). A PhD is the most common degree conferred in psychology and tends to be attained through research focused universities (Michalski et al., n.d.). PhD degrees generally emphasize scientific research and teaching (Michalski et al., n.d.), however, the training also usually includes clinical work. Individuals who pursue a PsyD are more likely to seek a career focused on providing psychological services (Michalski et al., n.d.). PsyD programs tend to be affiliated with a research or teaching university or in a free standing graduate school. In reality there is significant variability and overlap between PhD and PsyD programs, and a consideration of the training model of the program may help determine the right fit for students who wish to pursue careers in professional psychology. 2.4 Identifying Training Models in Professional Psychology There are multiple training models in psychology, and the model that a program ascribes to impacts the type of training experiences and curriculum that is provided as well as the eventual career paths of graduates of both clinical (Cherry et al., 2000) and counseling psychology programs (Neimeyer et al., 2005). There are approximately five models that exist in professional psychology programs. The first is the Clinical Science or Bench Scientist Model (Arizona Clinical Psychology Program, 2023; DeAngelis, 2003). These models have a heavy emphasis on scientific concepts, theories and their implications. Students in such programs would be more apt to have research as a central part of their training and future career goals. They will likely have multiple courses in research methods and have a focus on pursuing research and academic careers following graduate school. Further, they will likely conduct research in labs, perhaps with animals (DeAngelis, 2003). To engage with social justice, such models might benefit by explicitly exploring the ways in which research programs might promote social justice, including obtaining input from members of communities who will be impacted by research. It is likely that programs that are more geared to research than practice, will tend to favor this approach. In contrast to the Clinical Scientist or Bench Scientist model, the Scientist-Practitioner model emphasizes an equal balance between research and therapy experiences in the training of psychologists. The Scientist-Practitioner model was originally developed at an educational conference in Boulder, Colorado (see Petersen (2007) for a succinct history of the model). This model was a response to the realization that research training should be incorporated into clinical training and application (Jones &amp; Mehr, 2007). Philosophically, this model asserts that in order to engage in research on psychological constructs, one must also have clinical experience (DeAngelis, 2003). The model also notes research and practice should continually inform each other (Jones &amp; Mehr, 2007). Students in such programs would be more likely to pursue research that focuses on clinical work or how to improve the mental health of their population(s) of interest. Such programs may promote social justice through encouraging students to utilize their clinical experiences to inform how their research promotes change for therapy clients and systems. Scientist-Practitioner programs will likely have a mentorship model similar to that of a Bench Scientist wherein students will have shared research interest that aligns with the faculty member and/or program. The Local Clinical Scientist model is another model that is present in professional/ health service psychology programs (Trierweiler et al., 2010). This model utilizes many of the same principles as the scientist practitioner model with an emphasis on a more localized praxis. The meaning of the term “local” in this model is multifaceted in this context. The first definition of “local” is to a particular application of general science (Stricker &amp; Trierweiler, 2006). The local clinical scientist would ask, even if an intervention is evidenced based, is it effective in a specific case? The term local also applies to local knowledge which is often specific to a particular culture or group (Stricker &amp; Trierweiler, 2006). The idiosyncratic is also seen as a part of the “local,” as there are many nuances within a specific client or clinical population in a certain context that cannot be explained by theory alone (Stricker &amp; Trierweiler, 2006). Finally, there is the space-time conception of local (Stricker &amp; Trierweiler, 2006). Events happen, both in the world and in individual lived experiences that impact the ways they make meaning of their experiences. The thread that hangs between all of these definitions is that context matters and should be an essential part of how interventions are utilized. Regarding social justice, local scientist programs might focus on how to adapt interventions that are “evidence based” to their specific context. This endeavor may inform both their research and practice. A fourth model, endorsed by the University of Tennessee Knoxville (UTK),is the Scientist-Practitioner-Advocate Model. In 2007, UTK decided to respond to calls for psychologists to more actively engage in social justice by explicitly adding the advocate role to their training model (“Scientist-Practitioner Advocate Training Model - Psychology Department,” n.d.). This model works to address the core competencies inherent in the Scientist-Practitioner model, while also addressing the important role of the advocate to training programs (Mallinckrodt et al., 2014). The model encourages synergies between research and advocacy along with clinical work and advocacy (Mallinckrodt et al., 2014). The Scientist-Practitioner-Advocate model also supports a unique practicum experience in social justice (Mallinckrodt et al., 2014). This model has theoretical underpinnings in feminist, multicultural and social justice principles (“Scientist-Practitioner Advocate Training Model - Psychology Department,” n.d.). Such models more explicitly address social justice as an integral part of a career in professional psychology. A fifth model is the Scholar-Practitioner model of which Dr. Roger Peterson, professor emeritus at Antioch University New England, is a major proponent (Peterson et al., 1997). This model has the greatest emphasis on clinical work, and uses research to inform clinical practice. The model also prioritizes a humanistic approach, focusing on how students can bring their authentic selves into their clinical work (DeAngelis, 2003). As two professors at Antioch New England, we can speak personally to the orientation towards research methods. PsyD students at Antioch take two research methods courses, one that focuses on quantitative methods and statistics and a second that focuses on qualitative research. The students also complete a dissertation, but are not required to publish scholarly research. In this model, social justice may be emphasized by cultivating individual insights into the strengths that scholar practitioners bring to the field. Students may then be encouraged to consider how their specific strengths may be cultivated to promote social justice and change. Keeping in mind the program philosophy can help to inform how to best justify research methods courses and structure them to meet the needs and training goals of students. Students in programs that follow the Bench Scientist model will be more likely to see the inherent value and importance of research methods courses as a part of their degree. Students operating under the scientist practitioner model may be a bit more ambivalent about the pursuit of research methods. Such programs are more likely to be split in terms of students who are hoping to pursue research and practice following graduate school (DeAngelis, 2003). Within the science practitioner frame, those who emphasize a “local” model will explore discernment and understand context when pursuing interventions that are considered evidence-based practice. Scientist-practitioner-advocate programs will likely have a strong emphasis on how their research informs social justice and the needs of communities. Scholar practitioner programs tend to train individuals who are unlikely to pursue research as a career in favor of more applied, health service oriented careers. Scientific knowledge and methods and research evaluation are two of the competencies under “science” in the APA Competency Benchmarks in Professional Psychology. Regardless of the training model presented, there are scientific competencies that trainees have to meet in order to become professional psychologists . It is incredibly important that professional psychologists are able to comprehend, critically examine and summarize scholarly research. What may differ, as discussed above, is the degree of emphasis placed on the production of scholarly research. 2.5 The Value of Research and Quantitative/Qualitative Literacy as a Competency Qualitative and quantitative research methods and statistics are multifaceted and highlight different ways of “knowing” in psychology. Quantitative and qualitative literacy skills are essential to the training of future psychologists regardless of program orientation. Psychologists, at a minimum, need to be able to comprehend and communicate the findings of relevant scholarly literature. As mentioned above, the specific skills and level of expertise required of students in their research methods training may depend on the training model and values of the program. That said, the ability to understand different ways of thinking and knowing, to critically evaluate the quality and rigor of published research and to integrate and apply findings to help individuals and society solve real world problems transcend any variability in program emphasis. Further, human behavior is inherently complex. Psychologists need to rely on more than intuition or lived experience when making assessments about human behavior (Dumper et al., 2019). Enhancing inductive and deductive reasoning through knowledge of research methodologies can be a helpful step in this process (Dumper et al., 2019). Psychology, like many of the other health professions, has shifted to a competency-based model in recent years (Kaslow et al., 2009). APA’s Graduate Benchmarks Evaluation System has delineated a series of benchmarks that are suggested across all levels of training. Building on the work of the American Psychological Association, National Council of Schools and Programs in Professional Psychology, and others, the Association of State and Provincial Psychology Boards (ASPPB) identified clusters of competencies that were meant to capture the most important elements of professional training and practice. These competencies may provide helpful guidelines that should be highlighted regardless of the training model. Under the APA’s benchmark of “science” there are numerous competencies and sub-competencies. The first competency is scientific knowledge and methods (Benchmarks Evaluation System, 2012). Sub-competencies within this competency include scientific mindedness, scientific foundations of psychology and scientific foundation of professional practice (Benchmarks Evaluation System, 2012). The second competency is research/evaluation. Sub-competencies within this competency are scientific approach to knowledge generation and the application of the scientific method to practice (Benchmarks Evaluation System, 2012). Scientific mindedness broadly involves independently displaying evidence of scientific thinking and valuing and applying scientific method to practice (Benchmarks Evaluation System, 2012). In the area of Research Methods both “doing” and “knowing” skills are essential to achieving professional proficiency. At minimum, students are expected to be able to demonstrate competencies in the areas of formulating, conducting, evaluating and disseminating research/scholarship. Importantly, at the most advanced level of training (the post-doctoral level), the ability to integrate science and practice is emphasized above research competency per se, according to APA’s Commission on Accreditation (section C-8 D; Accreditation (2018)). Within the study of different methods for conducting psychological research , students should be familiar with qualitative methods (different paradigms, types and methods of research that rely on words to capture phenomena), a variety of quantitative research designs (i.e., correlational and experimental designs that utilize numbers rather than words as the way data are represented), and other related topics such as sample and meta-analysis, description and inference, hypothesis testing and power. Ultimately, being able to solve complex problems and organize and synthesize information are essential skills for psychologists. In clinical, assessment and academic settings, professional psychology involves solid problem solving skills and an ability to integrate and organize information. The field is ever changing remaining up to date with best practice is essential. A psychologist who is hesitant to consume up-to-date literature or new ideas risks competency as a clinician, researcher and advocate. Thus, quantitative and qualitative literacy are essential foundational skills in professional psychology. 2.6 Embedding Social Justice Regardless of Training Model Regardless of the philosophy of the psychology program, embedding social justice is essential for professional psychology program administrators and professors, particularly given the historical damage inflicted by some researchers on minority groups and continued harm that research created under a colonial lens causes. Social justice is often seen as an afterthought in quantitative courses, given the bias, often no longer outwardly expressed but inwardly held, that numbers are “objective” in some way. We know this to be untrue, as quantitative research has been used and continues to be used to harm marginalized communities. One of the most egregious examples of quantitative methods violating human rights in the United States were the Tuskegee experiments, which involved denying Black patients’ treatment for syphilis to understand the long-term effects (Cokley &amp; Awad, 2013). The way that professors choose to highlight (or not highlight) social justice undoubtedly impacts the degree to which students will see social justice as a priority in research moving forward. Further, for many students, the research methods sequence serves as a “jumping off point” for their thinking around their dissertations and abstract reasoning throughout their careers. Thus, centering social justice in such courses can provide a framework for how students choose to advocate and support marginalized communities as a part of their work both in and beyond their time in their programs. 2.6.1 White Supremacy Culture in research The field of psychology remains overwhelmingly White (Dupree &amp; Kraus, 2022). White scholars are more likely to live in homogenous communities and lack understanding of the impact of racism on people of color (Dupree &amp; Kraus, 2022). Thus, their research is less likely to take a nuanced approach to understanding how marginalization impacts mental health. Even if White scholars decide to include race as a variable in their work, the questions they ask and ways in which they integrate race will be limited by their positionality. It is therefore vital that training programs, regardless of orientation, acknowledge and actively take steps to address the ways in which Whiteness permeates every aspect of the research process. Given that the field of psychology tends to uphold the values of White supremacy culture (Dupree &amp; Kraus, 2022), it is unsurprising that what is valued in academia tends to mirror the colonial structures that oppress marginalized communities. Okun (2021) outlines the characteristics of White supremacy culture they first developed after a decade of facilitating racial justice workshops. They have continued to update their materials as their understanding of the ways Whiteness permeates society evolve. Here, we will explore the ways in which the characteristics of White supremacy culture manifest in academia and research. One such principle of White supremacy present in academia and research is perfectionism (Okun, 2021). Inherent in this principle is the idea that there is one “right way” to do or to know. In research, particularly quantitative research, there is an emphasis on objectivity, and the idea that what we learn from numeric data is somehow a superior form of truth. Further, the pressure to not make mistakes and be some sort of “objectivity robot” makes it hard for researchers to admit when they are wrong or make mistakes. This can also lead to practices like p-hacking (Field, 2012) where, because a person’s original hypotheses were not successful, they will search their data for significant findings and selectively report significant results. Likely related to the importance placed on perfectionism are defensiveness and denial (Okun, 2021). We have a replication crisis in psychology (Diener &amp; Biswas-Diener, 2016) yet scholars are hesitant to admit any fault in their work or judgment at the time. This impulse is understandable given the ways in which White supremacy culture may treat them upon doing so. That being said, if psychology researchers focused less on quantity and their reported objectivism, and were more open to obtaining feedback on their work at the time it was being completed, perhaps there would be an improvement in the quality of the work being produced. Either/or thinking is another characteristic of White supremacy culture (Okun, 2021) that is often reflected in research. Quantitative researchers routinely engage in this bias, particularly when considering whether or not their results have “meaning.” Hypothesis testing and p value cutoffs are two examples of how quantitative research embodies the either/or mentality. The null hypothesis is either rejected or fails to be rejected. Researchers and journals alike are less likely to publish non-significant findings, seeing their perceived “failures” as unworthy contributions. Quantity over quality is a characteristic of White supremacy culture (Okun, 2021) that is relevant to academia and research. In order to obtain a job in academia or acquire tenure, researchers are encouraged (often not consciously) to focus on the number of publications they obtain instead of prioritizing the quality of what they publish (Blaszczynski &amp; Gainsbury, 2019). This undoubtedly impacts the quality of the research being produced. When people look at a CV, they look at the length and the number of publications. It is highly unlikely that those on an academic committee would take the time to read through a candidate’s published work to get a sense of the quality of the content. Such biases speak to what is valued in academia, namely, the quantity of publications. Worship of the written word (Okun, 2021) is another tenant of White supremacy culture that researchers often prioritize. Academics center peer reviewed literature in their articles. We are not here to say that we should do away with the peer review process or that it is “wrong” to use such sources. The potential problem lies in the reality that the only kind of knowledge scientists value tends to be peer reviewed articles. Further, given that psychology is dominated by Whiteness (Dupree &amp; Kraus, 2022) our peer review committees likely embody similar identities. Such an emphasis on peer review discounts the importance of gray literature and sources that fall outside of the “ivory tower” of academic circles. Further, the written word is not the only way to communicate information, and discounts other embodied ways of knowing (Hargons et al., 2017). We as psychologists should also be open to learning from people’s collective knowledge and experiences. Many of the character traits valued in academics and researchers are also embedded in White supremacy culture. The traits include qualities like individualism and urgency (Okun, 2021). Individualism in a research context shows up in so many ways, but one obvious way is the value placed on being first author. Further, the fewer contributors attributed to an article the more impressive an individual’s contribution is seen from the perspective of many in academia (hence the clout ascribed to “single author” publications in many professional spaces). Such a perspective devalues collaboration and makes individualism seem like a superior way of producing knowledge. Individualism can also lead to saviorism, particularly for White folks, and can lead to the perceived goal of research being to “save” communities as opposed to working with communities and amplifying voices. Urgency is also strongly reinforced in research and academic circles. When a new topic becomes “hot” in psychological research, there can be a race to be the first person to publish on a topic. Further, particularly when going up for academic jobs or tenure, there is a sense of urgency that many feel to publish in order to prove their worth in their profession. This likely means that corners are cut philosophically and methodologically to increase the number of publications one has to their name. This culture adversely impacts the quality and scope of the knowledge we consume about psychological constructs. 2.6.2 Not the good kind of WEIRD Race is not the only identity that requires centering in research and training. Psychology research has traditionally focused largely on WEIRD populations (Western, Educated, Industrialized, Rich and Democratic; Henrich et al. (2010)). In centering these populations, researchers in psychology are focusing on limited views of understanding the world. For example, cognitive and motivational processes, along with views on fairness and equality differ across populations (Henrich et al., 2010). In not prioritizing diversity in terms of the samples we use in our research; psychological interventions lack the nuance necessary to treat those who fall outside of White and WEIRD communities. 2.7 What Do We Do? Recommendations for reducing the entrenchment of White supremacy culture in research and training include greater representation of marginalized groups at all levels of the publication process, White psychologists being open to other views and grant agencies prioritizing diversity in research (Cokley &amp; Awad, 2013; Dupree &amp; Kraus, 2022; Henrich et al., 2010). Regardless of the training model used, providing education about the historical and continued erasure of identity differences in psychology is important. Further, professors should provide education about research comparing identity-based groups that has caused harm to communities (Cokley &amp; Awad, 2013). One example is the comparison of racial differences in academic achievement without considering the environmental factors that impact differences in performance (Cokley &amp; Awad, 2013). As professors, it is important that we highlight that comparing groups based on identity factors needs to be done intentionally and with collaborative input from stakeholders in those communities. The Building Equity into Research Design: Community-Based Participatory Research in this OER is one of the approaches for engaging in such collaboration. The tension between modernism and postmodernism (Cokley &amp; Awad, 2013) is present in how White supremacy culture and WEIRDness permeate research. One value of Modernism supports the idea of an objective, knowable truth from which the scientific method can be used to understand psychological processes (Cokley &amp; Awad, 2013). In contrast, postmodernism rejects the idea of objective truth and instead highlights the idea of perspectivism and the social construction of reality (Cokley &amp; Awad, 2013). In their book of qualitative methodologies (Creswell &amp; Poth, 2016) go a step farther, noting that beyond postmodern theories are philosophical paradigms including constructivism (co-creating truth), transformativism (what is true is what will promote systems change), and pragmatism (what is true is what is useful to answer research questions). One constructivist, transformative theory is liberation psychology, or the view that reality is socially and politically constructed (Comas-Díaz, &amp; Rivera, 2020). Liberation psychology highlights the need to center the voices of those most marginalized by systems. The theory posits that psychology should aim to focus on systems and see the oppression and suffering of individuals as a symptom of the problem (Comas-Díaz, &amp; Rivera, 2020). The theory also centers on the idea of mutual accompaniment, and treating the communities we work with as co-investigators in the research process (Comas-Díaz &amp; Torres Rivera, 2020). In reviewing the training models mentioned in the previous section, we might hypothesize that the Bench Model more closely represents a modernist perspective with an emphasis on empiricism and laboratory studies. The Scientist- Practitioner-Advocate model most closely resembles a postmodern, liberation psychology view given its emphasis on advocacy as a part of the role of a psychologist. Qualitative research may help students access different ways of thinking about truth and the ways in which knowledge is constructed beyond the idea of one objective truth. While students are likely learning about diversity and social justice in intervention-based courses, social justice tends to be less of a focus in research methods sequences. Sometimes in Research Methods courses, diversity and multiculturalism will get one week of focus at the end of the academic term. They may be further reduced by doubling as a “catch up” day on other material that felt unclear to students throughout the semester. While likely unintentional, not prioritizing multiculturalism and social justice provides clear messages to our students that such concepts are fringe in their importance to our understanding of research methods. Instead, social justice and multiculturalism should be embedded throughout the semester(s) of methodology courses. Hopefully in doing so, students will be primed to highlight diversity and social justice intentionally in their own research and thinking. 2.7.1 Not just quant- bring in the qual! Another important consideration regardless of training model is the integration of qualitative methods. While this is explained in much more detail in other parts of this textbook, we thought it necessary to bring up qualitative research in a chapter focused on research methods and training models. Research programs in psychology have traditionally emphasized quantitative methods as a way to position themselves as a “hard science” and thereby be taken seriously. Tied in with the previous section on the characteristics of White supremacy culture, it is easy to see how the urge for perfectionism, prioritizing the written word, either/or thinking and urgency might value quantitative methods. Anecdotally, both myself (Rachel) and Karen (who trained in a scientist-practitioner Clinical PhD program) did not have mandatory courses in qualitative methods. Rachel was lucky enough to have a qualitative elective that greatly informed my engagement with qualitative research, along with my thinking as a clinician and researcher. That said, many psychologists from scientist practitioner programs may receive little to no training in qualitative methods. Recent decades have led to advancement in the understanding of the methodological rigor of qualitative research (Levitt et al., 2013). Further, qualitative research allows students to contend with biases they may hold as a researcher and different epistemological ways of knowing that can inform their research process moving forward (Levitt et al., 2013; Ponterotto, 2005). As mentioned above, instead of presupposing some sort of “objective” reality, qualitative research amplifies the idea that reality is socially constructed, and that instead of denying that we have biases, we as researchers should acknowledge our biases and our positionality (Creswell &amp; Poth, 2016). For example, if a researcher’s goal is transformative, or to make positive changes for marginalized groups in society, the researcher can and should champion that research objective, while also acknowledging the identities and experiences that impact their work with a given community. Further, qualitative methods like participatory action research (PAR) champion working with communities and treating community mentors as co-investigators. In the spirit of feminist, indigenous, AIDS and disability rights activists who refused to participate in top down research processes, PAR aims to engage in the philosophy of “no research on us without us” (Fine &amp; Torre, 2019). This leads to research driven by community needs as opposed to what researchers perceive as the needs of a given community. 2.7.2 Bringing the class the life: addressing the barriers of teaching research methods courses There are barriers to successfully teaching research methods regardless of the training model. While being an instructor for quantitative and qualitative research methods may feel like an uphill battle, we know that such courses are crucial in developing analytical reasoning, problem solving abilities, and understanding positionality (in the case of qualitative methods) for psychologists. In this section, we aim to address some of the barriers that come up for research methods professors and propose some ways to help the course(s) run successfully. These suggestions will come both from what we have found to be effective in our own classrooms and from suggestions and materials that other studies and professors have shared. One helpful strategy is for professors to utilize self-disclosure around their own relationships to quantitative reasoning. For example, I (Rachel), in my quantitative course, discuss my own journey with statistics, noting that I was nowhere close to being a “math prodigy” in high school and never expected that teaching a statistics heavy course would be in my future. In fact, I (Rachel) viewed math as an utter waste of my time and a subject that lacked any sort of practical importance (how self-serving, I know). I noted that it was during graduate school that I began to enjoy numbers and statistics. It was when I had professors who were willing to slow the content down and explain concepts in ways that made sense for my brain that(J. F. Anderson &amp; Withrow, 1981; Williams, 2010) I began to enjoy statistics. As I (Rachel) was acquainting myself with this course, I tested the hypothesis of self-disclosure of my own difficulties with math. Self-disclosing my history with math has seemed to relax my students and set an expectation of growth instead of instant mastery. This is not to say that people who are naturally strong mathematicians should not teach statistics – they most definitely should! Even for those folks, highlighting some of the aspects of math courses that they found/find difficult may help alleviate student anxiety related to the course sequence. It is certainly worth seeing how students respond to such disclosures, and if it helps to create a more open learning environment. Further, professors highlighting the ways in which they hope to make the course accessible to those with different learning styles may help to ease some of the inherent concerns that quantitative courses cause many psychology students. It also may be helpful to test using an individualized approach to quantitative courses. According to Samuel and Warner (2021) a growth mindset and utilization of mindfulness can lead to a decrease in math anxiety and increase self efficacy for students who struggle with quantitative content. Going along with the idea of a growth mindset is utilizing a process-oriented approach (Teaching by the Case Method - Christensen Center for Teaching &amp; Learning - Harvard Business School, n.d.). Even when students do not get the correct answer numerically, understanding their processing and reasoning skills and explaining how to arrive at the proper answer will improve quantitative reasoning. Another way to help students to be successful in quantitative methods is to address math anxiety directly at the very beginning of the course. According to Williams (2010) using immediacy as a response to student anxiety pertaining to statistics can reduce math anxiety between 6 and 20 percent (Williams, 2010). Immediacy in this context is defined as using nonverbal behaviors that communicate approachability (e.g., eye contact, open body language, nodding) and verbal behaviors that influence perceptions of closeness (J. F. Anderson &amp; Withrow, 1981; Williams, 2010). Verbal behaviors that were found to increase immediacy include self-disclosure, humor, addressing students by name, conversing with students outside of class and seeking student’s feedback on assignments (Gorham, 1988; Williams, 2010). Student buy-in to a research methods sequence may also be enhanced by discussing why quantitative and qualitative reasoning are important components of becoming a psychologist. As previously mentioned, even if students do not plan to pursue an academic career, in developing clinical judgment, the ability to problem solve and think through complex situations are critical. Further, students may, as a part of their career trajectory, enter leadership roles at psychology clinics. As a part of such roles, they may have to pursue grant funding and justify using data why their clinics may benefit from such funding using qualitative and quantitative research methods. Furthermore, psychologists need to be able to assess evidence-based practices and make decisions about what interventions will be most effective for their clients. Doing so involves an ability to understand and critically evaluate quantitative research. 2.7.3 Take risks! My brilliant co-author, Karen, a seasoned professor who has taught quantitative methods at Antioch for several years, came up with a fabulous idea to “bring the classroom to life”. She first informed me of the idea around Halloween, when she sent me a video of Jimmy Fallon and Shailene Woodly exploding a pumpkin. She then showed me a database of people in myriad classrooms who had exploded a pumpkin as a part of their class experience. They took measurements of the circumference, number of rubber bands needed, height, thickness of the rubber bands, and other factors that played a role in the explosion of the pumpkin. To say I was nervous was an understatement. As a new faculty member who had been at Antioch approximately two months, I didn’t want to do anything that might “rock the boat.” What if the activity backfired in some way? What if people didn’t like it or felt upset by the pumpkin becoming increasingly combustible? Karen, who was also open to exploring the question of whether or not we should do it, brought up the idea of taking risks, and actually “doing the thing.” The assignment was a roaring success. We were able to teach the concept of correlations through a method of having students use the previous data to make predictions about how many rubber bands it would take for the pumpkin to explode based on measurements we took about our own pumpkins. Students who were less thrilled about the idea of exploding pumpkins were invited to leave the room. The activity was such that different groups occupied different spaces outside the classroom anyway, making their absence less noticeable. The student whose prediction was closest to when the pumpkin actually exploded was invited to pick from a number of prizes. Many students came up to us after we were finished and spoke to how much fun they had engaging in this fall themed activity. There is a lot that can be taken away from this activity. One important point for new faculty is that having access to mentors who are more experienced than you to bounce ideas off of can be incredibly helpful. All new professors, and even professors who are seasoned in their work, should seek consultation and feedback about what sort of ideas and activities will successfully convey difficult concepts. When teaching research methods and statistics in particular, it can be easy to focus primarily on the content at the expense of process. We posit that content and process can coexist, even in a course where often there might be one right answer. Another activity that might benefit students is using data from real world populations in order to inform social justice. For example, our university is connected to a training clinic that serves the surrounding community. We have deidentified data from this clinic that we base our homework assignments and final projects on for our quantitative research methods course. Such data helps illuminate for students how mental health outcomes may differ based on identity factors including race, age, gender, social class and other variables collected at intake. Engaging meaningfully with clinical data can both provide the clinic with important information about how to improve services for the communities we serve. Such data also allows students to see how numbers can be used to better understand and improve clinical outcomes for diverse populations. 2.8 Summary, Conclusions, and/or Recommendations Instruction in research methods, both quantitative and qualitative, is a complicated course sequence for psychology professors. Ensuring competency in these areas, particularly for students who reject the idea of needing such courses as a part of their training, can be a difficult task. Depending on the training model utilized and the emphasis on research methods, the course sequence will inevitably look different. Further, motivations may differ to be competent in research methods based on whether or not a student wants to pursue a career in academia. At a minimum, students should be proficient in formulating, conducting, evaluating and disseminating research and scholarship. Regardless of training model, multiculturalism and social justice should be thoughtfully integrated throughout the course(s). In particular, an emphasis on working alongside communities, understanding the importance of quality over quantity and understanding the value of both numbers and words will be pivotal in liberating research methods from White supremacy culture. As instructors, we can learn to take risks and utilize creativity and vulnerability as parts of our teaching process. In shifting the ways in which we teach we may begin to subvert the ways research methods are perceived to future generations of psychologists. 2.9 Suggestions for Practice, Further Learning, and/or Conversation To better understand the impact of training models on the doctoral students research training, we encourage you to examine websites of 10 APA-accredited programs in clinical, counseling, and school psychology. Seek a diversity of programs (PhD and PsyD with differing training models). Complete this table and then reflect on the similarities and differences you noted. Did there seem to be patterns according to training model? Institution Program Training Model Statistics Courses Research Courses Doctoral Research Project Requirements Notes on Research Training 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. REFERENCES Accreditation, C. on. (2018). Standards of Accreditation for Health Service Psychology (2015) &amp; Accrediting Operating Procedures (2019). https://www.apa.org/ed/accreditation/about/policies/standards-of-accreditation.pdf Anderson, J. F., &amp; Withrow, J. G. (1981). The impact of lecturers‟ nonverbal expressiveness on improving mediated instruction. Communication Education, 30, 342–353. Arizona Clinical Psychology Program, U. of. (2023). Program Overview &amp; Training Model Psychology. https://psychology.arizona.edu/clinical/training-model Benchmarks Evaluation System. (2012). American Psychological Association. https://www.apa.org/ed/graduate/benchmarks-evaluation-system Blaszczynski, A., &amp; Gainsbury, S. M. (2019). Editor’s note: Replication crisis in the social sciences. International Gambling Studies, 19(3), 359–361. https://ideas.repec.org//a/taf/intgms/v19y2019i3p359-361.html Cherry, D. K., Messenger, L. C., &amp; Jacoby, A. M. (2000). An examination of training model outcomes in clinical psychology programs. Professional Psychology: Research and Practice, 31(5), 562–568. https://doi.org/10.1037/0735-7028.31.5.562 Cokley, K., &amp; Awad, G. H. (2013). In Defense of Quantitative Methods: Using the “Master’s Tools” to Promote Social Justice. Journal for Social Action in Counseling &amp; Psychology, 5(2), 26–41. https://doi.org/10.33043/JSACP.5.2.26-41 Comas-Díaz, L., &amp; Torres Rivera, E. (2020). Liberation psychology: Theory, method, practice, and social justice. American Psychological Association. https://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=2548637 Counseling Versus Clinical Psychology. (n.d.). In Towson University. Retrieved January 20, 2025, from https://www.towson.edu/cla/departments/psychology/grad/psychology/counseling/versus.html Creswell, J. W., &amp; Poth, C. N. (2016). Qualitative Inquiry and Research Design: Choosing Among Five Approaches. SAGE Publications. DeAngelis, T. (2003). Three programs: Three different training models. gradPSYCH Magazine, 09. https://www.apa.org/gradpsych/2003/09/three-programs Diener, E., &amp; Biswas-Diener, R. (2016). The Replication Crisis in Psychology. In NOBA. https://nobaproject.com/modules/the-replication-crisis-in-psychology Dumper, K., Jenkins, D., Lovett, M., &amp; Perlmutter, M. (2019). Psychological Research. OpenStax. https//openstax.org/details/books/psychology Dupree, C. H., &amp; Kraus, M. W. (2022). Psychological science is not race neutral. Perspectives on Psychological Science, 17(1), 270–275. https://doi.org/10.1177/1745691620979820 Field, A. P. (2012). Discovering statistics using R. Sage. Gorham, J. (1988). The relationship between verbal teacher immediacy behaviors and student learning. Communication Education, 37(1), 40–53. https://doi.org/10.1080/03634528809378702 Hargons, C., Mosley, D., Falconer, J., Faloughi, R., Singh, A., Stevens-Watkins, D., &amp; Cokley, K. (2017). Black Lives Matter: A call to action for counseling psychology leaders. The Counseling Psychologist, 45(6), 873–901. https://doi.org/10.1177/0011000017733048 Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29–29. https://doi.org/10.1038/466029a Jones, J. L., &amp; Mehr, S. L. (2007). Foundations and assumptions of the scientist-practitioner model. American Behavioral Scientist, 50(6), 766–771. https://doi.org/10.1177/0002764206296454 Kaslow, N. J., Grus, C. L., Campbell, L. F., Fouad, N. A., Hatcher, R. L., &amp; Rodolfa, E. R. (2009). Competency Assessment Toolkit for professional psychology. Training and Education in Professional Psychology, 3(4, Suppl), S27–S45. https://doi.org/10.1037/a0015833 Levitt, H., Kannan, D., &amp; Ippolito, M. R. (2013). Teaching qualitative methods using a research team approach: Publishing grounded theory projects with your class. Qualitative Research in Psychology, 10(2), 119–139. https://doi.org/10.1080/14780887.2011.586101 Mallinckrodt, B., Miles, J. R., &amp; Levy, J. J. (2014). The scientist-practitioner-advocate model: Addressing contemporary training needs for social justice advocacy. Training and Education in Professional Psychology, 8(4), 303–311. https://doi.org/10.1037/tep0000045 Michalski, D. S., PhD, Fowler, G., &amp; PhD. (n.d.). Doctoral degrees in psychology: How are they different, or not so different? In https://www.apa.org. Retrieved January 20, 2025, from https://www.apa.org/ed/precollege/psn/2016/01/doctoral-degrees Neimeyer, G. J., Saferstein, J., &amp; Rice, K. G. (2005). Does the Model Matter? The Relationship Between Science-Practice Emphasis and Outcomes in Academic Training Programs in Counseling Psychology. The Counseling Psychologist, 33(5), 635–654. https://doi.org/10.1177/0011000005277821 Okun, T. (2021). White Supremacy Culture – Still Here. https://www.dismantlingracism.org/uploads/4/3/5/7/43579015/white_supremacy_culture_-_still_here.pdf Petersen, C. A. (2007). A Historical Look at Psychology and the Scientist-Practitioner Model. American Behavioral Scientist, 50(6), 758–765. https://doi.org/10.1177/0002764206296453 Peterson, R. L., Peterson, D. R., Abrams, J. C., &amp; Stricker, G. (1997). The National Council of Schools and Programs of Professional Psychology educational model. Professional Psychology: Research and Practice, 28(4), 373–386. https://doi.org/10.1037/0735-7028.28.4.373 Ponterotto, J. (2005). Qualitative Research Training in Counseling Psychology: A Survey of Directors of Training. Teaching of Psychology. https://www.semanticscholar.org/paper/Qualitative-Research-Training-in-Counseling-A-of-of-Ponterotto/76f17a0d8342482ac56aa4930871310f54bb01a7 Samuel, T. S., &amp; Warner, J. (2021). \"I Can Math!\": Reducing Math Anxiety and Increasing Math Self-Efficacy Using a Mindfulness and Growth Mindset-Based Intervention in First-Year Students. Community College Journal of Research and Practice, 45(3), 205–222. https://doi.org/10.1080/10668926.2019.1666063 Scientist-Practitioner Advocate Training Model - Psychology Department. (n.d.). In Psychology. Retrieved January 20, 2025, from https://psychology.utk.edu/graduate-students/programs/phd_counseling/scientist-practitioner-advocate-training-model/ Stricker, G., &amp; Trierweiler, S. J. (2006). The local clinical scientist: A bridge between science and practice. Training and Education in Professional Psychology, S(1), 37–46. https://doi.org/10.1037/1931-3918.S.1.37 Teaching by the Case Method - Christensen Center for Teaching &amp; Learning - Harvard Business School. (n.d.). Christiansen Center for Teaching; Learning. Retrieved January 20, 2025, from https://www.hbs.edu/teaching/case-method/Pages/default.aspx Trierweiler, S. J., Stricker, G., &amp; Peterson, R. L. (2010). The research and evaluation competency: The local clinical scientist—Review, current status, future directions. In Competency-based education for professional psychology (pp. 125–141). American Psychological Association. https://doi.org/10.1037/12068-007 Types of Programs. (n.d.). In Psychology Graduate School. Retrieved January 20, 2025, from http://psychologygradschool.weebly.com/types-of-programs.html What is a Health Service Psychologist and Why Join. (n.d.). In National Register. Retrieved January 20, 2025, from https://www.nationalregister.org/hsp-credential/what-is-an-hsp/ Williams, A. S. (2010). Statistics Anxiety and Instructor Immediacy. Journal of Statistics Education, 18(2), 187. https://doi.org/10.1080/10691898.2010.11889495 "],["ComRes.html", "Chapter 3 Building Equity into Research Design: Community-Based Participatory Research in Counseling Psychology 3.1 Learning Objectives 3.2 Recommended Readings and Resources 3.3 What is Community-Based Participatory Research? 3.4 What are Health Disparities, and How Do We Promote Health Equity? 3.5 Principles of Community-Based Participatory Research 3.6 Considerations for Conducting Community-Based Participatory Research 3.7 Critical Questions for Conducting and Reviewing Community-Based Participatory Research 3.8 Suggestions for Carrying out Community-Based Participatory Research: Applications for Practice, Further Learning, and Conversation 3.9 Summary and Conclusions 3.10 Questions for Mastery and Reflection 3.11 Appendix A: Checklist for Community-Based Participatory Research", " Chapter 3 Building Equity into Research Design: Community-Based Participatory Research in Counseling Psychology Melissa M. Ertl (she/her), Columbia University and the New York State Psychiatric Institute Meredith R. Maroney (she/her),University of Calgary Sharon G. Horne (she/they), University of Massachusetts Boston The Columbia University Irving Medical Center and Washington Heights sit on the land of the Lenape, Wappinger, and Schaghticoke people, the Indigenous residents of Manahatta (“island of hills” in the Algonquian language). The University of Calgary is located on the traditional territories of the people of the Treaty 7 region in Southern Alberta, which includes the Blackfoot Confederacy (comprising the Siksika, Piikani, and Kainai First Nations), the Tsuut’ina First Nation, and the Stoney Nakoda (including the Chiniki, Bearspaw, and Wesley First Nations). The City of Calgary is also home to Métis Nation of Alberta, Region III. The University of Massachusetts Boston occupies the traditional, ancestral and unceded land of the Pawtucket and Massachusett First Nations. Let us commit ourselves to the struggle against the forces that have dispossessed Indigenous peoples of their lands. The focus of this lesson is to provide an overview of community-based participatory research, its principles, and its application to research in counseling psychology. We discuss critical considerations and recommendations for conducting community-based participatory research and offer resources to help guide the creation and critique of community-based participatory research studies. We hope, through introducing community-based approaches—with their focus on partnerships, community relationships, and building capacity—this chapter provides an opportunity to think about how relationships in research can represent important opportunities to engage in liberatory practices and solidarity to address inequities and underrepresentation. “Nothing that we do that is worthwhile is done alone. Everything worthwhile is done with other people.” -Mariame Kaba, activist and abolitionist “If you have come here to help me, you are wasting your time. But if you have come because your liberation is bound up with mine, then let us work together.” -Attributed to the Australia Aboriginal Rights Collective and Lilla Watson 3.1 Learning Objectives Learning objectives for this chapter include the following: Describe the principles of community-based participatory research. Discuss the roles of history, oppression, power, privilege, and structural inequities in creating health disparities. Assess community needs, strengths, resources, and assets to inform interventions to address community-identified health problems. Explain strategies to collaborate and partner with community organizations to reach community health goals. Critique your role as a health researcher and psychological professional and how to successfully work with communities to improve health outcomes. Apply collaborative, participatory approaches when working with communities and demonstrate attention to culturally appropriate community engagement and empowerment with diverse communities. Featuring a 20-minute interview facilitated by Dr. Meredith Maroney with Dr. Roberto Abreu about his line of community-based research at University of Florida, where he is working in community with LGBTQ+ and Latinx folks through collective efforts aimed at improving the lives of oppressed communities through research and development: https://youtu.be/dn9e9z-mEeQ Abreu, R. L., Gonzalez, K. A., Mosley, D. V., Pulice-Farrow, L., Adam, A., &amp; Duberli, F. (2022). “They feel empowered to discriminate against las chicas”: Latina transgender women’s experiences navigating the healthcare system. International Journal of Transgender Health, 23(1-2), 178-193. https://doi.org/10.1080/26895269.2020.1767752 3.2 Recommended Readings and Resources The following four articles served as critical references in the development of this chapter. The subsequent two videos discuss or present recent exceptional, thought-provoking examples of community-engaged and community-based research. We encourage you to review them. Israel, B. A., Schulz, A. J., Parker, E. A., Becker, A. B., Allen, A. J., Guzman, J. R., &amp; Lichtenstein, R. (2017). Critical issues in developing and following CBPR principles. In M. Minkler &amp; N. Wallerstein (Eds.), Community-based participatory research for health: Advancing social and health equity (2nd ed., pp. 32-35). Jossey-Bass. Dr. Israel and colleagues provide an excellent overview of community-based participatory research principles and discuss the challenges and facilitating factors in conducting community-based participatory research. The authors are leaders in the field of community-based participatory research. Baciu, A., Negussie, Y., Geller, A., Weinstein, J. N., &amp; National Academies of Sciences, Engineering, and Medicine. (2017). The root causes of health inequity. In Communities in action: Pathways to health equity (pp. 99-184). National Academies Press. This chapter is a phenomenal introduction to the social determinants of health, or the constellation of factors that lead to deeply rooted inequities in society, including education, income and wealth, employment, health systems and services, housing, the physical environment, transportation, the social environment, and public safety. The authors describe how these determinants are highly influenced by the institutional and societal structures, policies, and norms of the U.S. that are shaped by legacies of historical oppression and its contemporary manifestations. Fine, M., Torre, M. E., Oswald, A. G., &amp; Avory, S. (2021). Critical participatory action research: Methods and praxis for intersectional knowledge production. Journal of Counseling Psychology, 68(3), 344-356. https://doi.org/10.1037/cou0000445 Dr. Fine and colleagues reflect on their work conducting years of community based participatory action research and introduce a critical lens. They provide an overview of a CPAR and illustrate practices with LGBTQ+ youth as partners in the national What’s Your Issue Study (https://whatsyourissue.org/about-us/who-we-are/). Collins, S. E., Clifasefi, S. L., Stanton, J., The Leap Advisory Board, Straits, K., Gil-Kashiwabara, E., Rodriguez Espinosa, P., Nicasio, A. V., Andrasik, M. P., Hawes, S. M., Miller, K. A., Nelson, L. A., Orfaly, V. E., Duran, B. M., &amp; Wallerstein, N. (2018). Community-based participatory research (CBPR): Towards equitable involvement of community in psychology research. American Psychologist, 73(7), 884-898. https://doi.org/10.1037/amp0000167 This article introduces CBPR as an important and promising research framework that has been underutilized in psychology but is capable of guiding the implementation of more effective, culturally appropriate, socially just, and sustainable research. The authors discuss the unique aims of and challenges in conducting CBPR as well as practical and ethical challenges for its integration into psychology research. They also include a case study of the use of CBPR in psychology to illustrate its key constructs and implementation. Society for Qualitative Inquiry in Psychology. (2022). Society for Qualitative Inquiry in Psychology (SQIP) distinguished researcher interview: Eva Maria Simms. https://vimeo.com/685074894?fbclid=IwAR0ucXDJOPtEbVj9kVgtWmmuECC6xQccmPh0SaERJFDH2VZKCrNDlkjIGEI This interview by SQIP features Dr. Eva Maria Simms, highlighting her approach as a community-engaged qualitative researcher. Dr. Simms is professor of psychology at Duquesne University and a Distinguished University Professor with over 30 years of experience teaching qualitative research with a focus on phenomenological and community-engaged methods. She launched a research lab at Duquesne called “Placelab,” and through her lab, she mentors students in community-engaged qualitative inquiry to serve local communities of Pittsburgh. In this conversation, she outlines four community-engaged research projects that Placelab has conducted in collaboration with local communities in Pittsburgh for needs assessment and advocacy and describes the importance of social positionality, relationship-building, longevity and sustainability, and using qualitative inquiry and documentary filmmaking to influence the law. Prism Research. (2022). Becoming myself: Positive trans and nonbinary identities. https://www.youtube.com/watch?v=0pyzeSvv_dw Directed and produced by Michael Breeding MEDIA and Executive Producers Zakary Clements, Dr. Ellen D. B. Riggle, and Dr. Sharon S. Rostosky, this 10-minute film is the product of a community participatory action project that the authors created with participants to provide inspiration and support to trans and nonbinary people. This documentary is dedicated to the research team’s collective social justice mission and commitment to the well-being and flourishing of people who are marginalized and stigmatized. The film features narratives from the lives of seven transgender and nonbinary young adults who discuss the positive aspects of their identities and offer support to other transgender and nonbinary people. 3.3 What is Community-Based Participatory Research? Community engagement has been described as integral to improving health research and enhancing health promotion efforts within communities [Clinical and Translational Science Awards (CTSA), 2011]. Community-engaged research strives to improve community health and entails working collaboratively with groups of people to address issues affecting the well-being of those people, often through partnerships or coalitions that help mobilize resources, influence systems, change and forge relationships among partners, and serve as catalysts for change in policies and practices (CTSA, 2011). Because community engagement is rooted in principles of community organization, including fairness, justice, empowerment, participation, and self-determination (e.g., Alinsky, 1962; Freire, 1970; Martín-Baró et al., 1996; Wallerstein &amp; Duran, 2006), community-engaged health research seeks to embody these principles to produce social action. Community-based participatory research, or CBPR—a specific, well-established framework within community-engaged research that emerged from social justice and action traditions—involves full community participation in research and occurs in a context in which collaborators respect the strengths that each individual brings to the partnership (CTSA, 2011). The hallmark of CBPR is shared decision-making and leadership of the research activities through strong partnerships with community members and stakeholders. CBPR begins with an important research topic, the aim of which is to achieve social change to improve health outcomes and eliminate disparities in health (CTSA, 2011; Israel et al., 2017). Through participation, marked by a high level of engagement and mutual respect from all collaborators and stakeholders, CBPR seeks to integrate education and social action to improve quality of life and health for marginalized and oppressed communities. Counseling psychology as a field has increasingly focused on community-engaged and community-based approaches to research (Baranowski et al., 2016; Fine, 2007; Jensen &amp; Case, 2022; Jones et al., 2020). Because CBPR is strongly aligned with the values of counseling psychology for social justice, multiculturalism, equity, and liberation (Delgado-Romero et al., 2012; Smith et al., 2010; Singh, 2020), it is a promising framework for carrying out counseling psychology research with communities. Additionally, CBPR has been suggested to be capable of producing tangible benefits for communities and improving overall health outcomes while also improving the quality of the research itself as well as the relevance of the findings (Braun et al., 2012; CTSA, 2011). A recent example details how the arts-based research methodology of digital storytelling can use stories, photos, and videos to capture the lived experiences of underserved and underrepresented populations, the results of which can empower communities to promote social change (Fish &amp; Syed, 2020). CBPR is capable of fostering empowerment for community partners and stakeholders (Turin et al., 2022; Vivona &amp; Wolfgram, 2021), which can lead individuals to “gain greater control over their lives and environment, acquire resources and basic rights, achieve important life goals, and reduce societal marginalization” (CTSA, 2011, p. 15; Freire, 1970; Maton, 2008). 3.4 What are Health Disparities, and How Do We Promote Health Equity? Health disparities are socially constructed, unjust, and avoidable differences in health and well-being between and within groups of people (Browne et al., 2015; Farmer, 2013). Health disparities exist largely due to the structural inequities that underlie social conditions, including unequal social, economic, and environmental conditions as well as forms of social oppression (Baciu et al., 2017; Braveman, 2006; Kimber et al., 2022). Accordingly, health disparities tend to disproportionately affect individuals from marginalized and oppressed backgrounds that are underserved and underrepresented in health research (Wallerstein, 2002). Health disparities are complex and multifactorial and manifest in myriad ways, including in access to preventive and medical care, inequitable treatment decisions (e.g., studies have documented that Black patients in the Veterans healthcare system were much less likely to receive invasive cardiac procedures compared to similar White patients), and retention in care, as well as disparate care and health outcomes, incidence and prevalence rates, stage at diagnosis, the financial burden of the condition, length of survival after diagnosis, quality of life, and mortality (National Academy of Sciences, 2019; National Cancer Institute, 2020; Riley, 2012). Conversely, health equity is the pursuit of the highest possible standard of health for all people, with a particular focus on populations at greatest risk for poor health (Browne et al., 2015). Social justice is at the heart of health equity and serves to guide our best efforts to address and eliminate health disparities. Counseling psychologists are particularly well-positioned to conduct CBPR aimed to address health disparities and to promote health equity in communities (Tucker et al., 2007). Because counseling psychologists can intervene at the micro and macro levels and have expertise in health promotion and prevention, counseling psychologists are able to conduct socially responsive CBPR efforts to promote health equity, such as the design and adaptation of culturally responsive interventions, the implementation of programs to empower patient and community health, the assessment of capacity for structural and organizational change in groups and systems, the evaluation of trainings in how to provide culturally and linguistically responsive care, and the analysis of public health policy to advocate for more equity-oriented legislation, among others. 3.5 Principles of Community-Based Participatory Research CBPR principles require that academic and community partners work together to design studies, collect and interpret the data, and disseminate the findings (Braun et al., 2012). The following nine principles are viewed as critical components inherent to designing and carrying out ethical CBPR projects (Israel et al., 1998, 2017). However, it is important to note that all principles may not be applicable in all settings, cultures, or communities, and the principles used in any given study should be adapted to the local context of each partnership (Israel et al., 2017). 3.5.1 Recognize community as a unit of identity Although much of the research in psychology views participants as individuals, CBPR recognizes community as the key unit of identity. Individuals belong to larger communities based on their socially constructed identities, which shape the strengths, challenges, and disparities that affect a community (Collins et al., 2018). In CBPR projects, it is critical to clearly define the community of focus (Nicolaidis &amp; Raymaker, 2015). Moreover, the priority community must have a desire to engage with the researchers on problems of mutual interest (Braun et al., 2012). 3.5.2 Build on strengths and resources of the community CBPR should seek to build on the strengths, including assets, resources, and resourcefulness of a community, as opposed to only focusing on the problems affecting a community (CTSA, 2011). Respecting community values and capitalizing on the cultural assets and resources of a community have been described as necessary to improve the success of the research and to yield more meaningful findings (Braun et al., 2012). Through a CBPR lens, community partners are viewed as valued contributors to the research process (Collins et al., 2018). In order to successfully collaborate with a community and foster engagement, researchers must come to this work by engaging in self-reflection and recognizing their own culture and how it shapes one’s beliefs and understanding of health and illness (Airhihenbuwa, 2007; Duran et al., 2013). Because community-based and community-engaged programs may often involve people from universities and health institutions who are intending to collaborate with communities that are societally positioned as vulnerable to a given health outcome, it is necessary for researchers and stakeholders to acknowledge how society produces privilege, racism, and inequalities in power based on factors like background, experience, culture, race and ethnicity, socioeconomic status, gender identity, sexual orientation, ability status, and other social identities (CTSA, 2011). Taking a critical, culturally responsive approach can help partners better understand and address the social determinants of health issues and can mitigate the risk of reproducing oppressive patterns within health partnerships (CTSA, 2011; Chávez et al., 2008). Capacity building—the process by which the skills, resources, and competencies of community, organizational, and institutional partners are assessed and further developed—is one strategy that can be used to build on the strengths of those involved in the project (Collins et al., 2018). 3.5.3 Facilitate collaborative and equitable partnerships in all research phases CBPR requires equity and power-sharing between researchers and the community. By design, CBPR should be empowering and capacity-building for all those involved, all the way from the conception of a study to the communication of its results (Collins et al., 2018). Capacity-building among community partners addresses and challenges power imbalances, encouraging individuals to be able to discuss and resolve problems that may emerge in the research process (Muhammad et al, 2015). This facilitation involves acknowledging inequalities between researchers and community partners and the ways that these inequalities shape the participation in the research (Israel et al., 2017). Community partners, to the extent desired, should collaborate and share responsibility for all phases of research, including the design, conduct, analysis, interpretation of results, generation of conclusions, and dissemination phases of research. These partnerships can be with any number of community stakeholders, including but not limited to community members, people with lived experience, community-based organizations, nonprofits, public health agencies, practice-based researchers in clinics and health care organizations, schools and institutions, and policymakers (CTSA, 2011). Community members and stakeholders can participate as advisers, hired staff, administrators, or leaders of the research and should be empowered to approve, disapprove, and recommend changes to the proposed research (Braun et al., 2012). Collaborative partnerships enable the development of programs and research that are consistent with a community’s cultural framework (Airhihenbuwa, 1995). The end result allows for researchers and community partners to co-own the research process and resulting deliverables or products (Collins et al., 2018). Continuous reflection, evaluation, and adjustment helps ensure equity throughout the process (Nicolaidis &amp; Raymaker, 2015). 3.5.4 Provide mutual benefit of all partners and stakeholders Aside from simply producing new knowledge, CBPR should lead to tangible improvements in the community (Braun et al., 2012). Examples of benefits for community members, academics, and health professionals include networking opportunities, learning, access to knowledge and resources, the gratification of working together to help to solve community problems, improved stakeholder relationships, and increased problem solving capacity (CTSA, 2011). Additionally, through participating in the research, community partners may often experience benefits from taking an active role in bettering their own lives, having fulfilled social obligations, feeling a sense of community, and earning rewards for their time and energy (e.g., payment). A community’s time is valuable and limited, which underscores the need for CBPR to respect community partners’ efforts and contributions to the work. Information is gathered to inform action, and there is a commitment to translate and integrate research findings with community change efforts toward mutual benefit (Israel et al., 2017). 3.5.5 Promote reciprocal transfer of knowledge and skill CBPR requires more than simply conducting the research; projects should also have mechanisms in place to facilitate the reciprocal transfer of knowledge and skill across all stakeholders (Braun et al., 2012; Duran et al., 2013). This entails listening to one another and co-learning through completing the project tasks, including the generation of ideas, the contributions to research decision-making, and the sharing of responsibility by those involved (Duran et al., 2013). Through working together, CBPR offers opportunities for community partners to learn about research and for academic researchers to learn about the culture and health of the community (Braun et al., 2012). Researchers also learn from community members about local theories, which are understandings and beliefs of the community partners that are derived from the community and social context (Israel et al., 2017). Multidisciplinary and identity diverse teams that collaborate and share their unique perspectives are particularly helpful for promoting co-learning and reciprocal transfer of knowledge and skill among partners (Collins et al., 2018). 3.5.6 Focus on problems of relevance to the community and attend to social determinants CBPR must address health from a positive, socio-ecological perspective based on the identified needs in a particular community (CTSA, 2011; Israel et al., 2017). A community should be empowered to identify and address its own issues, and projects should strive to achieve social change to eliminate disparities in health affecting communities (Braun et al., 2012). Attending to the social determinants of poverty, discrimination, and structural racism through CBPR is a critical aspect of efforts to address health disparities (Collins et al., 2018). Before beginning the project, researchers should recognize and have an accurate view of their own intersecting social identities and cultivate cultural humility to promote respectful partnerships that honor the contributions of all those involved (Collins et al., 2018; Hook et al., 2013). 3.5.7 Involve a cyclical and iterative proce To develop and implement research projects that equally consider community and academic interest that obtain desired results, CBPR requires considerable time and effort (Braun et al., 2012). A research question that is collaboratively defined in the initial stages of the research may need to be redefined over the course of the research and as the project progresses (Collins et al., 2018). This iterative process often entails community meetings to discuss, propose, review, improve, or interpret findings related to proposed and ongoing projects. Ongoing research may reveal additional knowledge about the needs of the community, which provides opportunities to jointly redefine and recalibrate the research methods and interventions (Collins et al., 2018). This cyclical, iterative process includes partnership development and maintenance, community assessment, agreement on the problem, development of research design and methodology, data collection and analysis, interpretation of data, determination of action and social policy implications, dissemination of findings, action taking, identification of learnings, and the establishment of mechanisms for sustainability (Israel et al., 2017). 3.5.8 Disseminate and share back findings collaboratively A critical aspect of conducting CBPR is dissemination. It is important to disseminate findings and knowledge back to the community and involve all partners in the dissemination process (CTSA, 2011). Findings should be presented respectfully through various mediums, such as reports, newsletters, presentations, and community meetings. Presentations and authorship opportunities should be available to community members (Braun et al., 2012). Ongoing discussions of findings and how they are used and understood should also inform social action (Israel et al., 2017). 3.5.9 Support a long-term process and sustainability Although CBPR can be resource-intensive and funding levels may fluctuate, CBPR partnerships should continue to function as long-term initiatives (Braun et al., 2012; Duran et al., 2013). Communities should also be supported in efforts to obtain their own funding. As projects evolve into long-term partnerships, the focus may move from a single health issue to address a range of social, economic, political, and environmental factors that affect health (CTSA, 2011). Benefits from the research should ideally be long-lasting and can include community interventions that become routine and embedded in the community or policy changes at a larger level (Collins et al., 2018). Although partnerships may come to an end, the relationships that exist between communities, organizations, and researchers should be honored, and those involved should continue to collaborate with and support one another as desired (Israel et al., 2017). 3.6 Considerations for Conducting Community-Based Participatory Research 3.6.1 Positionality CBPR is a collaborative method that aims to center the voices of community members from underrepresented backgrounds in the research process. Social justice values and decolonization should both inform and be embedded throughout the research process. This approach is particularly important when the principal investigator and research team do not share the identities that they are exploring. This method can serve as a way to share power with communities whose experiences have often been misrepresented or pathologized in research and to center the community members’ needs and perspectives throughout the process. For researchers choosing to engage in CBPR, we strongly encourage engaging in researcher reflexivity practices (e.g., team discussions, memoing [i.e., a collection of hunches, interpretations, queries, and notes made by the researcher from the beginning to the end of the project], bracketing [i.e., the process of becoming aware of one’s implicit assumptions in order to avoid their undue influence on the research]) before joining with community partners (Morrow, 2005). This is quite common in qualitative research (Levitt, 2020), but should be explicitly integrated into quantitative and mixed methods CBPR designs. The specific processes for exploring assumptions and managing perspectives may differ with your chosen methodology and epistemology, but should not be overlooked in CBPR. CBPR research teams are comprised by people with different levels of power and privilege, and accordingly, should aim to center the voices of those who are most often silenced, marginalized, or oppressed (Fine et al., 2021; Muhammad et al, 2015; Torre, 2009). Although some researchers may share identities with communities, Duran and colleagues (2013) described in these cases the importance for researchers to “live with the contradictions of finding how our lived experiences of oppression intersect with those of our [community] partners, yet not advantage and claim the same level of marginalization” (p. 53). Practices to explore positionality and foster reflexivity may include reflecting on and sharing motives for engaging in this research, the identities held (both privileged and oppressed), how identities are situated in society and local context, and how researchers will ensure that they are not replicating harmful practices or dynamics through participation in the research with community partners. It can be helpful to have these conversations up front as a way to share power across the research team. Fine and colleagues (2021) detail their powerful process of joining with participants through critical conversations on how the population of study or issue has previously been framed, while also engaging in vulnerable conversations about systemic oppression and their perspectives and beliefs stemming from their positions. In addition to reflecting on power and assumptions brought to the research process, teams should also create space for the gifts and strengths that all co-researchers bring to the process (Fine et al., 2021). When approaching CBPR, those with the most power—whether it be in terms of race, ethnicity, gender, sexual orientation, education level, job title, or supervisory status—are encouraged to find a balance between centering others’ voices and experiences, while also challenging themselves to share in a vulnerable way. In our experience, this can set the tone for more meaningful work. 3.6.2 Ethics After deciding to engage in community-engaged research, it is important to think critically about the ethics of your study in several key domains. First, what is it you are hoping to achieve and in what ways can participants contribute? As you begin to consider this, there are different levels of involvement that may be appropriate. For instance, you may be approached by a community who already have a clear idea of what they would like to investigate and have articulated an interest in involvement throughout the process. It is important to engage in ethical decision-making, such as a risk-benefit ratio assessment, related to the dissemination or publication of findings as it relates to authorship (Collins et al., 2018). When conducting research with communities that may have potential risk, it can be important to communicate and uphold ethical standards related to privacy and confidentiality. For instance, there may be risks with naming collaborators, and circumstances could change over the course of collaboration. Our third author has worked with communities who initially wanted to be identified at the time of the study, but later, political circumstances changed, and she was relieved that they had used pseudonyms and participant numbers. This is particularly important to be mindful of when collaborating with populations that may be targeted by harmful legislation (e.g., lesbian, gay, bisexual, transgender, and queer [LGBTQ+] people) or stigmatization as a result of their involvement in such research. Another possible outcome could be seeking out community members to join an already established study to provide feedback on the measures, design, and study outcomes. Researchers who are focused on health equity may also benefit from partnering with community organizations, health service organizations (e.g., clinics, treatment programs), and government entities (e.g., Departments of Health) engaged in similar work in order to use existing infrastructures and maximize impact. If you are approaching a community or individual members of a particular group, it is important to have a clear sense of what the “ask” is—including the specific tasks and expectations, the time commitment, whether or not this is a paid opportunity, and other possible ways the community may be compensated. Additionally, it is important to have conversations about data-sharing up front, such as who has rights to the data, how collaborators will involve each other (or not) when publishing or sharing with key stakeholders. We advocate for paying people for their time whenever possible, particularly when engaging with groups who have been historically marginalized and systemically excluded from the research process. This may mean you have to adjust your expectations of involvement if you have a small budget to work with, or clearly state what aspects of involvement are expected and which are optional. Nicolaidis and colleagues (2019) created fantastic guidelines for involvement of autistic adults in research, which we see as important processes for researchers to adopt with any stigmatized group. Examples include to “individually assess accommodation needs, discuss as a group, and re-assess regularly,” “actively listen to community partners’ views and demonstrate value for the expertise that comes from lived experience,” and “co-create lay-language briefs that can be shared in non-academic venues” (see a complete list in Box 1; Nicholaidis et al., 2019). Researchers should spend time reflecting on these logistics. An important part of CBPR is relationship-building with community partners, which can take time and is often at odds with neoliberal and capitalist realities (including institutional expectations for research productivity). Wallerstein and colleagues (2019) found that relational practices played an important role in challenging power differences, particularly fostering an attention to bidirectional communication and reflection, which helped to build trust across partnerships. Furthermore, it is critical to ensure that relationships are fostered, built, and sustained whenever possible to avoid the perception and reality of researchers only joining with communities in order to leave when researchers feel that they have what they need (e.g., data). Well-meaning researchers should be sure not to over-promise to communities in the early stages of relationship-building. Particularly for White researchers working with communities from other racial and ethnic backgrounds, it is critical to engage in true ally behavior by committing to transform systems of White dominance and challenge interlocking forms of structural oppression in order to promote equity, as opposed to engaging in superficial commitments or paternalistic behaviors that reflect White saviorism and reinscribe the status quo of White dominance (Freire, 1970; Spanierman &amp; Smith, 2017). 3.6.3 Dissemination When engaged in CBPR, it is important to share knowledge back with communities in ways that best meet their needs. Researchers should be aware that this may not be in the form of academic presentations or peer-reviewed publications. We encourage conversations about dissemination and what kinds of deliverables or products will be produced from the research and knowledge to take place at the start of a working relationship, with the recognition that these conversations may evolve along with the evolving, iterative process of the research (Israel et al., 2017). The preferences and needs of community partners will likely vary by project, community, target audience, and their hopes for the findings. For instance, some community partners may appreciate updates on the research process and findings on a regular basis, while others may prefer to wait until the analysis has been completed. It is important to consider the target audience and adjust the dissemination plan accordingly. As CBPR teams are co-constructing their study design, it may be beneficial to have a discussion about the desired impact of the findings and the timeline. For instance, CBPR research teams may be simultaneously working on disseminating articles via traditional academic outlets, while also creating products that are more accessible, including zines, infographics, or presentations. A great example of creative dissemination emerged from a collaboration between Dr. Stephanie Budge, JKX Comics, and Hallie Funk, in which they created a comic to share results from a study focused on psychotherapy with transgender and nonbinary clients (see here: https://www.jkxcomics.com/psychotherapy; Budge et al., 2021). In addition, there may be a more time-sensitive need for the findings (e.g., an introduction or enactment of anti-transgender, nonbinary, gender diverse legislation and policies), in which case, products like white pages or policy briefs may need to be prioritized. To ensure that findings are written in a manner that is clear and impactful for the target audience and are grounded in social justice values, researchers should seek to center community perspectives to understand what they see as the most helpful deliverable (e.g., infographic vs. workshop co-presented with community partners). See the Dissemination chapter in this OER for more detailed information on this process (De La Rue et al., 2022). 3.6.4 Barriers or Challenges in Conducting CBPR Despite the many benefits to doing this work, the reality is often more complicated—with many barriers and challenges. As mentioned above, building sustainable and meaningful relationships takes time and intentionality in order to do so in an ethical, social justice-informed manner. Additionally, sharing power and decision-making of the research with community partners accordingly lessens the researchers’ control over initiatives and processes, which can seem labor-intensive and “slow moving” to researchers who are not used to working with community partners in meaningful ways. The time can often be at odds with competing demands and timelines, particularly for those with less power in the academy. For instance, a graduate student may feel quite passionate about conducting a CBPR project for their dissertation, while at the same time, may feel pressure to stick to a predetermined timeline in order to complete the dissertation on time for graduation. For graduate students, this pressure could also be financial in nature (e.g., the need to graduate on time to avoid accruing more debt and delaying the onset of being paid for their labor). Also, graduate students often relocate for clinical internships and jobs, making it challenging to foster and maintain these relationships. There may be pressure from a graduate program that is eager to ensure students graduate on time due to limited funding for students. Although graduate students may set out to conduct a CBPR project with passion and excitement, they may enter communities with high expectations that are difficult for both the researcher and partners to meet, resulting in tensions and unforeseen challenges (Lac &amp; Fine, 2018). We encourage students and their advisors to engage in thoughtful discussions about these realities and to discuss ways to support those committed to CBPR research in creative ways. This could mean having a longer timeline to graduation if this is possible for everyone involved, designating a segment of the project that has a defined timeline, or collaborating with a community with whom the advisor has already built a relationship, rather than starting over with a new community. There are also many external pressures for early career researchers who are committed to conducting CBPR research. For those who are on the tenure track, taking the time to build relationships and partnerships may be at odds with institutional expectations for publication for promotion and tenure, and community engaged work may or may not be valued by a given department or institution. Additionally, academics are often required to move from their communities in order to accept an academic job offering, which can make it challenging to “hit the ground running” in a new area. It is important to be thoughtful about the ways in which we choose to include community members so that it is done in a meaningful way and does not tokenize or exploit their resources. As we shift towards strongly valuing research that centers community in meaningful ways, we should also be mindful of these competing forces when reviewing articles or serving on dissertation or tenure and promotion committees. We run a risk of shaming those who have attempted to do the work, but do not have the privilege of tenure, large grant funding, or institutional support, among the other resources often needed to engage in participatory research practices like CBPR. Reviewers should take care to reduce the “ivory tower” expectations of work that may feel difficult, unattainable, or unreachable for early career researchers or graduate students with less financial stability. Research should be conducted in a meaningful, non-exploitative, and thoughtful manner, which could involve different levels of community involvement (e.g., solely including a community advisory board to consult and provide input on the project versus enacting all nine principles of CBPR in a research project). Researchers should be able to articulate the relationships and the processes that led to the study design, while also making recommendations for ways to build on this work. We see this as a gap in the literature on ways to provide meaningful recommendations for researchers who are interested in conducting community-based work at different levels. 3.6.5 Connecting Research to Policy for a Greater Impact CBPR emerged in response to the exclusion and mistreatment of participants in the research process, including lack of transparency, lack of data sharing, and short-lived relationships, particularly for individuals from marginalized backgrounds (Wallerstein et al., 2019). CBPR, and particularly models such as community based participatory action research (Fine, 2008) and critical participatory action research (Fine et al., 2021), are well-suited to creating an impact beyond the study. There is great precedence for using CBPR to work towards social change through a social justice-informed framework (Fine, 2007; Fine et al. 2021) alongside communities who have experienced injustice. Findings from CBPR projects can be tailored to different audiences to further work toward policy changes. There are a number of ways in which CBPR results can be leveraged to make an impact on policy, including defining a problem, increasing public awareness of the impact of specific policies or practices, encouraging the adoption of specific policies, proposing alternative policies, and responding to impending policies (Cacari-Stone et al., 2014). For instance, CBPR teams may choose to conduct a training with community members on how to leverage findings when talking to their local representatives and may also prepare a policy brief to be shared with those in power. Results can be positioned to advocate for the issues at hand, to ensure that there is a compelling story to tell policymakers (Horowitz et al., 2009). By translating findings into specific advocacy talking points, CBPR researchers can ensure that research contributes to sustained changes that will benefit the community for years to come (e.g., American Psychological Association [APA], 2022; APA Division 44 Public Policy Committee, 2018). 3.7 Critical Questions for Conducting and Reviewing Community-Based Participatory Research These questions, which were modeled after Levitt’s (2020) Qualitative Journal Article Reporting Standards (JARS-Qual) Guidelines, provide recommendations for authors and reviewers to consider when engaging in and reviewing CBPR. The JARS-Qual Guidelines, first developed in 2018, outline what should be included in qualitative research manuscripts to help facilitate the review process. Researchers: What privileged and oppressed identities do you hold? How are they situated within your research team and society? At which point in your study design will you reach out to community members? How will you engage in power sharing with community partners? What is the impact of systemic oppression on the issue or priority population? How will you embed social justice values throughout the research process? How will you balance institutional demands with community needs regarding timeline, ethics approval, grants, compensation? In what ways will you compensate community partners for their time? What is the desired timeline for project completion? At which points can you be most flexible? What are possible action steps your team can take related to advocacy or policy changes? How will you disseminate research findings through multiple avenues? What is the impact you are hoping for? Does the impact differ for community partners, and if so, in what way? How will you resolve challenges and/or conflicts that may arise during your project? Is there a conflict resolution process you will use? Who will participate, and how will power-sharing be engaged in during this process? What are the respective needs (e.g., publications, presentations, community visibility of the project and partners, findings that support social justice aims at health prevention, engagement in learning, etc.) of the project for you as researchers and for community partners? Peer Reviewers of Journal Articles: Consider researcher positionality when making comments about study design. Do authors share their identities? And do these dimensions of experience relate to the focus of study? What is the career stage of individuals on the authorship team? Consider the amount of funding researchers received when evaluating the duration of community members’ contributions. Inquire how researchers discussed this with participants and how the amount of funding may have impacted the research process. Were community partners compensated for their time, energy, and efforts? This may include incentives beyond funding, such as researchers providing findings to stakeholders, giving talks, or disseminating research with other avenues than traditional publishing. Reflect on the helpfulness of comments: Instead of critiquing beyond the scope of the project, ask researchers to specify why particular actions were taken and consider encouraging researchers to be transparent about the external forces that may have impacted their study design and how they sought to manage and address these challenges. Read carefully for information about the nature of community involvement. Was this managed in an ethical and social justice-oriented manner? Did involvement seem to match the needs of the study (and the stated desires of community partners)? Be wary of discouraging smaller-scale CBPR projects. What are the realities, pressures, and constraints facing the researchers, and how did they balance these realities, pressures, and constraints with CBPR values of collaboration, equity, and transformation? Are any community partners listed as co-authors? Were they offered the opportunity to share authorship? Did partners opt out due to reasons related to safety or impact? Due to the timelines of many CBPR projects, be open to writing multiple manuscripts on a project that each focus on a particular aspect of the project (e.g., the process of establishing a research project, preliminary findings, focus groups that supplement a project aim, longitudinal impact of the project, reflections from team members). 3.8 Suggestions for Carrying out Community-Based Participatory Research: Applications for Practice, Further Learning, and Conversation As part of this chapter, we have compiled three resources to provide assistance to those forming partnerships and exploring options for community-based participatory research. We recommend beginning by listening to an interview with Dr. Roberto L. Abreu (he, him, él), who conducts community-engaged research with Latinx and LGBTQ+ communities to promote bienestar colectivo (collective well-being). Dr. Abreu is an Assistant Professor and Director of the ¡Chevere! Lab in the Department of Psychology, Counseling Psychology, at the University of Florida. In this interview, Dr. Abreu shares invaluable insights as an early career researcher, such as the importance of building trust with communities, advocating within systems to conduct research in a social justice informed manner, sharing and disseminating knowledge gained with the community to promote co-learning, and asking research questions that will benefit communities. Next, we have provided some lessons learned informed by our third author’s (SG) experiences conducting CBPR research with a wide range of LGBTQ+ communities both in the U.S. and transnationally. Finally, we have included a checklist for researchers to adapt and share with potential community partners as they begin their relationships (see Appendix A). 3.8.1 Lessons Learned CBPR requires deep commitment to the research process, to collaborative engagement with community partners, and to the social justice aims of liberation psychology (Martín-Baró et al., 1996). As part of this work, we have reflected on several lessons learned. The nature of CBPR and action research more specifically requires great flexibility in adapting to changes during long-term projects. For example, some community partners may be invested initially but then have other priorities and concerns that take precedence. They may no longer be available (and may even be inaccessible depending on the circumstances). It can be helpful to work through an alternate plan if there are personnel changes or interruptions during the project. This can include spelling out who would be willing to support the project if community partners were to leave, re-evaluate plans for publishing and presenting responsibilities, and having several different ways to reach collaborators (i.e., phone, email, home address, a friend who is a close contact). It can be important to schedule check-ins about projects aims, goals, and needs, as well as project team member commitments, several times during a project. Sociopolitical events can impact CBPR work significantly, especially when collaborating with marginalized communities whose members may be managing event-related stigma, threats, and violence. For example, an onslaught of anti-transgender legislation in dozens of states over the past couple of years have changed privacy and safety concerns for transgender, nonbinary, and gender diverse people living in these contexts. It has also shaped where and how resources can be distributed due to the need for financial and human resources to be dedicated to battling these legislative initiatives. Similarly, LGBTQ+ research projects have been impacted by the Russian invasion of Ukraine (Gifkins et al., 2022), with Ukrainian LGBTQ+ people seeking safety and asylum in other countries and Russian LGBTQ+ people fleeing their country for neighboring countries of Armenia and Georgia. CBPR projects may need to be reevaluated for impact on and capacity of community partners when events like the murder of George Floyd occur, which highlight systemic racism on a national level. Sociopolitical events can cause tensions among research partners, and it is important to acknowledge and discuss potential impact even if such events do not appear to be directly related to the CBPR project. Even when a CBPR project is focused on a sensitive or serious issue (e.g., violence, traumatic events), it is important to prioritize community engagement and pleasure. It is very challenging to sustain projects that focus on negative impact or health-related harms for a lengthy period. In addition, U.S. researchers often receive research training that is goal-driven and results focused, but research that is often most creative and innovative evolves from spaces that prioritize joy and shared learning opportunities. This aspect of CBPR can involve group meals, timeouts for dancing or playing games or sports, and prioritizing opportunities for decompressing and discussing positive aspects of the project or collaboration. Other ways to build in pleasure include having family members or friends join in group events, especially if the project is requiring much emotional energy or time. As a group, CBPR researchers and partners can read books or articles that embed pleasure with transformative justice or show other ways that projects have disseminated research (e.g., Brown, 2019). CBPR researchers can share the excitement of engaging in this approach to research and all the ways they have learned and benefitted from the work. 3.9 Summary and Conclusions Counseling psychology, with its strong emphasis on the values of social justice, health equity, and liberation (e.g., Delgado-Romero et al., 2012), has an opportunity to provide exposure, training, and experience in community-based participatory research paradigms with the potential to partner meaningfully with communities, build relationships and community capacity, enhance the effectiveness of our health promotion efforts, and maximize the impact of our work. Through this chapter, we discuss definitions and principles of CBPR, how CBPR may offer more effective pathways to promoting health equity, considerations for conducting CBPR, barriers or challenges commonly encountered, critical questions for conducting and reviewing CBPR, lessons learned, and three resources to assist those getting started. 3.10 Questions for Mastery and Reflection What are the 9 principles of CBPR? What is important for researchers to consider when engaging in CBPR? Name some ways that reviewers can be more supportive of CBPR. Name 3 different approaches to CBPR. Describe challenges that an early career professional or tenure track faculty member may encounter when engaging in CBPR. What are ways that institutions can support CBPR? 3.11 Appendix A: Checklist for Community-Based Participatory Research Building Relationships and Trust – Semi-Structured Ways to Engage Visit community partners – listen and show up when invited Tour of the organization Informal meeting to learn about their work Attend a community event Review key resources (with partner permission) Researcher hosted event Informal meeting Exploratory conversation Luncheon Listening session Town hall Campus event Training or service that supports community partner’s articulated need Matching community partners with researchers Leverage existing infrastructures through partnerships with community-based organizations, health services prevention and treatment organizations, or governmental entities Work with more established researchers who can connect early stage investigators with potential partners Explore if university mechanisms exist that may allow partners to reach out to connect to a researcher Fostering Engagement and Participation through Creative Methods Community-driven health impact assessment (Cameron et al., 2011) Photovoice (Jurkowski &amp; Ward, 2007) Focus group (Kieffer et al., 2013) Mapping community capacity (McKnight &amp; Kretzmann, 1997) Concept mapping (Vaughn et al., 2017; Windsor, 2013) Digital storytelling (Fish &amp; Syed, 2020) Needs assessment (Collier et al., 2012) Creating Opportunities for Reflection Consider researcher positionality Reflect on power and privilege as it impacts research topic and relationships among research partners Bracketing and memoing as a part of qualitative methodology Disseminating Findings Zines Infographics or videos Presentations, workshops, or teach-ins White pages, advocacy talking points, and policy recommendations (APA, 2022; APA Division 44 Public Policy Committee, 2018) "],["Dissem.html", "Chapter 4 Best Practices for Dissemination to Scientific and Lay Audiences 4.1 Learning Objectives 4.2 Recommended Readings and Resources 4.3 Importance of Dissemination 4.4 Research Dissemination for Scientific Audiences 4.5 Dissemination to the Public 4.6 Summary, Conclusions, and/or Recommendations 4.7 Suggestions for Practice, Further Learning, and/or Conversation", " Chapter 4 Best Practices for Dissemination to Scientific and Lay Audiences Lydia HaRim Ahn (she/her/hers), Arizona State University Melissa K. Holt (she/her/hers),Boston University Lisa De La Rue (she/her/hers), University of San Francisco Tanvi N. Shah (she/her/hers), Boston University The University of San Francisco resides on the traditional homelands of the Ramaytush Ohlone (pronounced rah-my-toosh oh-loh-nee) tribal nation. We acknowledge the painful history of genocide and forced removal from this territory, and we celebrate the public presence of Ohlone descendants who are working today to preserve and nourish their indigenous identity. We (Melissa and Tanvi) acknowledge that the territory on which Boston University stands is that of The Wampanoag and The Massachusett People. Our classroom and BU’s campus are places to honor and respect the history and continued efforts of the Native and Indigenous community leaders which make up Eastern Massachusetts and the surrounding region. This statement is one small step in acknowledging the history that brought us to reside on the land, and to help us seek understanding of our place within that history. Ownership of land is itself a colonial concept; many tribes had seasonal relationships with the land we currently inhabit. Today, Boston is still home to indigenous peoples, including the Mashpee Wampanoag and Wampanoag Tribe of Gay Head (Aquinnah). For more information, please visit the North American Indian Center of Boston and the Commission on Indian Affairs of the State of Massachusetts. I (Lydia Ahn) acknowledge the 22 Native Nations that have inhabited this land for centuries. Arizona State University’s four campuses are located in the Salt River Valley on ancestral territories of Indigenous peoples, including the Akimel O’odham (Pima) and Pee Posh (Maricopa) India Communities, whose care and keeping of these lands allows us to be here today.  The focus of this chapter is on dissemination of research findings, geared toward those in the field of psychology. The chapter first reviews the importance of dissemination and then describes ways in which researchers disseminate their findings to scientific and lay audiences, with attention to culturally responsive and equitable dissemination practices. The chapter closes with recommendations for best practices in dissemination. 4.1 Learning Objectives Learning objectives for this chapter include the following: Describe examples of best practices in dissemination Discover ways to disseminate research for scientific and lay audiences, with a lens toward considering ways to amplify research that centers on dismantling oppressive practices and includes voices from marginalized groups Learn practical recommendations for enhancing dissemination tailored to specific audiences and formats 4.2 Recommended Readings and Resources The following served as central references in the development of this chapter and are valuable resources for academics interested in bolstering their dissemination efforts. Ashcraft, L. E., Quinn, D. A., &amp; Brownson, R. C. (2020). Strategies for effective dissemination of research to United States policymakers: A systematic review. Implementation Science, 15(89). https://doi.org/10.1186/s13012-020-01046-3 Sommer, R. (2006). Dual dissemination: Writing for colleagues and the public. American Psychologist, 61(9), 955-958. https://doi.org/10.1037/0003-066X.61.9.955 This article reviews both scientific writing and writing for the public. The Campbell Collaboration: The Campbell Collaboration, a social science research network, provides open source and policy-relevant syntheses, many of which include plain language summaries of research goals and results. Community Tool Box. (2022). Chapter 6: Communications to promote interest. This tool box offers in-depth guidance on developing and tailoring a communication plan for a general application. Schroeder, S., &amp; Bauman, S. (2019, Aug). Dissemination of rural health research: A toolkit. Rural Health Research Gateway. This document, focused on rural health research, provides individuals with a set of resources to help package and disseminate research findings. Additionally, this document describes multiple modes of information dissemination and offers examples, which could be applicable to other areas of research. 4.3 Importance of Dissemination Psychologists who engage in research are often under pressure to publish manuscripts in peer-reviewed journals and present at academic conferences, an experience that is similar across academic disciplines. More recently, other outlets–such as Twitter–have also become key points of dissemination for researchers. Indeed, only a few years after the launch of Twitter, one in four academics began to use Twitter (Priem et al., 2012) as a platform to disseminate their research, engage in virtual conversations around scholarship, and disseminate their research to broader audiences. Platforms such as Twitter, along with changes in the open access publishing space, have resulted in the general public having more access to research findings, which historically had been restricted to those with access to university library systems. There have also been renewed conversations regarding optimal ways of disseminating research, including to scientific audiences, lay audiences, and other specific audiences (e.g., schools, U.S. policy makers; (Ashcraft et al., 2020; Baker et al., 2021). In this chapter, we focus specifically on research dissemination for scientific and lay audiences, with attention to culturally responsive practices and considerations. 4.4 Research Dissemination for Scientific Audiences Historically, research has been designed for scientific audiences, such as other researchers, students, and faculty. Typically, academic psychologists are considered to be strong scientific communicators when they publish journal articles, write books, present at psychological conferences, receive grants or serve as advisors on grants, and serve on governing bodies of psychology organizations (Garvey &amp; Griffith, 1965). Further, in graduate school, students are trained to achieve these goals, from learning how to effectively engage in scientific writing (Sommer, 2006) to how to write in a way that is understandable to academic audiences (Lewis &amp; Wai, 2021). Similarly, graduate students, faculty, and researchers attend regional, national, and international conferences to share their own research findings and learn from others’ research through presentation modalities such as posters, roundtables, and symposia. Attendance at key conferences such as the American Psychological Association’s (APA) Annual Conference is high; over 12,000 individuals attended APA in 2019 (APA, 2019). While publications, books, and conferences have been the main sources of research dissemination to scientific audiences, existing barriers can limit the reach of scientific findings to other researchers. For example, if an independent researcher outside of the academic community would like to conduct research, they may have trouble accessing scientific articles without a database. In addition, some universities may not pay for specific databases or journals. An international scholar may struggle to find literature without access to certain databases at their university. A parent on parental leave may not have the time, energy, or resources to attend an in-person conference without consideration of their family needs. In addition, the review and publication process can be lengthy, making it challenging for research to be disseminated in a timely manner. To address some of these barriers, there has been an increase in quicker and accessible methods of dissemination, such as open science practices like pre-prints. One method for rapid dissemination that has become increasingly prevalent is the use of pre-prints. The Public Library of Science (PLOS) is a public server that allows researchers to post their manuscripts before they are published. This allows for others to read, provide feedback, and cite pre-prints in their work. Relatedly, open science practices highlight the importance of transparent, shared scientific research that makes research accessible to everyone. It creates knowledge freely accessible to everyone and can be useful for building scientific communities and collaboration (Fecher &amp; Friesike, 2013). The “publish or perish” culture refers to the academic pressures of publishing so that faculty members may gain tenure (De Rond &amp; Miller, 2005). Open science aims to be more equitable by enhancing collaboration through data sharing, transparency, and an orientation toward the public for recognizing the importance of the work rather than gatekeepers of science (Matsick et al., 2021). Piwowar and colleagues (2018) examined the prevalence and impact of open access publications, and they found that 27.9% of publications are in open access outlets, and notably, open access articles are cited more frequently. While there are numerous benefits to open access publications from the point of view of the reader, there are also challenges, particularly around equity. For instance, given the costs charged to authors for publishing in open access sources, this restricts who is able to do so and also may skew authors from high-income countries to publish open access publications (L. T. Smith, 2021). Buchanan and colleagues (2021) highlight the pervasiveness of racism and White supremacy in research dissemination. In turn, they provide strategies to dismantle systems of oppression in psychological science. For example, they note the importance of language, which can often perpetuate stereotypes towards People of Color. While APA now requires the reporting of race and ethnicity in research articles, Buchanan and colleagues (2021) provide additional recommendations for researchers. For instance, they advocate for using systems-centered language that discusses the roots of health inequities, selecting inclusive journal keywords, defining race contextually and conceptually (i.e., naming that race is a social construct), reporting ethnicity (not only race) of all participants, and highlighting variability among people of color. In addition to practical recommendations when writing articles for scientific communities, Buchanan and colleagues (2021) also suggest to “promote the visibility of BIPOC scholarship” via website spotlights or Twitter posts (p. 7). To do this, it would be useful for journals to require positionality statements, or the researchers’ worldview and the position that they take in research in research articles (Darwin Holmes, 2020; Savin-Baden &amp; Major, 2023), in order to showcase diversity in journals. There is also a need for methods to disseminate information to BIPOC communities, such as having practice-oriented, open-access journals that provide research findings in a digestible way, use popular platforms like social media networks or newsletters to disseminate research to the communities who would benefit from the information, and collaborate with community partners in the research and dissemination processes. Finally, scholars in the academic community also are utilizing “Academic Twitter” as a space to disseminate their research findings. With the development of social media, this allows for researchers to share their research in more accessible ways. As dissemination through platforms such as Twitter continue to progress, journal editors should also be reflective and intentional about the articles they choose to spotlight and should consider utilizing social media to highlight articles that focus on decolonizing and uprooting systemic oppression within the field of psychology. This extends to articles spotlighted through other avenues as well, including list-servs. 4.5 Dissemination to the Public Traditionally, research has been heavily confined to academic and formal scholarly spaces with the dissemination of work primarily focused on reaching other researchers. While dissemination to other scholars is an important consideration that allows for the continued refinement and generation of theories and interventions, this inherently limits the impact of work. Sommer (2006) stressed the importance of dual dissemination, noting that it is critical for psychologists to write articles for both academic journals and for the public. Indeed many researchers engage in their work with a greater vision of helping to address a societal-level concern or striving to improve the lives of others. In order to fully realize this hope, information needs to get to the people who will actually use it in the real world. Public dissemination can take many different formats. One helpful approach is to translate scholarly or academic works into plain language summaries or briefs. The Campbell Collaboration is a social science research network that provides open source and policy-relevant evidence syntheses (e.g., systematic analyses). Many of these syntheses also include plain language summaries, which capture the goal and results of the research in comprehensible language that can be understood by individuals from non-academic backgrounds. Science communication (i.e., SciComm) is a promising area of research that emphasizes the importance of communicating science to others. However, there are multiple trainee, mentor, and institutional barriers that make it challenging to teach and mentor trainees on scientific communication (C. B. Anderson et al., 2022). Despite these barriers, providing presentations at schools, community clinics, or neighborhood events are other examples of dissemination that can allow the public access to research findings. These spaces will allow for dialogue around the issues that are relevant to the particular space and give the researcher an opportunity to share how their work might support the needs of the community. The knowledge and learnings from research should be shared with the communities in which it was generated, and in language that is accessible and clear. The work should be disseminated back to people in a culturally appropriate way (L. T. Smith, 2021). There are two ways that this can be done as described by Smith (2021). The first is “reporting back,” which includes sharing the results of the research with the community. This may occur in the form of a ceremony, providing copies of the work to participants that can be read out loud, or in the form of symposia or presentations in local community settings. The second way is by “sharing knowledge.” While it may feel easier to hand out a report and resist continued dialogue, it is the latter that can have the most meaningful impacts. Researchers should share the theories and analyses which have informed the work and engage in dialogue with the community about what was identified in the research. This process of sharing knowledge also encourages the researchers to be open to different perspectives and other ways of thinking about the research topic and to lean into the valuable knowledge that the community can share. Lastly, dissemination of research can also be a form of advocacy and action. DeBlaere and colleagues (2019) suggest that counseling psychologists can engage in dissemination to advance social justice and change public attitudes. For example, psychologists and students could write op-eds, use social media, develop toolkits focused on social justice, and conduct workshops in the community. Although op-eds have the ability to reach a broader audience, there continue to be challenges with disincentivizing scholars to engage in public writing (DeBlaere et al., 2019). Thus, institutional structures and training programs should consider ways to reward dissemination to the public and community (Buchanan et al., 2021). In conclusion, disseminating research can be a critical method in advocating for marginalized communities and to raise critical consciousness (Fassinger &amp; Morrow, 2013). 4.6 Summary, Conclusions, and/or Recommendations In summary, some key recommendations with respect to dissemination are to: 1. Encourage open science practices. 2. Establish the use of pre-prints as standard practice. 3. Increase the spotlight on open-access journals. 4. Promote publishing in practice-oriented journals that provide research findings in a digestible way. 5. Make it the norm to translate scholarly or academic works into plain language summaries or briefs. 6. Utilize social media to highlight articles that focus on decolonizing and uprooting systemic oppression within psychology. 7. Support reflexive thinking about articles spotlighted on list-servs. 8. Collectively push towards rewarding dissemination to the public and community. 9. Include practical recommendations when writing articles for scientific communities. 10. Hold presentations at schools, community clinics, or neighborhood events to disseminate research findings to a larger audience. 4.7 Suggestions for Practice, Further Learning, and/or Conversation From this chapter, we recommend some suggestions to further practice dissemination of research for both scientific audiences and to the public. For example, researchers may consider learning open science practices. The Open Science Framework (OSF) has resources about pre-registration and additional webinars and instructions with examples. Researchers may also benefit from translating a research article to a tangible product, such as a brochure, public health flyer, presentation to the community, Twitter/Instagram/Tiksok, op-ed, or social media post, depending on the audience. For example, resources such as this LinkedIn articleon using social media to promote research findings can help researchers translate their research findings to the public. Using interpersonal communication strategies, such as first person pronoun-rich captions, using selfies, and two-way conversations, can encourage greater conversations about science on social media (Martin &amp; MacDonald, 2020). Lastly, a systematic review about communicating science to the public suggests avoiding jargon, including citations, using neutral language, highlighting open science, being mindful about uncertainty, and being mindful about structuring texts (König et al., 2024). REFERENCES Anderson, C. B., Chang, S., Lee, H. Y., &amp; Baldwin, C. D. (2022). Mentoring Barriers, Expected Outcomes, and Practices in Scientific Communication: Scale Development and Validation. Journal of Career Development, 49(3), 697–713. https://doi.org/10.1177/0894845321991680 Ashcraft, L. E., Quinn, D. A., &amp; Brownson, R. C. (2020). Strategies for effective dissemination of research to United States policymakers: A systematic review. Implementation Science, 15(1), 89. https://doi.org/10.1186/s13012-020-01046-3 Baker, E. A., Brewer, S. K., Owens, J. S., Cook, C. R., &amp; Lyon, A. R. (2021). Dissemination Science in School Mental Health: A Framework for Future Research. School Mental Health, 13(4), 791–807. https://doi.org/10.1007/s12310-021-09446-6 Buchanan, N. T., Perez, M., Prinstein, M. J., &amp; Thurston, I. B. (2021). Upending racism in psychological science: Strategies to change how science is conducted, reported, reviewed, and disseminated. American Psychologist, 76(7), 1097–1112. https://doi.org/10.1037/amp0000905 Darwin Holmes, A. G. (2020). Researcher Positionality - A Consideration of Its Influence and Place in Qualitative Research - A New Researcher Guide. Shanlax International Journal of Education, 8(4), 1–10. https://doi.org/10.34293/education.v8i4.3232 De Rond, M., &amp; Miller, A. N. (2005). Publish or Perish: Bane or Boon of Academic Life? Journal of Management Inquiry, 14(4), 321–329. https://doi.org/10.1177/1056492605276850 DeBlaere, C., Singh, A. A., Wilcox, M. M., Cokley, K. O., Delgado-Romero, E. A., Scalise, D. A., &amp; Shawahin, L. (2019). Social justice in counseling psychology: Then, now, and looking forward. The Counseling Psychologist, 47(6), 938–962. https://doi.org/10.1177/0011000019893283 Fassinger, R., &amp; Morrow, S. L. (2013). Toward Best Practices in Quantitative, Qualitative, and Mixed- Method Research: A Social Justice Perspective. Journal for Social Action in Counseling &amp; Psychology, 5(2), 69–83. https://doi.org/10.33043/JSACP.5.2.69-83 Fecher, B., &amp; Friesike, S. (2013). Open Science: One Term, Five Schools of Thought. RatSWD Working Paper Series, 14. Garvey, W. D., &amp; Griffith, B. C. (1965). Scientific communication: The dissemination system in psychology and a theoretical framework for planning innovations. American Psychologist, 20(2), 157–164. https://doi.org/10.1037/h0021711 König, L. M., Altenmüller, M. S., Fick, J., Crusius, J., Genschow, O., &amp; Sauerland, M. (2024). How to communicate science to the public? Recommendations for effective written communication derived from a systematic review. Zeitschrift Für Psychologie, No Pagination Specified–No Pagination Specified. https://doi.org/10.1027/2151-2604/a000572 Lewis, N. A., &amp; Wai, J. (2021). Communicating What We Know and What Isn’t So: Science Communication in Psychology. Perspectives on Psychological Science, 16(6), 1242–1254. https://doi.org/10.1177/1745691620964062 Martin, C., &amp; MacDonald, B. H. (2020). Using interpersonal communication strategies to encourage science conversations on social media. PLOS ONE, 15(11), e0241972. https://doi.org/10.1371/journal.pone.0241972 Matsick, J. L., Kruk, M., Oswald, F., &amp; Palmer, L. (2021). Bridging Feminist Psychology and Open Science: Feminist Tools and Shared Values Inform Best Practices for Science Reform. Psychology of Women Quarterly, 45(4), 412–429. https://doi.org/10.1177/03616843211026564 Piwowar, H., Priem, J., Larivière, V., Alperin, J. P., Matthias, L., Norlander, B., Farley, A., West, J., &amp; Haustein, S. (2018). The state of OA: A large-scale analysis of the prevalence and impact of Open Access articles. PeerJ, 6, e4375. https://doi.org/10.7717/peerj.4375 Savin-Baden, M., &amp; Major, C. H. (2023). Qualitative Research: The Essential Guide to Theory and Practice. Routledge. https://doi.org/10.4324/9781003377986 Smith, L. T. (2021). Decolonizing Methodologies: Research and Indigenous Peoples. Bloomsbury Academic. Sommer, R. (2006). Dual dissemination: Writing for colleagues and the public. American Psychologist, 61(9), 955–958. https://doi.org/10.1037/0003-066X.61.9.955 "],["OpSci.html", "Chapter 5 Open Science as a Step Toward Social Responsivity in Research 5.1 Learning Objectives 5.2 Recommended Readings 5.3 Defining “Open Science” 5.4 Transparency of the Research Process 5.5 Access to the Scientific Literature 5.6 Tools for an Open Science 5.7 Summary, Conclusions, and/or Recommendations 5.8 Suggestions for Practice, Further Learning, and/or Conversation", " Chapter 5 Open Science as a Step Toward Social Responsivity in Research Lynette H. Bikos (she/her) &amp; Jamie Layton (she/her) Seattle Pacific University Seattle Pacific University was built on the unceded ancestral lands of the Duwamish people, a people that are still here, continuing to honor and bring light to their ancient heritage. You can learn more about the Duwamish tribe here. People who live and work on Duwamish land can pay rent here In 2015, the Open Science Collaboration demonstrated that psychological science suffers from a problem of replicability. This has turned the field of psychology’s attention toward the benefits that open science can provide. Engaging in practices such as preregistering research studies, sharing data, open peer review, open access, and utilizing open education resources can assist with issues such as transparency and access and, in turn, could lead to greater social and cultural responsivity in research. 5.1 Learning Objectives Learning objectives for this chapter include the following: Distinguish reproducibility from replicability. Identify mechanisms for increasing transparency in the research process. Describe potential benefits of preregistering research studies. List elements of data sharing that would improve reproducibility. Obtain an ORCID persistent digital identifier to help track your scholarly record. 5.2 Recommended Readings The following served as critical references in the development of this chapter. We encourage you to review them. Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), 943–943. https://doi.org/10.1126/science.aac4716 This was the study that called attention to the failures of replicability in psychological science. Given its historic and activating role in promoting open science in psychology, it is well worth reviewing the original document. Bosnjak, M., Fiebach, C. J., Mellor, D., Mueller, S., O’Connor, D. B., Oswald, F. L., &amp; Sokol, R. I. (2021). A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force. American Psychologist, 77(4), 602-615. https://doi.org/10.1037/amp0000879 This document creates a detailed argument for why preregistrations are necessary and it provides instructions for completing a preregistration for quantitative research. 5.3 Defining “Open Science” Broadly defined, open science is the movement to make scientific results and processes accessible and reusable by all. As such it involves dimensions of both principles or culture and practices or technology. Although Steven Mann has been credited with coining the phrase “open science” in 1998 when he registered the domain names openscience.com and openscience.org, an earlier use the phrase was found in 1985, by Daryl E. Chubin in the article, “Open Science and Closed Science: Tradeoffs in a Democracy” (“Open Science,” 2022). Curiously, Chubin opened their article by referencing framework that is commonly used in introductory research methods textbooks [e.g., Krathwohl (2009). That is: Merton’s norms of science. Merton (1942) argued that communism (i.e., common owership of information), universalism (i.e., universal standards for claims of knowledge), disinterestedness (i.e., integrity in gathering and interpreting data), and organized skepticism (i.e., critical review of scientific reports) were necessary for producing objective knowledge. Separated by more than four decades, Merton and Chubin (Chubin, 1985) discussed the role of science in a democracy. While further elaboration of their arguments and conclusions extend beyond the goals of this chapter, the norms of communism and organized skepticism are foundational to the current understanding of open science. Given the expansiveness of the notion of open science and its connectedness to other open initiatives (e.g., education, government, advocacy) numerous conceptual models may apply. Fecher and Friesike’s (2013) model organizes open science into five schools of thought. The public school of thought advocates that science needs to be accessible to a wider audience. Themes within this school include accessibility to the production of research (e.g., the citizen scientist) and consumability/accessibility to research results. The democratic school of thought is concerned with access to knowledge – especially when it is publicly funded. Subthemes within the democratic school emphasize that data be open for forseen and unforseen re-use and that there be open access to research publications. This is particularly true for government-funded research (e.g., the citizen should not be twice-taxed to access research findings). The pragmatic school of thought is concerned with efficiency, specifically utilizing online tools to allow for greater collaboration. The infrastructure school is specifically concerned with the technical infrastructure including software tools, applications, and computing networks. Finally, the measurement school of thought considers alternative standards to evaluating scientific impact of findings. For example this school challenges the “impact factor” (i.e., a measure of the average number of citations to an article in a journal), and suggests also counting (and publishing) elements like peer review and dissemination via emerging publishing formats. In light of these foundational inputs, we will approach open science with a simultaneous review of its principles and practices in the three broad areas of (a) transparency of the research process; (b) access to research findings; and (c) tools for open science. 5.4 Transparency of the Research Process Replicability (re-performing an experiment and collecting new data) and reproducibility [re-performing the same analysis with the same code using a different analyst; Patil et al. (2016)] are foundational to science. Yet investigations across scientific disciplines have demonstrated significant failures to both. Through its large scale, collaborative effort, the Reproducibility Project (Open Science Collaboration, 2015) revealed that psychological research often fails to replicate prior research. The collaborative selected 100 studies (97% of which had significant results) from three leading journals. Research teams committed to conducting high-powered, high-fidelity replications. Results were compared to the original studies with five metrics that assessed various dimensions of the analyses. Only 36% of the replications yielded statistically significant results; similarly a subjective analysis concurred that only 38% of the effects replicated the original results. Regarding effect sizes, 47% of the original effects were in the 95% confidence interval of the replication effect size. Finally, when original and replication results were combined (e.g., a meta-anlytic combination), 70% of the effects were statistically significant. When studies are replicated, there are many reasons that the outcomes may differ. There may have been small-to-large differences in design and methods that are impactful enough to change the result. There could be a Type I (false positive) or Type II (false negative) error. Or there could be confirmation biases at any (or multiple) stage(s) of the design (Stevens, 2017). It is these biases that are particularly problematic. Bias at the individual-study-level occurs when the researcher reports, out of the many possible analyses, the one(s) that provide the most consistent or significant results (Hengartner, 2018; van’tVeer &amp; Giner-Sorolla, 2016). Bias occurs at the journal level when reviewers and editors favor significant findings over non-significant ones (Stevens, 2017). Driessen et al. (2015) has suggested that there is a 25% reduction in the estimated effect of psychotherapy because 24% of all NIH-funded trials aimed at evaluating the efficacy of psychological treatment for major depressive disorder were never published. Dubbed the “file drawer problem” by Rosenthal (1979), this problem is compounded by researchers who are skeptical about the possibility of finding null results and the tendency of journals to not publish them. Across scientific disciplines, transparency has been suggested as a potential remedy (Open Science Collaboration, 2015; van’tVeer &amp; Giner-Sorolla, 2016). Stated another way, for research to be truly reproducible, the entire process must be open to scrutiny (Stevens, 2017). We review three practices that would lead to more transparent, open, science. These include preregistration, data sharing, and open peer review. 5.4.1 Preregistration Preregistration of a study involves specifying, in advance, the research questions, variables, research design and planned analyses (Stevens, 2017). There are a number of websites for such preregistrations; a common one for psychological scientists is the Open Science Framework. Researchers are encouraged to preregister studies that involve the testing of a priori hypotheses and models as well as studies that are intended as exploratory (Bosnjak et al., 2021). Although studies can be preregistered at any time, posting the preregistration in advance of seeing (or even more strictly, in advance of collecting) the data provides a mechanism for reviewers and consumers to evaluate the degree of consistency with which the research design and planned analyses were followed (Bosnjak et al., 2021; Haven et al., 2020). Scholars have noted a number of benefits to preregistration. The most formal preregistration is a registered report (OSF, 2022). Registered reports first appeared in 2012 in the journals Cortex and Perspectives on Psychological Science. In 2013, Social Psychology adopted the practice. As of 2022, 300 journals across a wide variety of disciplines invite (or require) registered reports (Chambers &amp; Tzavella, 2022). Whereas traditional publication in peer-reviewed journals involves submitting the paper for review after the study is completed, the registered report is submitted and reviewed – twice. During the first stage, authors submit a detailed research proposal specifying research questions, hypotheses, methods,and planned analyses. Proposals that successfully pass through the review and revision(s) phase receive an “in principal acceptance”, which commits the journal to publishing the final paper regardless of whether the hypotheses are supported. Once the research is completed, the authors submit the completed manuscript. Any deviations or additions to the protocol must be clearly identified. Substantial departures from the proposed analyses may result in a rejection at this second review. Because the purpose is to prioritize sound scientific practice over significant outcomes, if the authors followed the a priori specified protocol, the article should be published (Chambers &amp; Tzavella, 2022). By making the publication decision to accept-or-reject before the results are known, preregistered reports are an attempt to reduce biased research practices on the part of the researcher (e.g., HARKing [hypothesizing after the results are known], p-hacking, and selective reporting) and publication bias on the part of journals and reviewers (Chambers &amp; Tzavella, 2022; van’tVeer &amp; Giner-Sorolla, 2016). Not all journals invite registered reports. In these cases preregistration of studies is voluntary and researchers may wonder if preregistration is worth the time and effort. van’t Veer &amp; Giner-Sorolla (2016) have suggested that the detailed mapping involved in a preregistration may improve the overall quality of the study and that this, alone, is beneficial. Further, when reviewers and readers are aware that a study was preregistered, and that the plan was followed, the credibility of the findings may be enhanced. Not surprisingly, there is resistance to preregistration. Moore (2016) described three primary concerns. First, preregistration constrains flexibility, exploration, and serendipitous discovery. Second, the preregistration invites additional scrutiny to the research process. Third, preregistration is more work. Moore (2016) countered these concerns by suggesting that researchers are still free to engage in exploratory work. The preregistration itself is not limited to a priori hypotheses; researchers can specify their intentions to be purely exploratory or consider post-hoc analysis that follow the preregistered ones. Further, researchers can deviate from a preregistration; researchers are just expected to narrate how and why they did so. Regarding the concern of additional scrutiny, Moore argues that a study that followed a registered report or voluntary preregistration is likely to have greater credibility in the eyes of the reader or reviewer. Finally, Moore suggests that preregistration templates, specific to psychology, can streamline the process. As an example for quantitative empirical research in psychology, the PRP-QUANT Template was designed by the Preregistration Task Force (Force, 2021). The PRP-QUANT has three primary sections: an introduction, method, and analysis plan. Each section includes multiple items that are accompanied with brief instructions (Bosnjak et al., 2021). For qualitative researchers, a Delphi method (Haven et al., 2020) provides a 13-item, pregistration template that is freely available at the Open Science Framework. Both templates align with the Journal Article Reporting Standards[JARS; (American Psychological Association, 2020). 5.4.2 Data sharing Data sharing (including the releasing of raw data, measures, codebooks, and analytic scripts for data cleaning and analysis(Alter &amp; Gonzalez, 2018) is a second pathway to a more transparent science. Proponents of data sharing argue that it (a) increases scientific integrity through greater transparency and the increased probability of reproducibility (Martone et al., 2018), (b) optimizes the value of data and will accelerate scientific progress when data are exposed to secondary analysis or combined in meta-analyses (Ross et al., 2018), and (c) creates a structure for greater collaboration (Bezjak et al., 2018). Along with public and private entities, the U.S. Government contributes to data sharing when its agencies such as the Bureau of Labor Statistics, Department of Education, and Census Bureau offer data freely over the internet or restricted license. The concept of data sharing is not new. In 1983, an article in the American Psychologist called for a mandate for data sharing (Ceci &amp; Walker, 1983)). This was codified in the APA ethical principles as early as 1992. The current ethical principle (American Psychological Association, 2017) states that, “psychologists should freely share published data with peers requesting access for the purpose of verification or reanalysis.” The language in the ethics code implies a one-to-one (researcher-to-requester) relationship; the current calls from funders and proponents of open science are calling for broader access (Martone et al., 2018). Whether in an institutional repository, with the journal, or in an open source, collaborative, internet platform such as the Open Science Framework, data sharing involves archiving the data (along with descriptions and codebooks) and the record of the analysis (Stevens, 2017). At the time of this writing, funding entities are increasingly requiring that data be made available for use by others (Ross et al., 2018) and journals are, similarly, requiring or encouraging such practices. In contrast to the language in the APA ethical principles which suggest that sharing data should be “for the purpose of verification or reanalysis,” most proponents of open data encourage the researcher to license the data such that there are no restrictions on reuse or redistribution. Exceptions could be made to protect the identity of the human participants or special limitations or restrictions related to ethical concerns (Bezjak et al., 2018). Not surprisingly, there is resistance to data sharing. Beyond ethical considerations, common concerns include (a) being critiqued for analytical errors, (b) being “scooped” (i.e., someone else analyzes and publishes first), and (c) expending significant effort that will be unrewarded (Martone et al., 2018). Regarding the first concern of “being critiqued,” a core principle of open science is that all research results are available for challenge through reexamination, reanalysis, reproducibility, and replication (Alter &amp; Gonzalez, 2018). So, yes; being critiqued is a very possible consequence of sharing data. In contrast, “being scooped” is a less likely consequence. In microbiology, the original data creators tended to publish two years after the data were made available whereas other authors tended to publish five or more years after the data were made available (Martone et al., 2018). “Being scooped” may be less of a concern if researchers limit data sharing to the variables used in their analysis and if it is shared at the time of publication. Countering the concern about “unrewarded effort,” there are initiatives underway that would recognize the contributions of those who share data and analytic code. For example, the Association for Psychological Science has adopted the use of Open Science Foundation badges in its journal, Psychological Science. Further, evaluation of this project has suggested that a display of badges is correlated with significant increases in data sharing (Martone et al., 2018). Several have argued that shared data, analytic code, and preregistrations should be assigned persistent identifiers (e.g., the DOI, digital object identifier) and be treated as scholarly products. That is, they should be listed on the contributor’s curriculum vita and counted in professional evaluations such as promotion and tenure applications (Alter &amp; Gonzalez, 2018). There are also significant ethical concerns related to the protections and rights of research participants. In the U.S. it has been standard practice to (a) inform participants of the restricted purpose for which their data will be used and (b) assure participants that all the information they provide during the research study will (to the extent permitted by the law) be kept confidential, only to be viewed by members of the research team. When non-anonymous data are collected, the informed consent may also indicate that data will be de-identified. These standard practices are counter to the notion that data be shared broadly with researchers-in-general and could be used for purposes other than that which was stated in the informed consent (Ross et al., 2018). Data collected where informed consent forms contained these traditional practices should not be shared. However, for new data collection, researchers who wish to share data should inform potential participants that consent extends beyond the present study; further, the nature and intent of future uses of the data are unknown (Ross et al., 2018). The participants should also be informed about the types of identifiable private information that will be retained and the types of researchers who may have access to that information (Alter &amp; Gonzalez, 2018). Conveying this information to potential participants in a manner that provides them fully informed consent while also encouraging their participation in the study can be tricky. Here is some language we use on our informed consents: The information in the study records will be kept confidential. We do request your e-mail address so that we can send and link the quarterly surveys. Data will be stored securely and will be made available only to persons conducting the study unless you specifically give permission in writing to do otherwise. No reference will be made in oral or written reports that could link you to the study. Your de-identified data may be used in present and future (a) research collaborations, (b) scholarly/professional publications and presentations, and (c) in classroom teaching, projects, and demonstrations. Consistent with both journal/guild expectations and the ethical principles of open science, a fully anonymous and non-identifiable version of the response (i.e., dataset) may be posted online (e.g., to the APA-endorsed “Open Science Framework” (www.osf.io) or to the journal, submitted with the research article). This data may be reanalyzed for purposes that we cannot anticipate. No data posted will contain any information that could identify participants in any way, either directly or indirectly. All data will be thoroughly inspected by the Principal Investigators prior to posting to confirm that no participant-provided responses could inadvertently identify or expose a participant. Posting data (commonly referred to as “data sharing”) is necessary for reproducibility and replicability in science, allows peer reviewers and meta-analysts to check statistical assumptions, protects the field against data fraud, and is increasingly seen as an ethical obligation within psychological science. Even with updated and IRB-approved informed consent forms, data sharing can be problematic, however. When samples or specific cell frequencies are small, certain combinations of information (e.g., tenure status, department, gender) could render a row of data identifiable. Another risk is when the data are used in unexpected ways that result in harm to the individuals or community. Ross et al. (2018) shared the story of blood samples being collected from an Indigenous group for what was believed to be a diabetes study. Later it was discovered that the data had been shared with other researchers to study topics that brought social and psychological harms to the tribe as a whole. Further, data that is shared in a public repository will likely be available globally where laws and ethical conventions for using research data may vary. As chapter authors, we are generally proponents of open science. However, we urge researchers to give thoughtful consideration of plans for data sharing at the beginning of the project, to imagine intended and unintended consequences, and to seek IRB consultation and review. Further, data takes many different forms (e.g., qualitative, clinical interviews, survey data, geographical identifiers) and protection of the research participants will require different types of considerations before deciding if and how it can be shared (Ross et al., 2018). 5.4.3 Open peer review Open peer review is another avenue for increased transparency in the scientific process. Although the term is interpreted differently, there are two primary mechanisms: open identities and open reports (Ross-Hellauer, 2017). With open identities, neither the reviewers nor authors are anonymized. With open reports, the review reports are published alongside the relevant article (Ross-Hellauer, 2017). As journals and other outlets experiment with open peer review, there may be variations of one or more of these mechanisms. Some have argued that published open reports – which could be cited by others and counted as a scholarly product for evaluative activities such as tenure and promotion – might incentivize scholars to accept peer review assignments (which are usually completed with no compensation) and invest the time and energy necessary to provide a constructive critique of the work and formative feedback to the researchers (Bezjak et al., 2018). At the time of this writing, there is evidence of gradual movement toward open reviews. Some journals will now ask if the peer review can be transferred to another journal (if the manuscript is rejected) and if the reviewer’s name can be transferred with it. While this is neither open identity nor open reports, it is a small step in the direction of sharing the work of peer review. Additionally, many journals are now inviting peer reviewers to register with Publons, a commercial organization that provides a mechanism for collecting and summarizing scholarly impact as a peer reviewer. In creating this textbook, we engaged in a form of open review. It was important to us that each chapter be peer reviewed, yet we desired the peer review process to be non-anonymous, constructive, and formative (as opposed to anonymized, critical, and summative/gate-keeping). Along with a common rubric, drafts and revisions of each chapter were placed in a Google Docs folder. Two reviewers and the authors could access these materials at any time. Peer reviewers were asked to comment on each element of the rubric and leave suggested edits and comments/questions directly on the chapter draft. 5.5 Access to the Scientific Literature Another aspect of open science concerns access to the literature. There are multiple forms of open access publishing (Bezjak et al., 2018; Shah, 2017). Self-archiving is the process of placing a published version of an author’s article into institutional repositories or websites. Sometimes self-archived articles have an embargo period (months to years) that must elapse before the article becomes open access. Open access publishing is immediately, freely available, upon its publication. This level of access usually involves an article processing charge (APC), a one-time payment by the author. A third type is the hybrid article. This happens when a pay-walled journal offers individual open access articles. This usually requires the author to pay a fee that is higher than the APC associated with open access. If an article is accepted into a hybrid model, it means that in a single journal issue, readers will find both open access and paywalled articles. Traditional journals are often motivated to use this model because it fulfills funder policies such as requiring immediate public access to research. Not surprisingly, there are pros and cons to open access literature. Regarding pros, with no subscription fees, fees for individual articles, nor requirements to be associated with an institution who has access to the book or journal (Bezjak et al., 2018; Shah, 2017) open access materials are free to everyone, including in international contexts (e.g., low-middle income countries) where barriers to scientific literature may be greater. This benefits the potential readership as well as the author – in that there may be greater dissemination [and, in turn, citations; (Hagger, 2022). Another positive aspect of open access journals is an expedited submission-to-publication timeline (Shah, 2017). Alternatively, while the rapid turnaround for open access articles may be viewed as a positive attribute of open access journals, this is not always the case. Sometimes, legitimate open-access publishers feel forced into this accelerated submission-to-publication timeline in order to compete. As a result, they may weaken their peer-review process to meet this deadline (Beall, 2012). In addition, not all open access journals are well intentioned or legitimate. Predatory publishers cleverly spam researchers with calls-for-papers and fail to mention required authors fees that range from 1200 USD to 1800 USD. An author who learns of the APC after signing the contract (which generally includes surrendering the copyright) has lost the right to withdraw and is faced with paying the fee and losing the possibility to publish elsewhere – therefore, essentially losing their work. Another common concern of open access research is that while these journals and articles are free to readers, this doesn’t mean that readers are always able to locate these publication. A crucial piece in making articles accessible to readers is getting the journal indexed in as many relevant databases as possible (Fortney &amp; Murphy, 2016). APA PSYCH Info an example of a commonly used database in psychology. A well-indexed article is more likely to be discovered and read regardless of whether or not the reader is familiar with the journal itself. Regrettably, for those hoping to join the open science movement, getting journals indexed can be incredibly difficult. In some cases, it will take multiple attempts over several years before a journal is accepted into an index or database (Fortney &amp; Murphy, 2016; Shah, 2017). While indexing journals is historically challenging, PubMed and Wellcome Trust are among some of the databases working with APA to increase accessibility of open access psychology research (Martone et al., 2018). Finally, we think it is critical to raise equity concerns about the APC. Much of academia involves pressure to “publish or perish.” Open access affords those who can afford APC a wider choice of outlets for dissemination. Those without similar financial resources may be unable to logistically access these journals as an author and may need to rely on institutional support, grant support or the backlog of traditional journals. Thus, the APC associated with open access journals may further widen the equity gaps in tenure and promotion, making it easier for those with greater financial resources to accrue publications more quickly. 5.6 Tools for an Open Science Tools that support open science abound and are constantly evolving and emerging. We review six types of tools including: statistical software, reference management systems, persistent identifiers, data repositories, collaborative platforms, and open educational resources (OERs). Behind many of these tools is the notion of open source. Open source tools are freely shared and the code (or platform) that powers them can be modified and redistributed. 5.6.1 Statistical Software The R statistical software environment is an open source tool that includes features for commenting on code and enabling reproducible data analysis (Alter &amp; Gonzalez, 2018; Bezjak et al., 2018). Using R requires statistical training as well as fluency with the integration of base R, R Studio, and R packages. Because each of these elements are continuously updated, the R user must always adapt to changes in the underlying sourcecode that powers the analyses. The birth of R might be associated with the 1997 launch of the CRAN (Comprehensive R Archive Network). The CRAN hosts R’s executable files, source code, and packages contributed by the users (“R (Programming Language),” 2022). Mirrors are the network of ftp (file-transfer-protocol) and web servers around the world that store identical (hence, “mirror”) materials. There were only three mirrors in 1997; currently there are more than 100. As can be seen on the global list, they represent a variety of organizational types ranging from higher education to corporations to governments and nonprofits. Users are encouraged to select the CRAN that is geographically closest. R Studio is a public benefit corporation, that is, a statutory or government owned corporation whose mission is to provide free or subsidized services to the public (“RStudio,” 2022). The organization provides open-source (zero-cost) and commercial software that is intended to serve in a “virtuous cycle” for mutual improvement (“RStudio,” 2022). Some are wary of open source statistical software, voicing concerns of trustworthiness. It may be reassuring to learn that the most common way to install an R package is through the CRAN. Packages available on the CRAN must adhere to its repository policy and are vetted prior to posting. Further, those engaged in psychological science who may scour the internet for “how-to” tutorials and blogs will quickly learn that there are a number of reliable, commonly used R packages to conduct the analyses that are frequently discussed and critiqued. Further, resources such as the peer-reviewed Journal of Statistical Software provide in-depth coverage of many R packages. Because writing R script can be daunting, GUI (graphical user interface, point-and-click) alternatives are emerging that are open-sourced and, perhaps, easier to use. The R package, shiny, allows developers to build interactive tools known as “shiny apps.” These apps tend to perform limited functions. An example is Shoemann et al.’s (2017) Monte Carlo power analysis for indirect effects (https://schoemanna.shinyapps.io/mc_power_med/). More comprehensive software tools are also built with R code. For example, the program jamovi (project, 2021) was designed as an alternative to fee-for-use programs (e.g., SPSS) for the social sciences. Although the user points-and-clicks, a syntax mode allows the production of R syntax for inputting directly to R or for retrieval as a completely reproducible project. Although three individuals are credited as co-founding jamovi and their website lists additional team members (project, 2021) there is not information about its organizational status. The jamovi website seeks financial contributions and volunteers for advocacy, content creation, and module development. JASP (“just another statistics program,” Team, 2022) is another GUI that is commonly used in psychological science. Sponsored by the University of Amsterdam, JASP’s features include both frequentist and Bayesian analyses. Further, JASP produces APA-formatted tables that can be copy-pasted into word processing documents. Although it is a future goal of the developers, at this time, it is not easy to retrieve R code from JASP for reproducible archiving. Users of open-source software should include both text and reference list citations. Citing the software (a) provides a complete description of the method (contributing to reproducibility), (b) documents the usage and development of that software in the developer’s field, and (c) credits (in the form of a scholarly citation) the developer (A. M. Smith et al., 2016). 5.6.2 Reference management software Zotero (Digital Scholarship, 2022) is one example of an open source reference management program operated by the non-profit group, Corporation for Digital Scholarship. Zotero stores, manages, and cites bibliographic references. Zotero allows (a) cloud and locally held storage, (b) collaboration, (c) and full integration with word processing documents and R markdown files. Zotero’s open source nature makes it immediately responsive to change; merely three months after the introduction of the 7th edition of the APA style manual, Zotero upgraded the default style to match. Although a non-profit, Zotero does operate on a freemium model where basic services are free, but cloud storage (allowing synchronization across devices and collaborators) requires a subscription fee. Another popular tool that offers similar functionality is Mendeley. In 2007, Mendeley was founded in the U.K. by three doctoral students from Germany. In 2018 it was purchased by the academic publisher, Elsevier (“Mendeley,” 2022). 5.6.3 Persistent identifiers You may have noticed that most items in our reference lists have DOI (digital object identifiers) numbers. The DOI is an example of a persistent identifier (PID). PIDs are long-lasting digital references to objects, people, or organizations that serve to provide a reliable link from citations to the publication (or its source). There are two parts to PIDs. First, is the identifying alphanumeric string, itself. Second, is the organization or agency that commits to providing an infrastructure to ensure that a URL will map to the correct location of the object. Owing to name, organizational, and geographic changes, researchers can also become disconnected from their work. The ORCID (Open Researcher and Contributor ID) was created as an independent nonprofit organization to provide a unique, persistent identifier to researchers. The sponsoring organization has also created an infrastructure to ensure the reliability of these connections. When scholarly products include the authors’ ORCID, there can be a permanent and clear record of research activities. Many journals request ORCID for authors and co-authors at the time of submission; ORCIDs are also commonly requested of peer reviewers. 5.6.4 Data repositories Data repositories collect, maintain, and disseminate data over time (Alter &amp; Gonzalez, 2018). This is accomplished, in part, by providing a public facing citation and assigning persistent identifiers (e.g., DOI, digital object identifiers). Data repositories exist across a number of institutions and platforms. For example colleges and universities may archive theses and dissertations. Other data repositories are more disciplinary-specific. 5.6.5 Collaborative platforms Collaborative platforms are online services that provide a virtual environment where multiple people can connect and work on the same task (Bezjak et al., 2018). If you have used Google Docs, Dropbox, or the Microsoft packages such as OneDrive or Sharepoint, you have used a collaborative platform. While these are terrific tools for word processing, spreadsheets, and slide presentations, they may less helpful in co-authoring statistical code. GitHub (“GitHub,” 2022b) is an example of a cloud-based, collaborative platform, that specializes in hosting code (e.g., R code) where multiple people can contribute, track, and control changes to the code. GitHub is also host to numerous open source projects. For projects that are openly licensed, others can “fork” (i.e., copy the project for yourself and make changes to it without altering the original) the project. Additionally, so long as the secondary user credits the author/developer, they can use, revise, remix, and further distribute its contents. Initially a start-up business, Microsoft purchased GitHub in 2018 GitHub (2022a). GitHub’s basic features are free for individuals and organizations; there are more advanced tools for a fee. Our OER is produced and hosted on GitHub and GitHub pages for zero cost. The Open Science Framework (OSF, 2022) was created by the not-for-profit Center for Open Science and is a collaborative infrastructure to support the entire research cycle. Across multiple disciplines, including psychology, researchers can preregister studies and use the same project for sharing data and analytic code (as well as literature, IRB materials, experimental materials, presentations, preprints). OSF also offers the capacity to connect with other systems such as GitHub, Google Docs, and Dropbox. The GitHub and OSF are only two examples. Other commonly used collaborative platforms include Zenodo and Figshare (Martone et al., 2018). 5.6.6 Open education resources Open education resources (OERs) include any tools, materials, or techniques that are used to support access to knowledge (Bezjak et al., 2018). While not specific to open science, there is considerable overlap in the values and resources that contribute to them and emerge from them. This very textbook was created using the open source software R and R Studio. The primary packages used to format the content are R Markdown and Bookdown. The book is hosted in GitHub and is rendered to the internet via GitHub Pages. As described in the preface, the OER holds a CC BY-SA 4.0 license allowing the user to share (copy and redistribute the material in any medium or format) and adapt (remix, transform, and build upon the material for any purpose). The license requires that proper attribution is made (e.g., appropriate credit, a link to the license, and indication of changes were made) and that your redistribution must use the same CC BY-SA 4.0 license. 5.6.7 Bearing the Costs of “Open” Our attention to “who owns” open tools may be a surprising inclusion in a chapter on open science. We were intentional in providing this information because the who of ownership and the type of business model may have bearing on the trustworthiness with which researchers’ tools, materials, and data may be protected and maintained. Further, as the popularity of open tools increase (and decrease) there is the hope that tools will improve, the risk that the tool will be ignored or deprecated, and the possibility that use fees will be added or increased. Our only recommendation is that potential users take some time to become familiar with open tools before committing to use them. 5.7 Summary, Conclusions, and/or Recommendations Researchers have demonstrated that psychological science is threatened by problems of reproducibility and replicability (Open Science Collaboration, 2015). Through preregistration, data sharing, and open peer review, proponents of open science have identified pathways to increase the transparency of the scientific process. Ensuring open access to research findings remains problematic. Many findings are behind pay walls (restricting their access) and open access journals often charge fees that are prohibitive for new scholars and those groups who experience marginalization. More work will be needed to solve this access. In the meantime, tools to facilitate open science are abundant, emerging, and constantly improving. We encourage emerging researchers to participate in open science. Further, as it continues to develop, we encourage all to be vigilant so that the new practices are not co-opted in ways that maintain historic and inequitable power structures and privileges. 5.8 Suggestions for Practice, Further Learning, and/or Conversation Obtain an ORCID iD for yourself Find a published study that had been preregistered. Trace it to its preregistered location. Is there also access to data, analytic code, and other materials? How easy was it to locate? How easy would it be to reproduce the study? Download one of the preregistration templates (either quantitative or qualitative) and preregister a study of your own. REFERENCES Alter, G., &amp; Gonzalez, R. (2018). Responsible practices for data sharing. American Psychologist, 73(2), 146–156. https://doi.org/10.1037/amp0000258 American Psychological Association. (2017). Ethical principles of psychologists and code of conduct. https://www.apa.org/ethics/code American Psychological Association. (2020). Publication manual of the American Psychological Association: The official guide to APA style. (Seventh edition.). American Psychological Association. Beall, J. (2012). Predatory publishers are corrupting open access. Nature, 489(7415), 179–179. https://doi.org/10.1038/489179a Bezjak, S., Clyburne-Sherin, A., Conzett, P., Fernandes, P., Görögh, E., Helbig, K., Kramer, B., Labastida, I., Niemeyer, K., Psomopoulos, F., Ross-Hellauer, T., Schneider, R., Tennant, J., Verbakel, E., Brinken, H., &amp; Heller, L. (2018). Open Science Training Handbook. Zenodo. https://doi.org/10.5281/ZENODO.1212496 Bosnjak, M., Fiebach, C. J., Mellor, D., Mueller, S., O’Connor, D. B., Oswald, F. L., &amp; Sokol, R. I. (2021). A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force. American Psychologist. https://doi.org/10.1037/amp0000879 Ceci, S. J., &amp; Walker, E. (1983). Private archives and public needs. American Psychologist, 38(4), 414–423. https://doi.org/10.1037/0003-066X.38.4.414 Chambers, C. D., &amp; Tzavella, L. (2022). The past, present and future of Registered Reports. Nature Human Behaviour, 6(1), 29–42. https://doi.org/10.1038/s41562-021-01193-7 Chubin, D. E. (1985). Open Science and Closed Science: Tradeoffs in a Democracy. Science, Technology, &amp; Human Values, 10(2), 73–81. http://www.jstor.org/stable/689511 Digital Scholarship, C. for. (2022). Zotero Your personal research assistant. https://www.zotero.org/ Driessen, E., Hollon, S. D., Bockting, C. L. H., Cuijpers, P., &amp; Turner, E. H. (2015). Does Publication Bias Inflate the Apparent Efficacy of Psychological Treatment for Major Depressive Disorder? A Systematic Review and Meta-Analysis of US National Institutes of Health-Funded Trials. PLOS ONE, 10(9), e0137864. https://doi.org/10.1371/journal.pone.0137864 Fecher, B., &amp; Friesike, S. (2013). Open Science: One Term, Five Schools of Thought. RatSWD Working Paper Series, 14. Force, P. T. (2021). Preregistration Standards for Psychology - the Psychological Research Preregistration-Quantitative (aka PRP-QUANT) Template. https://www.psycharchives.org/en/item/088c79cb-237c-4545-a9e2-3616d6cc8453 Fortney, K., &amp; Murphy, L. S.-L. (2016). Getting Found: Indexing and the Independent Open Access Journal. Western Journal of Emergency Medicine, 17(5), 508–510. https://doi.org/10.5811/westjem.2016.6.30836 GitHub. (2022b). In Wikipedia. https://en.wikipedia.org/w/index.php?title=GitHub&amp;oldid=1097996266 Hagger, M. S. (2022). Developing an open science “mindset.” Health Psychology and Behavioral Medicine, 10(1), 1–21. https://doi.org/10.1080/21642850.2021.2012474 Haven, T., Errington, T. M., Gleditsch, K., Grootel, L. van, Jacobs, A. M., Kern, F., Piñeiro, R., Rosenblatt, F., &amp; Mokkink, L. (2020). Preregistering Qualitative Research: A Delphi Study. SocArXiv. https://doi.org/10.31235/osf.io/pz9jr Hengartner, M. P. (2018). Raising Awareness for the Replication Crisis in Clinical Psychology by Focusing on Inconsistencies in Psychotherapy Research: How Much Can We Rely on Published Findings from Efficacy Trials? Frontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.00256 Krathwohl, D. R. (2009). Methods of Educational and Social Science Research: The Logic of Methods, Third Edition. Waveland Press. Martone, M. E., Garcia-Castro, A., &amp; VandenBos, G. R. (2018). Data sharing in psychology. The American Psychologist, 73(2), 111–125. https://doi.org/10.1037/amp0000242 Mendeley. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Mendeley&amp;oldid=1095492319 Merton, R. K. (1942). Science and technology in a democratic order. Journal of Legal and Political Sociology, 1, 115–126. Moore, D. A. (2016). Preregister if you want to. American Psychologist, 71(3), 238–239. https://doi.org/10.1037/a0040195 Open science. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_science&amp;oldid=1087027147 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), 943–943. https://doi.org/10.1126/science.aac4716 OSF. (2022). Open Science Framework. https://osf.io/ Patil, P., Peng, R. D., &amp; Leek, J. T. (2016). A statistical definition for reproducibility and replicability (p. 066803). bioRxiv. https://doi.org/10.1101/066803 project, T. jamovi. (2021). About - jamovi. https://www.jamovi.org R (programming language). (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=R_(programming_language)&amp;oldid=1098779048 Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 Ross, M. W., Iguchi, M. Y., &amp; Panicker, S. (2018). Ethical aspects of data sharing and research participant protections. American Psychologist, 73(2), 138–145. https://doi.org/10.1037/amp0000240 Ross-Hellauer, T. (2017). What is open peer review? A systematic review (No. 6:588). F1000Research. https://doi.org/10.12688/f1000research.11369.2 RStudio. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=RStudio&amp;oldid=1099226518 Shah, D. (2017). Open Access Publishing: Pros, Cons, and Current Threats. Marshall Journal of Medicine, 3(3), 1. https://doi.org/http://dx.doi.org/10.18590/mjm.2017.vol3.iss3.1 Shoemann, A. M., Boulton, A. J., &amp; Short, S. D. (2017). Determining power and sample size for simple and complex mediation models. Social Psychological and Personality Science, 8, 379–386. https://schoemanna.shinyapps.io/mc_power_med/ Smith, A. M., Katz, D. S., &amp; Niemeyer, K. E. (2016). Software citation principles. PeerJ Computer Science, 2, e86. https://doi.org/10.7717/peerj-cs.86 Stevens, J. R. (2017). Replicability and Reproducibility in Comparative Psychology. Frontiers in Psychology, 8. https://doi.org/10.3389/fpsyg.2017.00862 Team, J. (2022). JASP (Version 0.16.3). https://jasp-stats.org/faq/ van’tVeer, A. E., &amp; Giner-Sorolla, R. (2016). Pre-registration in social psychology—A discussion and suggested template. Journal of Experimental Social Psychology, 67, 2–12. https://doi.org/10.1016/j.jesp.2016.03.004 "],["APAstyle.html", "Chapter 6 APA Style: Critical Considerations 6.1 Learning Objectives 6.2 Readings &amp; Resources 6.3 Abbreviations in this Chapter 6.4 The Culture of APA Style 6.5 The JARS: The Core of APA Style 6.6 What the Style Manual Has to Say About… 6.7 Revisiting the Culture of APA Style 6.8 Suggestions for Practice, Further Learning, and/or Conversation", " Chapter 6 APA Style: Critical Considerations Lynette H. Bikos (she/her) &amp; Kiana Clay (she/her) Seattle Pacific University Seattle Pacific University was built on the unceded ancestral lands of the Duwamish people, a people that are still here, continuing to honor and bring light to their ancient heritage. You can learn more about the Duwamish tribe here. People who live and work on Duwamish land can pay rent here Chapter Status: Under Review There are many excellent resources for learning APA Style. This chapter is not one of them. Rather, the intent of this chapter is to locate APA Style within health services psychology. In-so-doing, we hope to convey why it is valued and how adherence to the guidelines presented in the style manual can facilitate an effective and efficient transmission of ideas, information, and scientific findings. Simultaneously, we critically examine APA Style and question whether and how unquestioned allegiance to it may contribute to exclusion and oppression. While we do not teach APA style, we review the journal article reporting standards (JARS) by comparing the recommendations to a recently published article. We also provide an overview of the sometimes surprising topics that are covered in the Style Manual. 6.1 Learning Objectives Learning objectives from this lecture include the following: List and define three cultural characteristics or values reflected in APA style. Identify two ways that Thompson (2004) has suggested that APA style perpetuates Whiteness and patriarchy in the academy. Describe how adherence to the JARS facilitates effective and efficient transmission of information. Evaluate a manuscript for JARS. 6.2 Readings &amp; Resources In preparing this chapter, we drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list. American Psychological Association. (2020). Publication manual of the American Psychological Association: The official guide to APA style. (Seventh edition.). American Psychological Association. This resource is essential for writing many types of papers within the discipline of psychology. Not only does it provide instruction on reference lists, but it also also provides guidance on how to write effectively, how to reduce bias in language, and how to structure research papers in ways that convey information efficiently and effectively. Madigan, R., Johnson, S., &amp; Linton, P. (1995). The language of psychology: APA style as epistemology. American Psychologist, 50(6), 428–436. https://doi.org/10.1037/0003-066X.50.6.428 Madigan et al. (1995) argued that as we learn APA style we are inculcating the professional values of our discipline (and we do this without awareness). Thompson, A. (2004). Gentlemanly Orthodoxy: Critical Race Feminism, Whiteness Theory, and the APA Manual. Educational Theory, 54(1), 27–57. https://doi.org/10.1111/j.0013-2004.2004.00002.x Critiquing the 5th edition of the style manual (at the time of this writing we are using the 7th Edition). Thompson (2004) pointed out how aspects of APA style contribute to preserving Whiteness. White Supremacy Culture. (n.d.). DRworksBook. Retrieved August 8, 2020, from https://www.dismantlingracism.org/white-supremacy-culture.html Identifies characteristics of White Supremacy Culture in organizations (often used to describe academia). Huang, Y.-T., &amp; Chan, R. C. H. (2022). Effects of sexual orientation concealment on well-being among sexual minorities: How and when does concealment hurt? Journal of Counseling Psychology. https://doi.org/10.1037/cou0000623 We use this article as a tool for understanding where and how the JARS requirements are implemented into a research article. 6.3 Abbreviations in this Chapter In this chapter on APA Style we use three abbreviations. Because each is represents a larger, proper title, we are capitalizing them as if they were proper nouns: The “Style Manual” refers, generally, to the Publication Manual of the American Psychological Association (all editions) and the cultural expectation of using its conventions. The “7th Edition” refers to the most recent edition of the Publication Manual of the American Psychological Association (American Psychological Association, 2020). “APA Style” refers more broadly to the culture and use of the Style Manual. 6.4 The Culture of APA Style The Publication Manual of the American Psychological Association has grown from its 7-page writers guide in the Psychological Bulletin (1929) to 427 in the 7th Edition (American Psychological Association, 2020). Many who use the Style Manual are looking up guidelines for formatting parts of a paper that requires APA style, locating examples of resources to learn how to provide accurate citations, or consulting the journal article reporting standards (JARS) to learn what information is required in an empirical manuscript. Madigan et al. (1995) has suggested that within the individual who seeks to master APA Style and across the near-century that of Style Manual’s existence, the Style Manual has served an epistemological function. That is, through learning and practicing the guidelines, it has instilled attitudes and values, many of which are unarticulated and operate outside of our explicit awareness. 6.4.1 APA Style as Epistemology Madigan (1995) and colleagues compared APA style to that of other disciplines (e.g., history, literary criticism) and identified characteristics that uniquely define our writing. First, we use a story schema that consists of a prescribed introduction, method, results, and discussion. On the face of it, this seems straightforward – even linear. However research is messy and this messiness may not be reflected well (or at all) in the final paper. Within this story arc, readers and reviewers who are familiar with APA Style can expect to see a (a) review of past empirical studies, (b) collection and analysis of new data, and (c) connections between conclusions and psychological theory. A second characteristic is the depersonalized language of disagreement that focuses on the method, analyses, and conclusions and not on the investigators as individuals (Madigan et al., 1995) . An example might be a statement like, “The relatively small sample likely contributed to Type I error.” Madigan and colleagues suggested that a goal of psychological science is a collaborative, cumulative endeavor based on research data that avoids animosity and confrontation and that the culture of APA style facilitates this conveyance. Related to this is the use of hedged conclusions. Hedged words like “tend,” “suggest,” and “may” are abundant in our writing. Further, we almost always claim that “more research is needed” before we can assert firm conclusions. APA Style involves other people (Madigan et al., 1995). Our papers are more likely to have multiple authors (than single authors). In fact, the 7th Edition instructs writers to list, by name, up to 20 authors in a reference list entry (when there are 21 or more, an elipses follows the first 19 and then lists the last author). This is an increase from prior editions and serves something of permission to extend authorship to those who contributed to the project. Relatedly, our introduction and discussion sections include abundant citations that locate the research within a proximal body of literature. Rather than footnotes or endnotes, we use in-text references that include the last names of authors; this provides immediate information about the “who” of the research. Madigan (Madigan et al., 1995) and colleagues also documented distinct writing conventions that define the culture of our writing. For example, we tend to use headings and subheadings rather than narrative transitions; we are more likely to paraphrase than use direct quotes; and we use self-effacing and low-profile language that “does not call attention to itself.” These approaches maintain the focus on the topic of the paper and not the personhood of the author. 6.4.2 APA Style, Whiteness, and Patriarchy Nearly a decade after Madigan et al.’s paper (1995), Thompson (2004) examined APA style (i.e., the 5th edition) through the lens of critical race feminism and Whiteness Theory and argued that the expectations regarding clarity, precision, appropriateness, sensitivity, and objectivity likely contribute to the academy’s investments in Whiteness and patriarchy. Thompson’s analysis of power and property investments was organized into five themes that continue to be relevant for the 7th Edition. The first theme is property rights. Predominantly White institutions (PWIs) have treated refereed scholarship as intellectual property, demarcated with a certain, privileged status. That is, individuals “own knowledge.” Thompson (2004) cited evidence for this in the conventions of: Using last names as a shorthand reference for work (e.g., “Thompson’s [1995] ctitique of APA style”). Citing one’s own work (e.g., “Bikos [2021] has argued…) rather than writing, “When I previously claimed…” Using et al. for in-text citations when articles have three or more authors. Ordering authorship on the basis of relative contribution and not professional position, power, or hierarchy (see section 1.22 in the 7th Edition). Thompson (2004) also suggested that power dynamics (especially around race and gender) likely interferes with adherence to this principle. Thompson’s (2004) concern about the proprietariness of knowledge for individuals is that it provides no recognition for the communities from where the the knowledge was gathered. After all, where is stylistic guidance for citing community or institutional knowledge? For example, data can be obtained from and about the Black Church, but there is no guidance from the Style Manual on how to cite such knowledge. There is only guidance on citing others who have also written about it. A second theme is precedent and pedigree. Our disciplinary commitment to the notion that knowledge is cumulative is reflected in voluminous citations of relevant earlier works. These include citations of the gurus, citations of statements that may seem obvious, and self-citations. Including these citations in the introductory sections of a research paper locates the proposed study within a body of knowledge and provides the requisite justification for its inclusion within the lineage of the research topic. This practice disadvantages novel ideas and excludes ideas and projects that would challenge existing power structures. Proceduralism, the third theme, refers to the constriction of communication via the standardized format and style (Thompson, 2004). Regarding format, Thompson suggested that the Introduction, Method, Results and Discussion sections may be insufficient to capture the lived experiences of marginalized and oppressed groups. Regarding stylistic conventions, the Style Manual’s prescriptions for elegant writing and unambiguous clarity may not work for all groups, especially when information does not fit well into clean categories. The standardization of the et al. convention contributes to both gender- and color-blindness. On one hand, it is “fair.” On the other hand, the use of the first author’s surname obscures their identity and the identities of coauthors. Finally, APA Style discourages footnotes. We agree with Thompson’s claim that the most juicy arguments are always in the footnotes. Following professional protocol means avoiding extremes and blending in. In this fourth theme, Thompson (2004) notes that when one adheres to the protocol, they signal a recognition of the prevailing relations of power, authority, and legitimacy. The 7th Edition expanded what was previously a recommendation to avoid using insensitive and pejorative language, to an entire chapter on bias-free language. We agree with Thompson’s pre-7th Edition clasim – that such recommendations are admirable, but they do not address unequal power relations. Further, distinctions between what is insensitive and pejorative may be undetectable to those who hold privilege. The fifth theme was the notion of a gentleman’s agreement; that is, a “language that conveys professionalism and formality” where “differences should be presented in a professional and noncombative manner” (7th Edition, section 4.7). Thompson (2004) is concerned that while this is offered with the hope of pluralism and the creation of safe spaces, it causes controversies to be ignored or dismissed and may bolster complicity in racism. 6.4.3 APA Style and White Supremacy Culture For more than 20 years, Tema Okun (Okun, 2021; “White Supremacy Culture,” n.d.) and colleagues have curated a list of characteristics of White supremacy culture. The Centre for Community Organizations has prepared a handout that provides definitions, descriptions, examples, and antidotes for each of the White supremacy culture characteristics as they occur in organizations. With Madigan (1995) and Thompson’s (2004) conceptualizations in the foreground, we are using this chapter to ask the question, “How might APA Style reflect and further contribute to White Supremacy Culture?” Before we move onto an deeper investigation of some of the essentials of APA Style, we encourage you to follow the link to the definitions and review them. Then, as we present the information of APA Style, note where you notice overlap. We will return to this examination at the end of the chapter. Perfectionism Perfectionistic culture Worship of the written word Only one right way Either/or thinking Concentration of power Power hoarding Paternalism Defensiveness Right to comfort Fear of open conflict Individualism I’m the only one Progress is more/bigger Objectivity Quantity over quality Sense of urgency 6.5 The JARS: The Core of APA Style The JARS [Journal Article Reporting Standards; Appelbaum et al. (2018)] were born out of a concern for transparency in psychological science. When researchers present their work using these comprehensive, uniform reporting standards, it makes it easier for readers and reviewers to work their way through individual papers, compare research, and use the results in meta-analyses (American Psychological Association, 2020). Thus, the JARS provides an interconnected set of tables that list information that should be reported in prescribed sections of empirical manuscripts. The JARS does not specify in what order that the information should be presented – just that it should be available to the reader at the time they need it, and written such that does not interfere with the readability of the paper. The JARS were first introduced in a 2008 feature in the American Psychologist (“Reporting Standards for Research in Psychology,” 2008) and were included in the 6th Edition of the Style Manual. An updated JARS, published in 2018, expanded the types of quantitative research (JARS-Quant) as well as introduced tables for qualitative (JARS-Qual)and mixed methods studies (JARS-Mixed). Chapter 3 of the 7th Edition is devoted to the JARS. It contains numerous tables, definitions, explanations, and a flowchart. Curiously, the second chapter of the 7th Edition provides another overview of each section of an APA Style manuscript. The information provided in the second chapter has a high degree of overlap and consistency with the JARS but focuses more on formatting conventions. We encourage authors to consult both. A third set of guidelines for authors are found in individual journals. These guidelines may have inconsistencies with APA Style. When submitting to a specific journal, it is that journal’s guidelines that likely take precedent. Given that the JARS and Chapter 2 recommendations, should be directly consulted when drafting an APA Style paper, we will not repeat them here. We thought, however, that it might be a useful exercise to review a recently published article, mapping if and where each of the JARS elements are included. We have chosen Yu-Te Huang and Randolph C. H. Chan’s (2022), “Effects of Sexual Orientation Concealment on Well-Being Among Sexual Minorities: How and When Does Concealment Hurt?” published in the Journal of Counseling Psychology. We strongly encourage you to work along with us by accessing a copy of the JARS either in the 7th Edition or online and identifying (by highlighting) the elements in Huang and Chan’s paper. Utilization of the JARS should begin with its flowchart. Huang and Chan’s (2022) research is quantitative in nature. The flowchart indicates that all quantitative studies should start with JARS-Quant Table 1. Because the study did not involve an experimental manipulation, the write-up should also adhere to JARS-Quant Table 3. In the table below, we paraphrase the JARS instructions, provide a quick summary of if-and-how Huang and Chang addressed it, and then provide a “+”, “-”, or “+/-” designation to indicate if it was present, absent, or partially addressed. Table 1 JARS Element Location in Huang and Chan (2022) +/- Title Huang and Chan (2022) +/- Main variables and theoretical issues and their relationship Primary IV (concealment) and DV (well-being) were named. Asking “how” and “when” suggests that mechanisms (mediation, moderation) will be evaluated + Population(s) studied No indication that the participants were from Hong Kong - Author Note Huang and Chan (2022) +/- Acknowledgement and explanation of special circumstances (e.g., preregistration, use of data in other publications, if primary data is used in dissertation or conference papers, sources of funding/support, potential conflicts of interest, affiliation of authors if different than author byline, contact information for corresponding author, other relevant information) Provides ORCiDs. Identifies funding source. Addresses prior presentations, publications, conflicts of interest. Lists contact information. + Abstract (120 to 250 words) Huang and Chan (2022) +/- Problem under investigation including primary hypotheses Defines problem under investigation. Although not specified as such, the hypotheses are reflected in an efficient presentation of results. + Key participant characteristics N = 737 sexual minority individuals. Lists gender and age. + Method: research design, sample size, materials used, outcome measures, data-gathering procedures Participants recruited through LGBTQ networks for baseline and 1-year follow-up + Findings: primary statistical output Results are narrated, but no statistical output is included - Conclusions: results plus implications or applications Results pointed to the importance of family and loneliness + Introduction Huang and Chan (2022) +/- Problem statement connecting to theoretical and/or practical implications Opening paragraph provides a problem statement. Project is grounded in self-determination theory. + Succinct review of relevant scholarship – connecting to extant literature Organized into subheadings with a distinction between identity diclosure/nondisclosure and concealment. + Hypotheses (primary and secondary), aims, and objectives, including theoretical derivation and additional planned analyses Three hypotheses plus and exploratory question were listed. + Method Huang and Chan (2022) +/- Inclusion and exclusion criteria Listed in Participants section + Participant characteristics: standard demographic and topic-specific Participants section includes summary of general (e.g., age, education) and topic-specific variables. + Sampling procedures addressing the following (if used): sampling plan, response rate, self-selection, settings and dates, participant agreements/compensation, and IRB and safety monitoring. Procedures section describes recruitment platforms, voluntary nature, consent process. Participants section describes attrition across the two waves. + Sample size, power, and precision: intended and achieved sample size, power analysis, use of interim analyses or stopping rules Participants section identifies how the minimum sample size was determined. + Measures and covariates: all primary and secondary measures and covariates (including those not utilized in report) Measures section lists the variables and tools used to measure them. Preliminary analysis lists covariates (demographic variables); the tools for assessing them were not described. There is no mention of additional, unused data. +/- Data collection: methods used to collect data Procedures indicates that an online survey platform was used and describe the invitation, consent, follow-up protocol. + Quality of measurements: Methods used to enhance quality of measurements including training and reliability of data collectors and use of multiple observations NA since surveys collected online. NA Instrumentation: review of psychometric and biometric properties Measures provides descriptions of the instruments used, including citations related to their psychometric properties, and study-specific internal consistency coefficients for four of the five instruments. + Masking: In experimental manipulations, describe and evaluate presence of masking techniques (if used) NA NA Psychometrics: reliability and validity estimates for instruments within the reported study and related studies or test manuals Measures provides descriptions of the instruments used, including citations related to their psychometric properties, and study-specific internal consistency coefficients for four of the five instruments. + Conditions and design: since this is non-experimental, it directs to JARS Table 3 NA; a quick review of Table suggests that meeting the criteria has already been addressed NA Data diagnostics: planned data diagnostics including criteria for post-data-collection exclusion, description of managing missing data, definition and processing of statistical outliers, analyses of distributional characteristics, data transformations (if used) Results/Preliminary Analysis reviews the screening for normality and provides descriptive and correlational results. Independent t tests and one-way ANOVAS were used to identify differences in the primary variables and in missingness as a function of demographic variables. MNAR (missing not at random) was assumed and FIML (full information maximum likelihood) was used in primary analyses. + Analytic strategy: for primary, secondary, and exploratory hypothesis; include plans to protect against experiment-wise error There was not a separate section in the Method to first outline the analytic strategy. Rather, detailed descriptions of the analytic process were presented in the Results +/- Results Huang and Chan (2022) +/- Participant flow: Include n of each group at each stage of study; consider including a figure Method/Participants details the N from initial interest through the second wave. + Recruitment: Dates defining periods of recruitment and follow-up A timeline regarding Time1 and Time 2 was provided; dates of collection were not provided. +/- Statistics and data analysis: the JARS provides detailed information about what to include regarding missing data and analysis, descriptive data, inferential statistics, and complex statistics; additionally, problems with statistical assumptions, data distributions, convergence are to be reported Subsections in the Results organize the primary and exploratory hypotheses. While they did not review plans to to protect against experiment-wise error, they reported B weights, confidence intervals, and fit statistics. +/- Discussion Huang and Chan (2022) +/- Support (or nonsupport) of original hypotheses: distinguish between primary, secondary, exploratory; consider implication of exploratory as it relates to error rates Although the opening paragraphs of the Discussion summarize the findings, they do not identify them as hypotheses or exploration. +/- Similarity of results to findings and work of others The mid-section of the Discussion compares findings to prior literature. + Interpretation of results, considering validity threats, sample size, measurement protocols Study Limitations considers threats to internal validity and the measures that were used. + Generalizability taking into consideration target population and contextual issues Study Limitations considers threats to external validity, particularly around self-selection into the study. + Implications for future research, program, policy Practical Implications provide guidance to counseling psychologists when working with sexual minority clients. + 6.6 What the Style Manual Has to Say About… It can be surprising to learn that the Style Manual addresses issues beyond the order, content, and conventions of a manuscript. With explicit reference to the APA Ethical Principles of Psychologists and Code of Conduct [APA Ethics Code; American Psychological Association (2017)], the very first chapter of the 7th Edition includes a review ethical, legal, and professional standards in publishing. Chapter 1 in the 7th Edition reviews the importance of ensuring the accuracy of scientific findings, protecting the rights and welfare of research participants, and protecting intellectual property rights. Given that most of these topics are covered elsewhere in this OER, we wanted to review only one issue, protecting intellectual property rights, because it addresses a sometimes sticky issue: publication credit and authorship order. 6.6.1 Authorship Both the 7th Edition (section 1.21) and the APA Ethics Code (standard 8.12a) state that authorship is reserved for those who make a “substantial contribution.” The list of contributions that are substantial include formulating the research question, designing the research project, organizing and conducting the analysis, or interpreting the findings and result. Lesser contributions (e.g., entering data, recruiting subjects) on their own, do not garner authorship, but might warrant authorship if the potential author was involved in a number of these tasks. Once the list of authors is determined, the next task is to determine their order. The 7th Edition (section 1.22) states that the general rule for determining author order is that the principal contributor appears first, with subsequent names appearing in order of decreasing contribution. There are some traditions where the principle investigator appears last. When authors play equal roles, this can be stated in the author note. The Style Manual acknowledges that hierarchy has the potential to complicate the determination of authorship and its order. Relative status (e.g., department leader, tenure, promotion, student) should not determine the order of authorship. Further, given that a student’s dissertation is considered to be an independent project, it should be rare that a faculty member holds the position of first author. This can happen, though, when the dissertation is part of a collection of studies that are published together, the student uses data from from an existing research project sponsored by the faculty member, and/or the faculty member (with the students’ permission) engages in post-dissertation reanalysis/rewriting to ensure that the work is published. RENDERING PROBLEM STARTED HERE Academia can be fiercely competitive and hurt feelings (in fact, broken relationships) over issues like authorship are common. In the preface of their book on longitudinal modeling, a beautiful story is told by Judith Singer and John Willett (2003). The authors met at Harvard in 1985 when they competed for a single position but an unexpected vacancy led to them both being hired. Their colleagues expected them to be competitive. Early in their careers they decided to be intentionally collaborative. This involved never divulging who wrote what part of a paper, inviting the other when one was invited to speak/present, and never competing for opportunities. Part of their strategy was to include this statement in every jointly published work: “The order of the authors has been determined by randomization.” Given that the collaboration evidenced by Singer and Willett (2003) is unique, other strategies may be used. The Style Manual recommends that authors talk as soon as practical to establish the process and criteria for determining the author order. We have linked a rubrics that is sometimes used in research teams to help make these decisions. 6.6.2 Writing Effectively There are a number of popular resources for writing scholarly, scientific, and empirical papers. The content of the fourth chapter of the 7th Edition provides guidelines for achieving the hallmarks of APA Style: continuity, flow, conciseness, and clarity. The topics within Chapter 4 represent suggestions for transitioning more casual writing (e.g., wordiness, anthropomorphisms, subject-verb agreement) to be more clear, precise, and efficient. The last section of the 7th Edition’s fourth chapter reviews broader strategies to improve writing. We highlight two of these examples. Section 4.25 recommends the vicarious strategy of reading. We find it extremely valuable to find one or more articles, from a high quality journal, that is similar or parallel to what is being written or created. After reviewing the paper for content, examine it from a meta perspective, asking the questions that are important to you. For example: At what breadth or specificity does the paper open? What heading/subheadings are used? How are transitions made? How are the hypotheses identified – numbered? narrated? Is there a format for presenting statistical output? What is presented in the text versus a table? How does the paper end (e.g., summary vs. call-to-action)? The 7th Edition’s Chapter 4 also recommends creating an outline. A well-structured outline should guide a reader/reviewer through a paper, even if the content is missing. To demonstrate this point, we present the headings of our example article by Huang and Chan (2022). As you examine it, note how the content of the headings and subheadings in the introduction map onto the method and results. Effects of Sexual Orientation Concealment on Well-Being Among Sexual Minorities: How and When Does Concealment Hurt? The Mechanisms of Sexual Orientation Concealment LGB-Specific Authenticity Loneliness The Moderating Effect of Inclusive Environments The Present Study Method Participants Procedure Measures Sexual Orientation Concealment Perceived LGBT-Friendliness LGB-Specific Authenticity Loneliness Well-Being Results Preliminary Analysis The Effect of Sexual Orientation Concealment on Well-Being Mediating Effects of LGB-Specific Authenticity and Loneliness Moderating Effects of Perceived LGBT-Friendliness Moderating Effects of Perceived LGBT-Friendliness of the Family (Model 2) Moderating Effects of Perceived LGBT-Friendliness of the Peer Networks (Model 3) Moderating Effects of Perceived LGBT-Friendliness of the Workplace/School (Model 4) Moderating Effects of Perceived LGBT-Friendliness of the Wider Society (Model 5) Discussion Practical Implications Study Limitations Conclusions References In many research methods classes, tutorials, and books on writing, trainees are instructed to write an outline. In our own program graduate students are encouraged to create a “mangy cat outline,” where “some part of the outline will have more fur (details) than others.” Writers can expect to revise, update, and restructure the outline as they work through all the steps of the research process. Elsewhere, this OER’s chapter on open science describes the value that preregistration of a research project can have on transparency of science. We also suggest that in lieu of (or in addition to) outlining the paper, completing a preregistration may also serve as a tool for collecting and organizing ideas that can be a starting place for drafting the journal article. 6.6.3 Reducing Bias New to the 7th Edition is the content of the fifth chapter, “Bias-Free Language Guidelines.” Accompanying this chapter is a strong message that the guidelines for bias-free language (note the use of language and not just writing) will be forever-evolving. We highlight a few general guidelines: Call people what they call themselves (especially in qualitative or participant-informed research designs). Recognize that no group is monolithic; remain attentive to within-group differences. For example, There is debate about person-first language (“a child with autism”) and identity-first language (“an autistic child”). Get consultation and listen to the research participants. Accept that language changes with time; remain attentive and flexible. “White,” “heteronormative,” and “U.S.” is often the standard against others are judged. When comparisons are made, placing the socially dominant group on the left side of a graph or at the top of a table may imply these groups are the universal standard. In describing any population, provide a summary of relevant characteristics. It is unnecessary to collect and report all personal characteristics (particularly if they are distal to the focal topic). Avoid the temptation to be passive or euphemistic. Instead, describe actual differences between the target and general populations clearly and professionally. There are specific guidelines for level of specificity in labels/language for various populations (e.g., disability, age, gender identity, race/ethnicity/nationality, sexual orientation, SES). When writing about a topic or population, take time to look up the current guidelines, again. The chapter had extensive and specific guidelines for reducing bias by age, disability, gender, research participants, racial and ethnic identity, sexual orientation, socioeconomic status, and the recognition of intersectionality. In keeping with the opening theme, the guidelines continue to evolve. 6.6.4 The Rules of Style The sixth chapter of the 7th Edition provides the guidelines that bring uniformity to papers. Among so many other things, we learn to: use the Oxford comma, use single quotation marks only within double quotation marks, use lowercase for diseases, theories, therapies, or statistical procedures, and space once after punctuation. The chapter also includes a helpful table that provides properly formatted statistical abbreviations and symbols. Additionally, there are detailed instruction for creating tables and figures. 6.6.5 References Chapters 8 through 11 of the 7th Edition address in-text citations and the reference list. These chapters are are probably most consulted in the Style Manual. What may be surprising is the broader information provided about (a) the degree to which material should be cited, (b) plagiarism, (c) self-plagiarism, and (d) choosing between paraphrases and direct quotations. In the era of open science, there is also a section addressing “which” version of any document should be cited when there are multiple versions (e.g., pre-print, online first, print version)available. The answer to this question is: cite the version used. Authors of the 7th Edition have continued to provide numerous examples and variations for users to emulate. They have also invested more time in creating a reference list entry when no match is perfect. The 7th Edition indicates that the general format of the reference is: Author (Date). Title. Source. Each element of the reference entry is associated with a question: Author: Who is responsible for the work? Date: When was the work published? Title: What is the work called? Source: Where can I retrieve the work? Table 9.1 in the 7th Edition provides a chart for creating a reference when information is missing. The text-citations and reference lists must be 100% consistent. If you have consistently used a reference management system (e.g., Zotero, Mendeley) and each reference entry is accurate and complete, this will happen rather magically. Even so, one of the last things we recommend is a text-cite/ref-list cross check. We open two copies of the manuscript and put them side-by-side on a computer monitor. Starting with the text (using track changes), each time we come to a reference, we put an X in front of it the reference list entry. We go through the entire document. If there are items in the reference list missing, we note and add them. If there are additional references in the reference list that we have not checked, we do a quick search to see if they are really missing, and if so, we delete them. In addition to ensuring text-cite/ref-list consistency, this type of review often reveals other issues that should be corrected prior to submission or dissemination. 6.6.6 Publication The 12th chapter of the 7th Edition provides an overview of the publication process including preparing publication, understanding the editorial publication process, preparing the manuscript, and copyright and permission guidelines. Especially helpful in the 12th chapter is the overview of the publication process and the descriptions of the roles of the decision-makers: the editor-in-chief, associate editors, consulting or advisory editors, and ad hoc reviewers. Also helpful is the description and options associated with the editorial adjudications: acceptance, revise-and-resubmit, and rejection. 6.7 Revisiting the Culture of APA Style As a reader, you may correctly presume that if we have taken the time to review core elements of APA Style, that we must value them. We do. A manuscript or article that is organized predictably and contains the required information in the assigned places makes it much more efficient to evaluate and utilize. As the first author (Bikos), I have spent decades learning APA Style, and as Madigan (1995) predicted, I have inculcated many of its values. I truly enjoy reading an article that is clear, precise, and efficient. Preparing this chapter has helped me become increasingly aware, though of the liabilities of such a narrowly prescribed way of writing. Critical consciousness requires the identification and interrogation of all the systems in which we operate – especially and including those things we love. Earlier in this chapter we encouraged the reader to review Okun’s (2021) characteristics of White Supremacy Culture and to make note of places where APA Style reflects White Supremacy Culture. Below, we have added a few notes describing our observations, especially as they relate to the publication of scholarship more broadly. Below we have again listed the characteristics of White Supremacy Culture. This time we have added brief descriptions when we have observed overlap. Perfectionism Perfectionistic culture The level of negotiable detail surrounding research designs, choice of statistics and interpretation, and writing up the results – combined with the culture of peer review – means that the perfect study is never attained. While this cautiousness offers strength in the form of humility, it may also contribute to a self-deprecation and doubt that stalls works from being completed and prevents authors from moving their scientific findings into advocacy and practice. Worship of the written word The entire Style Manual is about “the written word.” For example, of the 7th Edition’s 427 pages, 310 (73%) are devoted to citing other work. Further, abundant citation of other works is expected, especially in the Introduction, Method, and Results sections. In our own writing, we have shied away from citing personal communication, online videos, and podcasts because (a) the mechanisms for citation are less clear and (b) we fear they would be viewed as less credible. Only one right way The 7th Edition provides explicit instructions for everything from the formatting of statistical symbols to the use of commas. Many of these conventions leave no room for debate; they are either “correct” or “incorrect.” Either/or thinking We are not thinking of examples of this that we observe in APA Style. With psychology’s recognition of philosophies of science including constructivist-interpretivist and critical-ideological and attention to the way that confounds and error can be introduced into a study, we do not see this as a general characteristic of the profession or APA Style. Concentration of power Power hoarding As described in the 7th Edition’s 12th chapter, the publication process is hierarchical, with decisions relying on layers of peer reviewers and decision-makers. Additionally, power is earned and retained through the requirement to locate new studies in the context of the prior literature through the citation of prior literature. In turn, citations are directly related to impact factors and, in turn, tenure, promotion, prestige, and status. Thompson’s (2004) proposal that knowledge (i.e., scholarly products, in particular) is property, underscores this point. Paternalism Defensiveness Although manuscripts and correspondence about them (i.e., cover letters, editorial feedback, revisions letters) are to be written with tones that are professional, neutral, and objective, the purpose of each is to defend ones’ work. With experience, scholars become quite skilled at providing compelling defenses of their work and posturing toward “acceptance” of a peer-reviewed submission. Right to comfort Fear of open conflict As we noted earlier, Madigan (1995) described APA Style has one that was tentative and cautious. That is, authors hedge with hesitant language and statements that “more confirmation is needed.” Through the use of techniques like using first person (section 4.16) and an active voice (section 4.13), the 7th Edition is promoting a more direct style of writing. That said, in the publication process, peer reviewers provide their impressions to the editor, the editor summarizes the feedback and renders a decision to the author(s). The correspondence between all parties tends to be very professional and somewhat ingratiating. Combative tones are discouraged. Individualism I’m the only one Although not necessarily communicated in the Style Manual, replication studies have been avoided because they fail to make a unique contribution to the literature. As described in the chapter on open science, this has likely contributed to the replicability/reproducibility crisis, in that if replications failures had been observed sooner, then researchers could begin closer investigations into why. Progress is more/bigger Objectivity The precision, clarity, and efficiency goals of APA Style are intended to provide the assurance of objectivity. Even in qualitative studies, researchers provide disclosure/transparency statements to assure the reviewers and readers that they have approached their topic in a manner that is unbiased and neutral. Quantity over quality Common among the curriculum vita of academics is the enumeration of scholarly products so that their counts can be easily included in applications for promotion and tenure. Additionally, many institutions require additional data about the impact that faculty work has had; the most common being “times cited” as well as the impact factor of the journal. This latter criteria, though, is a reflection of quality. Sense of urgency Historically, the publication process has been quite slow. With competition, though, from open access journals (where the time from submission to publication can be a matter of days or weeks), we have observed top tier journals shortening the timeframe that volunteer peer reviewers are given to review and submit their reviews. We have also noted that some journals will offer “online first” access to publications and permit researchers to post preprints to institutional repositories and their own social media (e.g., ResearchGate). To summarize, our review suggests that APA Style incorporates characteristics of White Supremacy Culture. Subsequently, we might ask, “How do these serve to maintain the power structures within psychology (or academia, or scholarship)?” Although we expect there are many contexts in which APA Style facilitates the location of power in the Western, White, cis-hetero, male, the global context is particularly salient to the first author (Bikos). International scholars, particularly those for whom English is a foreign language, are one group that face significant challenges in publishing their research – even when the science is meritorious (Gibbons, 2012). I have lived and taught internationally; co-authored papers with international colleagues; and peer-reviewed a number of manuscripts from international authors and for international journals. It is from these experiences I share first-hand observations and offer recommendations for moving forward. In the early 2000s, my family was living and working in Ankara, Turkey. I had finished up a manuscript and was ready to submit it for publication. At the time, submitting to a U.S. journal required that multiple copies of the paper be printed on 8.5 x 11 inch (i.e., “letter sized”) paper. The international standard is A4 (8.25 x 11.75), and my searching for letter-sized paper turned up empty-handed. Our family’s assignment in Turkey was because of my partner’s position with a multinational corporation. This privilege made it possible to have several reams of paper delivered within a couple of weeks. Given that we now submit papers online, this particular barrier no longer exists. It does, though, provide a concrete example of how barriers that have nothing to do with the rationale for the study, the quality of the research design, or the readability of the final manuscript, create inequitable barriers to participate in the process. A second observation came when a Turkish colleague asked me to proofread a paper. The editorial letter my colleague received recommended that the paper be “edited by a native speaker.” The colleague was confused because the second author of the manuscript was a well-published, White, male, Counseling Psychologist, from the U.S., who had stated that the manuscript was ready for submission. After a weekend of deep editing, I (an early career professional, at the time) emerged with a feeling of disillusionment. In this circumstance, the positionality of the co-author should have facilitated the publication process for my colleague. Yet, this esteemed scholar failed to invest the time and resources (a task well within his skill set) that would benefit them both. As my career has unfolded, the exploitative relations between U.S. scholars and international collaborators (especially LMICs [low/middle income countries]) has been well documented CITATION. Recognizing the significant challenges to publication, International Perspectives in Psychology has prioritized disseminating research from global scholars (Gibbons, 2012). One response to that was an editorial mentoring program. Unfortunately, only three projects were completed through that program before it was deemed unsustainable (Gibbons &amp; Carr, 2016). I served as an editorial mentor in this program. As one who embraces APA Style, it was deeply rewarding to assist authors translate their manuscripts to the language of the APA journal. This work, though, brought to life the notion that APA Style is much more than (a) sorting the information into the pre-defined categories, (b) editing the writing so that it is clear, precise, and efficient, (d) formatting tables and figures, (e) presenting the results in the prescribed ways, and (e) creating an APA Style reference list that is consistent with the text citations. As Madigan (1995) noted, APA Style is multi-layered, including the guidelines for most aspects of writing, but also conveying values and a way of thinking. The editorial mentor role contrasts with that of the gate-keeping peer reviewer. Further, the systems used for peer review make it difficult to provide the detailed copyediting feedback that I could provide. For most journals, feedback must be written on a separate document in an enumerated, line-by-line, narration. Pointing out errors of grammar/style (particularly when they are plentiful) is time consuming because each must be described. I suspect many peer reviewers “mark up” the document as they are reviewing it. Thus, using a system that allowed this document to be included in the peer review could functon to teach APA Style. The three recommendations that emerge from these personal examples include the following: We must critically examine every aspect of the publication process and ask, “How does it facilitate the dissemination of good science?” If it does not, consider its removal or restructuring. We must recognize that APA Style interacts with the existing power structures and can be leveraged to prevent dissemination from those who hold marginalized and minoritized identities. We could examine the tools and processes we use for peer review. To that end, Grammar, APA style, and formatting should not prevent a project that has scientific merit from being published. Given our current technologies, it could be relatively easy to allow peer reviewers (or perhaps a designated reviewer) to provide direct, copyediting feedback, to the author. 6.8 Suggestions for Practice, Further Learning, and/or Conversation Using a journal article that is of interest to you, create an outline from the headings and subheadings. Consider how well these guide the reader/reviewer through the research project. Audit the same journal article using the empty table provided below. Make notes of if, how, and where the article followed the JARS guidelines. 6.8.1 Empty Table for JARS Audit Activity Table 1 Element Your Article +/- Title Your Article +/- Main variables and theoretical issues and their relationship Population(s) studied Author Note Your Article +/- Acknowledgement and explanation of special circumstances (e.g., preregistration, use of data in other publications, if primary data is used in dissertation or conference papers, sources of funding/support, potential conflicts of interest, affiliation of authors if different than author byline, contact information for corresponding author, other relevant information) Abstract (120 to 250 words) Your Article +/- Problem under investigation including primary hypotheses Key participant characteristics Method: including research design, sample size, materials used, outcome measures, data-gathering procedures Findings: primary statistical output Conclusions: results plus implications or applications Introduction Your Article +/- Problem statement, including theoretical and/or practical implications Succinct review of relevant scholarship – connecting to extant literature Hypotheses (primary and secondary), aims, and objectives, including theoretical derivation and additional planned analyses Method Your Article +/- Inclusion and exclusion criteria + Participant characteristics: standard demographic and topic-specific Sampling procedures addressing the following (if used): sampling plan, response rate, self-selection,settings and dates, participant agreements/compensation, and IRB and safety monitoring. Sample size, power, and precision: intended and achieved sample size, power analysis, use of interim analyses or stopping rules Measures and covariates: all primary and secondary measures and covariates (including those not utilized in report) Data collection: methods used to collect data Quality of measurements: Methods used to enhance quality of measurements including training and reliability of data collectors and use of multiple observations Instrumentation: review of psychometric and biometric properties Masking: In experimental manipulations, describe and evaluate presence of masking techniques (if used) Psychometrics: reliability and validity estimates for instruments within the reported study and related studies or test manuals Conditions and design: May need to consult supplemental tables Data diagnostics: planned data diagnostics including criteria for post-data-collection exclusion, description of managing missing data, definition and processing of statistical outliers, analyses of distributional characteristics, data transformations (if used) Analytic strategy: for primary, secondary, and exploratory hypothesis; include plans to protect against experiment-wise error Results Your Article +/- Participant flow: Include n of each group at each stage of study; consider including a figure Recruitment: Dates defining periods of recruitment and follow-up Statistics and data analysis: the JARS provides detailed information about what to include about missing data and analysis, descriptive data, inferential statistics, and complex statistics; problems with statistical assumptions, data distributions, convergence are to be reported Discussion Your Article +/- Support (or nonsupport) of original hypotheses: distinguish between primary, secondary, exploratory; consider implication of exploratory as it relates to error rates Similarity of results to findings and work of others T Interpretation of results, considering validity threats, sample size, measurement protocols Generalizability taking into consideration target population and contextual issues Implications for future research, program, policy REFERENCES American Psychological Association. (2017). Ethical principles of psychologists and code of conduct. https://www.apa.org/ethics/code American Psychological Association. (2020). Publication manual of the American Psychological Association: The official guide to APA style. (Seventh edition.). American Psychological Association. Appelbaum, M., Cooper, H., Kline, R. B., Mayo-Wilson, E., Nezu, A. M., &amp; Rao, S. M. (2018). Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. American Psychologist, 73(1), 3–25. https://doi.org/10.1037/amp0000191 Gibbons, J. L. (2012). Inaugural editorial. International Perspectives in Psychology: Research, Practice, Consultation, 1(1), 1–2. https://doi.org/10.1037/a0027222 Gibbons, J. L., &amp; Carr, S. C. (2016). IPP—Quo vadis? International Perspectives in Psychology: Research, Practice, Consultation, 5(4), 207–210. https://doi.org/10.1037/ipp0000062 Huang, Y.-T., &amp; Chan, R. C. H. (2022). Effects of sexual orientation concealment on well-being among sexual minorities: How and when does concealment hurt? Journal of Counseling Psychology. https://doi.org/10.1037/cou0000623 Madigan, R., Johnson, S., &amp; Linton, P. (1995). The language of psychology: APA style as epistemology. American Psychologist, 50(6), 428–436. https://doi.org/10.1037/0003-066X.50.6.428 Okun, T. (2021). White Supremacy Culture – Still Here. https://www.dismantlingracism.org/uploads/4/3/5/7/43579015/white_supremacy_culture_-_still_here.pdf Reporting standards for research in psychology: Why do we need them? What might they be? (2008). American Psychologist, 63(9), 839–851. https://doi.org/10.1037/0003-066X.63.9.839 Singer, J. D., &amp; Willett, J. B. (2003). Applied Longitudinal Data Analysis: Modeling Change and Event Occurence. Oxford University Press, Incorporated. http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3054153 Thompson, A. (2004). Gentlemanly Orthodoxy: Critical Race Feminism, Whiteness Theory, and the APA Manual. Educational Theory, 54(1), 27–57. https://doi.org/10.1111/j.0013-2004.2004.00002.x White Supremacy Culture. (n.d.). In dRworksBook. Retrieved August 8, 2020, from https://www.dismantlingracism.org/white-supremacy-culture.html "],["coming-soon.html", "Chapter 7 Coming Soon 7.1 Planned Chapters 7.2 Desired Chapters 7.3 Appendices Plans", " Chapter 7 Coming Soon The following is a listing of planned chapters and authors who have committed to writing them. Please suggest missing chapters and consider locating a co-author(s) and contributing one yourself(ves). 7.1 Planned Chapters Philosophy of science Nuha Alshabani &amp; Nathan Bitecofer Psychometrics/scale development Cirleen DeBlaere &amp; Casey Hinger Qualitative research methods Ashley MacPherson &amp; Pearis Bellamy Program evaluation Maryam Jernigan-Noesi Internationalizing and decolonizing psychological research Jenny Vaydich, Jacob Bentley, &amp; Oscar Baldelomar Institutional Review Boards Meredith Maroney &amp; Emily Coombs-Montanez Research environment &amp; culture Sally Stabb 7.2 Desired Chapters Experimental design Non-experimental design Case study/single case design Action Research Other chapters that you think would be important to include 7.3 Appendices Plans Research methods syllabus Proposed assignments (beyond those embedded in the chapter) Grading rubrics for research-related assignments Lists of additional resources "],["references-2.html", "Chapter 8 References", " Chapter 8 References "],["references-3.html", "REFERENCES", " REFERENCES Accreditation, C. on. (2018). Standards of Accreditation for Health Service Psychology (2015) &amp; Accrediting Operating Procedures (2019). https://www.apa.org/ed/accreditation/about/policies/standards-of-accreditation.pdf Alter, G., &amp; Gonzalez, R. (2018). Responsible practices for data sharing. American Psychologist, 73(2), 146–156. https://doi.org/10.1037/amp0000258 American Psychological Association. (2017). Ethical principles of psychologists and code of conduct. https://www.apa.org/ethics/code American Psychological Association. (2020). Publication manual of the American Psychological Association: The official guide to APA style. (Seventh edition.). American Psychological Association. Anderson, C. B., Chang, S., Lee, H. Y., &amp; Baldwin, C. D. (2022). Mentoring Barriers, Expected Outcomes, and Practices in Scientific Communication: Scale Development and Validation. Journal of Career Development, 49(3), 697–713. https://doi.org/10.1177/0894845321991680 Anderson, J. F., &amp; Withrow, J. G. (1981). The impact of lecturers‟ nonverbal expressiveness on improving mediated instruction. Communication Education, 30, 342–353. Appelbaum, M., Cooper, H., Kline, R. B., Mayo-Wilson, E., Nezu, A. M., &amp; Rao, S. M. (2018). Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. American Psychologist, 73(1), 3–25. https://doi.org/10.1037/amp0000191 Arizona Clinical Psychology Program, U. of. (2023). Program Overview &amp; Training Model Psychology. https://psychology.arizona.edu/clinical/training-model Ashcraft, L. E., Quinn, D. A., &amp; Brownson, R. C. (2020). Strategies for effective dissemination of research to United States policymakers: A systematic review. Implementation Science, 15(1), 89. https://doi.org/10.1186/s13012-020-01046-3 Baker, E. A., Brewer, S. K., Owens, J. S., Cook, C. R., &amp; Lyon, A. R. (2021). Dissemination Science in School Mental Health: A Framework for Future Research. School Mental Health, 13(4), 791–807. https://doi.org/10.1007/s12310-021-09446-6 Beall, J. (2012). Predatory publishers are corrupting open access. Nature, 489(7415), 179–179. https://doi.org/10.1038/489179a Benchmarks Evaluation System. (2012). American Psychological Association. https://www.apa.org/ed/graduate/benchmarks-evaluation-system Bezjak, S., Clyburne-Sherin, A., Conzett, P., Fernandes, P., Görögh, E., Helbig, K., Kramer, B., Labastida, I., Niemeyer, K., Psomopoulos, F., Ross-Hellauer, T., Schneider, R., Tennant, J., Verbakel, E., Brinken, H., &amp; Heller, L. (2018). Open Science Training Handbook. Zenodo. https://doi.org/10.5281/ZENODO.1212496 Blaszczynski, A., &amp; Gainsbury, S. M. (2019). Editor’s note: Replication crisis in the social sciences. International Gambling Studies, 19(3), 359–361. https://ideas.repec.org//a/taf/intgms/v19y2019i3p359-361.html Bosnjak, M., Fiebach, C. J., Mellor, D., Mueller, S., O’Connor, D. B., Oswald, F. L., &amp; Sokol, R. I. (2021). A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force. American Psychologist. https://doi.org/10.1037/amp0000879 Buchanan, N. T., Perez, M., Prinstein, M. J., &amp; Thurston, I. B. (2021). Upending racism in psychological science: Strategies to change how science is conducted, reported, reviewed, and disseminated. American Psychologist, 76(7), 1097–1112. https://doi.org/10.1037/amp0000905 Ceci, S. J., &amp; Walker, E. (1983). Private archives and public needs. American Psychologist, 38(4), 414–423. https://doi.org/10.1037/0003-066X.38.4.414 Chambers, C. D., &amp; Tzavella, L. (2022). The past, present and future of Registered Reports. Nature Human Behaviour, 6(1), 29–42. https://doi.org/10.1038/s41562-021-01193-7 Cherry, D. K., Messenger, L. C., &amp; Jacoby, A. M. (2000). An examination of training model outcomes in clinical psychology programs. Professional Psychology: Research and Practice, 31(5), 562–568. https://doi.org/10.1037/0735-7028.31.5.562 Chubin, D. E. (1985). Open Science and Closed Science: Tradeoffs in a Democracy. Science, Technology, &amp; Human Values, 10(2), 73–81. http://www.jstor.org/stable/689511 Cokley, K., &amp; Awad, G. H. (2013). In Defense of Quantitative Methods: Using the “Master’s Tools” to Promote Social Justice. Journal for Social Action in Counseling &amp; Psychology, 5(2), 26–41. https://doi.org/10.33043/JSACP.5.2.26-41 Comas-Díaz, L., &amp; Torres Rivera, E. (2020). Liberation psychology: Theory, method, practice, and social justice. American Psychological Association. https://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=2548637 Counseling Versus Clinical Psychology. (n.d.). In Towson University. Retrieved January 20, 2025, from https://www.towson.edu/cla/departments/psychology/grad/psychology/counseling/versus.html Creswell, J. W., &amp; Poth, C. N. (2016). Qualitative Inquiry and Research Design: Choosing Among Five Approaches. SAGE Publications. Darwin Holmes, A. G. (2020). Researcher Positionality - A Consideration of Its Influence and Place in Qualitative Research - A New Researcher Guide. Shanlax International Journal of Education, 8(4), 1–10. https://doi.org/10.34293/education.v8i4.3232 De Rond, M., &amp; Miller, A. N. (2005). Publish or Perish: Bane or Boon of Academic Life? Journal of Management Inquiry, 14(4), 321–329. https://doi.org/10.1177/1056492605276850 DeAngelis, T. (2003). Three programs: Three different training models. gradPSYCH Magazine, 09. https://www.apa.org/gradpsych/2003/09/three-programs DeBlaere, C., Singh, A. A., Wilcox, M. M., Cokley, K. O., Delgado-Romero, E. A., Scalise, D. A., &amp; Shawahin, L. (2019). Social justice in counseling psychology: Then, now, and looking forward. The Counseling Psychologist, 47(6), 938–962. https://doi.org/10.1177/0011000019893283 Diener, E., &amp; Biswas-Diener, R. (2016). The Replication Crisis in Psychology. In NOBA. https://nobaproject.com/modules/the-replication-crisis-in-psychology Digital Scholarship, C. for. (2022). Zotero Your personal research assistant. https://www.zotero.org/ Driessen, E., Hollon, S. D., Bockting, C. L. H., Cuijpers, P., &amp; Turner, E. H. (2015). Does Publication Bias Inflate the Apparent Efficacy of Psychological Treatment for Major Depressive Disorder? A Systematic Review and Meta-Analysis of US National Institutes of Health-Funded Trials. PLOS ONE, 10(9), e0137864. https://doi.org/10.1371/journal.pone.0137864 Dumper, K., Jenkins, D., Lovett, M., &amp; Perlmutter, M. (2019). Psychological Research. OpenStax. https//openstax.org/details/books/psychology Dupree, C. H., &amp; Kraus, M. W. (2022). Psychological science is not race neutral. Perspectives on Psychological Science, 17(1), 270–275. https://doi.org/10.1177/1745691620979820 Fassinger, R., &amp; Morrow, S. L. (2013). Toward Best Practices in Quantitative, Qualitative, and Mixed- Method Research: A Social Justice Perspective. Journal for Social Action in Counseling &amp; Psychology, 5(2), 69–83. https://doi.org/10.33043/JSACP.5.2.69-83 Fecher, B., &amp; Friesike, S. (2013). Open Science: One Term, Five Schools of Thought. RatSWD Working Paper Series, 14. Field, A. P. (2012). Discovering statistics using R. Sage. Force, P. T. (2021). Preregistration Standards for Psychology - the Psychological Research Preregistration-Quantitative (aka PRP-QUANT) Template. https://www.psycharchives.org/en/item/088c79cb-237c-4545-a9e2-3616d6cc8453 Fortney, K., &amp; Murphy, L. S.-L. (2016). Getting Found: Indexing and the Independent Open Access Journal. Western Journal of Emergency Medicine, 17(5), 508–510. https://doi.org/10.5811/westjem.2016.6.30836 Garvey, W. D., &amp; Griffith, B. C. (1965). Scientific communication: The dissemination system in psychology and a theoretical framework for planning innovations. American Psychologist, 20(2), 157–164. https://doi.org/10.1037/h0021711 Gibbons, J. L. (2012). Inaugural editorial. International Perspectives in Psychology: Research, Practice, Consultation, 1(1), 1–2. https://doi.org/10.1037/a0027222 Gibbons, J. L., &amp; Carr, S. C. (2016). IPP—Quo vadis? International Perspectives in Psychology: Research, Practice, Consultation, 5(4), 207–210. https://doi.org/10.1037/ipp0000062 GitHub. (2022a). https://github.com GitHub. (2022b). In Wikipedia. https://en.wikipedia.org/w/index.php?title=GitHub&amp;oldid=1097996266 Gorham, J. (1988). The relationship between verbal teacher immediacy behaviors and student learning. Communication Education, 37(1), 40–53. https://doi.org/10.1080/03634528809378702 Hagger, M. S. (2022). Developing an open science “mindset.” Health Psychology and Behavioral Medicine, 10(1), 1–21. https://doi.org/10.1080/21642850.2021.2012474 Hargons, C., Mosley, D., Falconer, J., Faloughi, R., Singh, A., Stevens-Watkins, D., &amp; Cokley, K. (2017). Black Lives Matter: A call to action for counseling psychology leaders. The Counseling Psychologist, 45(6), 873–901. https://doi.org/10.1177/0011000017733048 Haven, T., Errington, T. M., Gleditsch, K., Grootel, L. van, Jacobs, A. M., Kern, F., Piñeiro, R., Rosenblatt, F., &amp; Mokkink, L. (2020). Preregistering Qualitative Research: A Delphi Study. SocArXiv. https://doi.org/10.31235/osf.io/pz9jr Hengartner, M. P. (2018). Raising Awareness for the Replication Crisis in Clinical Psychology by Focusing on Inconsistencies in Psychotherapy Research: How Much Can We Rely on Published Findings from Efficacy Trials? Frontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.00256 Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29–29. https://doi.org/10.1038/466029a Huang, Y.-T., &amp; Chan, R. C. H. (2022). Effects of sexual orientation concealment on well-being among sexual minorities: How and when does concealment hurt? Journal of Counseling Psychology. https://doi.org/10.1037/cou0000623 Jones, J. L., &amp; Mehr, S. L. (2007). Foundations and assumptions of the scientist-practitioner model. American Behavioral Scientist, 50(6), 766–771. https://doi.org/10.1177/0002764206296454 Kaslow, N. J., Grus, C. L., Campbell, L. F., Fouad, N. A., Hatcher, R. L., &amp; Rodolfa, E. R. (2009). Competency Assessment Toolkit for professional psychology. Training and Education in Professional Psychology, 3(4, Suppl), S27–S45. https://doi.org/10.1037/a0015833 König, L. M., Altenmüller, M. S., Fick, J., Crusius, J., Genschow, O., &amp; Sauerland, M. (2024). How to communicate science to the public? Recommendations for effective written communication derived from a systematic review. Zeitschrift Für Psychologie, No Pagination Specified–No Pagination Specified. https://doi.org/10.1027/2151-2604/a000572 Krathwohl, D. R. (2009). Methods of Educational and Social Science Research: The Logic of Methods, Third Edition. Waveland Press. Levitt, H., Kannan, D., &amp; Ippolito, M. R. (2013). Teaching qualitative methods using a research team approach: Publishing grounded theory projects with your class. Qualitative Research in Psychology, 10(2), 119–139. https://doi.org/10.1080/14780887.2011.586101 Lewis, N. A., &amp; Wai, J. (2021). Communicating What We Know and What Isn’t So: Science Communication in Psychology. Perspectives on Psychological Science, 16(6), 1242–1254. https://doi.org/10.1177/1745691620964062 Madigan, R., Johnson, S., &amp; Linton, P. (1995). The language of psychology: APA style as epistemology. American Psychologist, 50(6), 428–436. https://doi.org/10.1037/0003-066X.50.6.428 Mallinckrodt, B., Miles, J. R., &amp; Levy, J. J. (2014). The scientist-practitioner-advocate model: Addressing contemporary training needs for social justice advocacy. Training and Education in Professional Psychology, 8(4), 303–311. https://doi.org/10.1037/tep0000045 Martin, C., &amp; MacDonald, B. H. (2020). Using interpersonal communication strategies to encourage science conversations on social media. PLOS ONE, 15(11), e0241972. https://doi.org/10.1371/journal.pone.0241972 Martone, M. E., Garcia-Castro, A., &amp; VandenBos, G. R. (2018). Data sharing in psychology. The American Psychologist, 73(2), 111–125. https://doi.org/10.1037/amp0000242 Matsick, J. L., Kruk, M., Oswald, F., &amp; Palmer, L. (2021). Bridging Feminist Psychology and Open Science: Feminist Tools and Shared Values Inform Best Practices for Science Reform. Psychology of Women Quarterly, 45(4), 412–429. https://doi.org/10.1177/03616843211026564 Mendeley. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Mendeley&amp;oldid=1095492319 Merton, R. K. (1942). Science and technology in a democratic order. Journal of Legal and Political Sociology, 1, 115–126. Michalski, D. S., PhD, Fowler, G., &amp; PhD. (n.d.). Doctoral degrees in psychology: How are they different, or not so different? In https://www.apa.org. Retrieved January 20, 2025, from https://www.apa.org/ed/precollege/psn/2016/01/doctoral-degrees Moore, D. A. (2016). Preregister if you want to. American Psychologist, 71(3), 238–239. https://doi.org/10.1037/a0040195 Neimeyer, G. J., Saferstein, J., &amp; Rice, K. G. (2005). Does the Model Matter? The Relationship Between Science-Practice Emphasis and Outcomes in Academic Training Programs in Counseling Psychology. The Counseling Psychologist, 33(5), 635–654. https://doi.org/10.1177/0011000005277821 Okun, T. (2021). White Supremacy Culture – Still Here. https://www.dismantlingracism.org/uploads/4/3/5/7/43579015/white_supremacy_culture_-_still_here.pdf Open science. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_science&amp;oldid=1087027147 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), 943–943. https://doi.org/10.1126/science.aac4716 OSF. (2022). Open Science Framework. https://osf.io/ Patil, P., Peng, R. D., &amp; Leek, J. T. (2016). A statistical definition for reproducibility and replicability (p. 066803). bioRxiv. https://doi.org/10.1101/066803 Petersen, C. A. (2007). A Historical Look at Psychology and the Scientist-Practitioner Model. American Behavioral Scientist, 50(6), 758–765. https://doi.org/10.1177/0002764206296453 Peterson, R. L., Peterson, D. R., Abrams, J. C., &amp; Stricker, G. (1997). The National Council of Schools and Programs of Professional Psychology educational model. Professional Psychology: Research and Practice, 28(4), 373–386. https://doi.org/10.1037/0735-7028.28.4.373 Piwowar, H., Priem, J., Larivière, V., Alperin, J. P., Matthias, L., Norlander, B., Farley, A., West, J., &amp; Haustein, S. (2018). The state of OA: A large-scale analysis of the prevalence and impact of Open Access articles. PeerJ, 6, e4375. https://doi.org/10.7717/peerj.4375 Ponterotto, J. (2005). Qualitative Research Training in Counseling Psychology: A Survey of Directors of Training. Teaching of Psychology. https://www.semanticscholar.org/paper/Qualitative-Research-Training-in-Counseling-A-of-of-Ponterotto/76f17a0d8342482ac56aa4930871310f54bb01a7 project, T. jamovi. (2021). About - jamovi. https://www.jamovi.org R (programming language). (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=R_(programming_language)&amp;oldid=1098779048 Reporting standards for research in psychology: Why do we need them? What might they be? (2008). American Psychologist, 63(9), 839–851. https://doi.org/10.1037/0003-066X.63.9.839 Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638 Ross, M. W., Iguchi, M. Y., &amp; Panicker, S. (2018). Ethical aspects of data sharing and research participant protections. American Psychologist, 73(2), 138–145. https://doi.org/10.1037/amp0000240 Ross-Hellauer, T. (2017). What is open peer review? A systematic review (No. 6:588). F1000Research. https://doi.org/10.12688/f1000research.11369.2 RStudio. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=RStudio&amp;oldid=1099226518 Samuel, T. S., &amp; Warner, J. (2021). \"I Can Math!\": Reducing Math Anxiety and Increasing Math Self-Efficacy Using a Mindfulness and Growth Mindset-Based Intervention in First-Year Students. Community College Journal of Research and Practice, 45(3), 205–222. https://doi.org/10.1080/10668926.2019.1666063 Savin-Baden, M., &amp; Major, C. H. (2023). Qualitative Research: The Essential Guide to Theory and Practice. Routledge. https://doi.org/10.4324/9781003377986 Scientist-Practitioner Advocate Training Model - Psychology Department. (n.d.). In Psychology. Retrieved January 20, 2025, from https://psychology.utk.edu/graduate-students/programs/phd_counseling/scientist-practitioner-advocate-training-model/ Shah, D. (2017). Open Access Publishing: Pros, Cons, and Current Threats. Marshall Journal of Medicine, 3(3), 1. https://doi.org/http://dx.doi.org/10.18590/mjm.2017.vol3.iss3.1 Shoemann, A. M., Boulton, A. J., &amp; Short, S. D. (2017). Determining power and sample size for simple and complex mediation models. Social Psychological and Personality Science, 8, 379–386. https://schoemanna.shinyapps.io/mc_power_med/ Singer, J. D., &amp; Willett, J. B. (2003). Applied Longitudinal Data Analysis: Modeling Change and Event Occurence. Oxford University Press, Incorporated. http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3054153 Smith, A. M., Katz, D. S., &amp; Niemeyer, K. E. (2016). Software citation principles. PeerJ Computer Science, 2, e86. https://doi.org/10.7717/peerj-cs.86 Smith, L. T. (2021). Decolonizing Methodologies: Research and Indigenous Peoples. Bloomsbury Academic. Sommer, R. (2006). Dual dissemination: Writing for colleagues and the public. American Psychologist, 61(9), 955–958. https://doi.org/10.1037/0003-066X.61.9.955 Stevens, J. R. (2017). Replicability and Reproducibility in Comparative Psychology. Frontiers in Psychology, 8. https://doi.org/10.3389/fpsyg.2017.00862 Stricker, G., &amp; Trierweiler, S. J. (2006). The local clinical scientist: A bridge between science and practice. Training and Education in Professional Psychology, S(1), 37–46. https://doi.org/10.1037/1931-3918.S.1.37 Teaching by the Case Method - Christensen Center for Teaching &amp; Learning - Harvard Business School. (n.d.). Christiansen Center for Teaching; Learning. Retrieved January 20, 2025, from https://www.hbs.edu/teaching/case-method/Pages/default.aspx Team, J. (2022). JASP (Version 0.16.3). https://jasp-stats.org/faq/ Thompson, A. (2004). Gentlemanly Orthodoxy: Critical Race Feminism, Whiteness Theory, and the APA Manual. Educational Theory, 54(1), 27–57. https://doi.org/10.1111/j.0013-2004.2004.00002.x Trierweiler, S. J., Stricker, G., &amp; Peterson, R. L. (2010). The research and evaluation competency: The local clinical scientist—Review, current status, future directions. In Competency-based education for professional psychology (pp. 125–141). American Psychological Association. https://doi.org/10.1037/12068-007 Types of Programs. (n.d.). In Psychology Graduate School. Retrieved January 20, 2025, from http://psychologygradschool.weebly.com/types-of-programs.html van’tVeer, A. E., &amp; Giner-Sorolla, R. (2016). Pre-registration in social psychology—A discussion and suggested template. Journal of Experimental Social Psychology, 67, 2–12. https://doi.org/10.1016/j.jesp.2016.03.004 What is a Health Service Psychologist and Why Join. (n.d.). In National Register. Retrieved January 20, 2025, from https://www.nationalregister.org/hsp-credential/what-is-an-hsp/ White Supremacy Culture. (n.d.). In dRworksBook. Retrieved August 8, 2020, from https://www.dismantlingracism.org/white-supremacy-culture.html Williams, A. S. (2010). Statistics Anxiety and Instructor Immediacy. Journal of Statistics Education, 18(2), 187. https://doi.org/10.1080/10691898.2010.11889495 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
