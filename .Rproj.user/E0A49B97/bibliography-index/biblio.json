{
    "sources": [
        {
            "DOI": "10.17605/OSF.IO/HF7DQ",
            "ISBN": "978-1-9991981-0-7",
            "URL": "https://kpu.pressbooks.pub/psychmethods4e/",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        8,
                        22
                    ]
                ]
            },
            "author": [
                {
                    "family": "Jhangiani",
                    "given": "Rajiv S."
                },
                {
                    "family": "Chiang",
                    "given": "I.-Chant A."
                },
                {
                    "family": "Cuttler",
                    "given": "Carrie"
                },
                {
                    "family": "Leighton",
                    "given": "Dana C."
                }
            ],
            "container-title": "Research Methods in Psychology",
            "id": "jhangiani_replicability_2019",
            "issued": {
                "date-parts": [
                    [
                        2019,
                        8
                    ]
                ]
            },
            "publisher-place": "362-369",
            "title": "From the \"Replicability Crisis\" to open science practices (Chapter 60).",
            "type": "chapter"
        },
        {
            "DOI": "10.3389/fpsyg.2017.00862",
            "ISSN": "1664-1078",
            "URL": "https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00862/full",
            "abstract": "Replicability and Reproducibility in Comparative Psychology",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "family": "Stevens",
                    "given": "Jeffrey R."
                }
            ],
            "container-title": "Frontiers in Psychology",
            "id": "stevens_replicability_2017",
            "issued": {
                "date-parts": [
                    [
                        2017
                    ]
                ]
            },
            "keyword": "Pre-registration, animal research, comparative psychology, Replication, Reproducible Research",
            "note": "Publisher: Frontiers",
            "title": "Replicability and Reproducibility in Comparative Psychology",
            "type": "article-journal",
            "volume": "8"
        },
        {
            "DOI": "10.1016/j.jesp.2016.03.004",
            "ISSN": "0022-1031",
            "URL": "http://www.sciencedirect.com/science/article/pii/S0022103116301925",
            "abstract": "Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied—reviewed and unreviewed pre-registration—and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "dropping-particle": "van ’t",
                    "family": "Veer",
                    "given": "Anna Elisabeth"
                },
                {
                    "family": "Giner-Sorolla",
                    "given": "Roger"
                }
            ],
            "collection-title": "Special Issue: Confirmatory",
            "container-title": "Journal of Experimental Social Psychology",
            "id": "van_t_veer_pre-registration_2016",
            "issued": {
                "date-parts": [
                    [
                        2016,
                        11
                    ]
                ]
            },
            "keyword": "Pre-registration, Research methods, Reviewed pre-registration (RPR), Solid science, Unreviewed pre-registration (UPR)",
            "page": "2-12",
            "title": "Pre-registration in social psychology—A discussion and suggested template",
            "type": "article-journal",
            "volume": "67"
        },
        {
            "DOI": "10.3389/fpsyg.2018.00256",
            "ISSN": "1664-1078",
            "PMCID": "PMC5835722",
            "PMID": "29541051",
            "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5835722/",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "family": "Hengartner",
                    "given": "Michael P."
                }
            ],
            "container-title": "Frontiers in Psychology",
            "id": "hengartner_raising_2018",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        2
                    ]
                ]
            },
            "title": "Raising Awareness for the Replication Crisis in Clinical Psychology by Focusing on Inconsistencies in Psychotherapy Research: How Much Can We Rely on Published Findings from Efficacy Trials?",
            "title-short": "Raising Awareness for the Replication Crisis in Clinical Psychology by Focusing on Inconsistencies in Psychotherapy Research",
            "type": "article-journal",
            "volume": "9"
        },
        {
            "DOI": "10.1037/a0040195",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=2016-15883-005&site=ehost-live",
            "abstract": "Prespecification of confirmatory hypothesis tests is a useful tool that makes our statistical tests informative. On the other hand, selectively reporting studies, measures, or statistical tests renders the probability of false positives higher than the p values would imply. The bad news is that it is usually difficult to tell how much higher the probability is. Fortunately, there are enormous opportunities to improve the quality of our science by preregistering our research plans. Preregistration is a highly distinctive strength that should increase our faith in the veracity and replicability of a research result. (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "family": "Moore",
                    "given": "Don A."
                }
            ],
            "container-title": "American Psychologist",
            "id": "moore_preregister_2016",
            "issue": "3",
            "issued": {
                "date-parts": [
                    [
                        2016,
                        4
                    ]
                ]
            },
            "keyword": "Statistical Probability, Statistics, open science, Experimental Replication, false positives, hypothesis testing, Hypothesis Testing, pre-registration, selective reporting",
            "note": "Publisher: American Psychological Association",
            "page": "238-239",
            "title": "Preregister if you want to",
            "type": "article-journal",
            "volume": "71"
        },
        {
            "URL": "https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html",
            "abstract": "Federal Policy for the Protection of Human Subjects or the “Common Rule”",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        17
                    ]
                ]
            },
            "container-title": "HHS.gov",
            "genre": "Text",
            "id": "noauthor_federal_2009",
            "issued": {
                "date-parts": [
                    [
                        2009,
                        6
                    ]
                ]
            },
            "note": "Last Modified: 2016-03-18 00:00:00",
            "title": "Federal Policy for the Protection of Human Subjects (’Common Rule",
            "type": ""
        },
        {
            "DOI": "10.1037/amp0000258",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=2018-06770-004&site=ehost-live",
            "abstract": "Research transparency, reproducibility, and data sharing uphold core principles of science at a time when the integrity of scientific research is being questioned. This article discusses how research data in psychology can be made accessible for reproducibility and reanalysis by describing practical ways to overcome barriers to data sharing. We examine key issues surrounding the sharing of data such as who owns research data, how to protect the confidentiality of the research participant, how to give appropriate credit to the data creator, how to deal with metadata and codebooks, how to address provenance, and other specifics such as versioning and file formats. The protection of research subjects is a fundamental obligation, and we explain frameworks and procedures designed to protect against the harms that may result from disclosure of confidential information. We also advocate greater recognition for data creators and the authors of program code used in the management and analysis of data. We argue that research data and program code are important scientific contributions that should be cited in the same way as publications. (PsycINFO Database Record (c) 2018 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        15
                    ]
                ]
            },
            "author": [
                {
                    "family": "Alter",
                    "given": "George"
                },
                {
                    "family": "Gonzalez",
                    "given": "Richard"
                }
            ],
            "collection-title": "Data Sharing in Psychology",
            "container-title": "American Psychologist",
            "id": "alter_responsible_2018",
            "issue": "2",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        2
                    ]
                ]
            },
            "keyword": "Psychology, Experimentation, data sharing, Data Sharing, Best Practices, Databases, Experimental Ethics, Experimental Subjects, Informed Consent, Privileged Communication, reproducible research, research ethics, 2018",
            "note": "Place: US Publisher: American Psychological Association",
            "page": "146-156",
            "title": "Responsible practices for data sharing",
            "type": "article-journal",
            "volume": "73"
        },
        {
            "DOI": "10.1037/amp0000240",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=2018-06770-003&site=ehost-live",
            "abstract": "Open access is fast becoming the norm across science. Sharing research data broadly has the potential to accelerate scientific progress, optimize the value of data, and promote scientific integrity. However, data sharing also poses new practical and ethical challenges to the conduct of research with human participants. This article provides an overview of how open access to research data has impacted the core principles of research ethics—respect for persons, beneficence, and justice—and, in turn, how a reinterpretation of these principles translates to procedures for the protection of the rights and wellbeing of human research participants. (PsycINFO Database Record (c) 2018 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        15
                    ]
                ]
            },
            "author": [
                {
                    "family": "Ross",
                    "given": "Michael W."
                },
                {
                    "family": "Iguchi",
                    "given": "Martin Y."
                },
                {
                    "family": "Panicker",
                    "given": "Sangeeta"
                }
            ],
            "collection-title": "Data Sharing in Psychology",
            "container-title": "American Psychologist",
            "id": "ross_ethical_2018",
            "issue": "2",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        2
                    ]
                ]
            },
            "keyword": "Psychology, Well Being, data sharing, Data Sharing, Experimental Subjects, 2018, data access, Ethics, group harms, Justice, research participant protections, Respect",
            "note": "Place: US Publisher: American Psychological Association",
            "page": "138-145",
            "title": "Ethical aspects of data sharing and research participant protections",
            "type": "article-journal",
            "volume": "73"
        },
        {
            "DOI": "10.1037/amp0000197",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=2018-06770-002&site=ehost-live",
            "abstract": "An ’open science movement’ is gaining traction across many disciplines within the research enterprise but is also precipitating consternation among those who worry that too much disruption may be hampering professional productivity. Despite this disruption, proponents of open data collaboration have argued that some of the biggest problems of the 21st century need to be solved with the help of many people and that data sharing will be the necessary engine to make that happen. In the United States, a national strategic plan for data sharing encouraged the federally funded scientific agencies to (a) publish open data for community use in discoverable, machine-readable, and useful ways; (b) work with public and civil society organizations to set priorities for data to be shared; (c) support innovation and feedback on open data solutions; and (d) continue efforts to release and enhance high-priority data sets funded by taxpayer dollars. One of the more visible open data projects in the psychological sciences is the presidentially announced ’Brain Research Through Advancing Innovative Neurotechnologies’ (BRAIN) initiative. Lessons learned from initiatives such as these are instructive both from the perspective of open science within psychology and from the perspective of understanding the psychology of open science. Recommendations for creating better pathways to ’walk the walk’ in open science include (a) nurturing innovation and agile learning, (b) thinking outside the paradigm, (c) creating simplicity from complexity, and (d) participating in continuous learning evidence platforms. (PsycINFO Database Record (c) 2019 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2020,
                        11,
                        13
                    ]
                ]
            },
            "author": [
                {
                    "family": "Hesse",
                    "given": "Bradford W."
                }
            ],
            "collection-title": "Data Sharing in Psychology",
            "container-title": "American Psychologist",
            "id": "hesse_can_2018",
            "issue": "2",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        2
                    ]
                ]
            },
            "keyword": "Psychology, Sciences, big data, Big Data, Collaboration, culture of science, data sharing, Data Sharing, open science, Sociocultural Factors, Technology",
            "note": "Publisher: American Psychological Association",
            "page": "126-137",
            "title": "Can psychology walk the walk of open science?",
            "type": "article-journal",
            "volume": "73"
        },
        {
            "DOI": "10.5281/ZENODO.1212496",
            "URL": "https://zenodo.org/record/1212496",
            "abstract": "<<strong>>For a readable version of the book, please visit https://book.fosteropenscience.eu<</strong>>A group of fourteen authors came together in February 2018 at the TIB (German National Library of Science and Technology) in Hannover to create an open, living handbook on Open Science training. High-quality trainings are fundamental when aiming at a cultural change towards the implementation of Open Science principles. Teaching resources provide great support for Open Science instructors and trainers. The Open Science training handbook will be a key resource and a first step towards developing Open Access and Open Science curricula and andragogies. Supporting and connecting an emerging Open Science community that wishes to pass on their knowledge as multipliers, the handbook will enrich training activities and unlock the community’s full potential.In this first release of the Open Science Training Handbook, some initial feedback from the community is already included.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        14
                    ]
                ]
            },
            "author": [
                {
                    "family": "Bezjak",
                    "given": "Sonja"
                },
                {
                    "family": "Clyburne-Sherin",
                    "given": "April"
                },
                {
                    "family": "Conzett",
                    "given": "Philipp"
                },
                {
                    "family": "Fernandes",
                    "given": "Pedro"
                },
                {
                    "family": "Görögh",
                    "given": "Edit"
                },
                {
                    "family": "Helbig",
                    "given": "Kerstin"
                },
                {
                    "family": "Kramer",
                    "given": "Bianca"
                },
                {
                    "family": "Labastida",
                    "given": "Ignasi"
                },
                {
                    "family": "Niemeyer",
                    "given": "Kyle"
                },
                {
                    "family": "Psomopoulos",
                    "given": "Fotis"
                },
                {
                    "family": "Ross-Hellauer",
                    "given": "Tony"
                },
                {
                    "family": "Schneider",
                    "given": "René"
                },
                {
                    "family": "Tennant",
                    "given": "Jon"
                },
                {
                    "family": "Verbakel",
                    "given": "Ellen"
                },
                {
                    "family": "Brinken",
                    "given": "Helene"
                },
                {
                    "family": "Heller",
                    "given": "Lambert"
                }
            ],
            "id": "bezjak_open_2018",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        4
                    ]
                ]
            },
            "keyword": "Training, Open Science, Translational skills, Vocational training",
            "publisher": "Zenodo",
            "title": "Open Science Training Handbook",
            "type": "book"
        },
        {
            "abstract": "Open Science is an umbrella term that encompasses a multitude of assumptions about the future of knowledge creation and dissemination. Based on a literature review, this paper aims at structuring the overall discourse by proposing five Open Science schools of thought: The infrastructure school (which is concerned with the technological architecture), the public school (which is concerned with the accessibility of knowledge creation), the measurement school (which is concerned with alternative impact measurement), the democratic school (which is concerned with access to knowledge) and the pragmatic school (which is concerned with collaborative research).",
            "author": [
                {
                    "family": "Fecher",
                    "given": "Benedikt"
                },
                {
                    "family": "Friesike",
                    "given": "Sascha"
                }
            ],
            "container-title": "RatSWD Working Paper Series",
            "id": "fecher_open_2013",
            "issued": {
                "date-parts": [
                    [
                        2013
                    ]
                ]
            },
            "page": "14",
            "title": "Open Science: One Term, Five Schools of Thought",
            "title-short": "Open Science",
            "type": "article-journal"
        },
        {
            "DOI": "10.7717/peerj-cs.86",
            "ISSN": "2376-5992",
            "URL": "https://peerj.com/articles/cs-86",
            "abstract": "Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Smith",
                    "given": "Arfon M."
                },
                {
                    "family": "Katz",
                    "given": "Daniel S."
                },
                {
                    "family": "Niemeyer",
                    "given": "Kyle E."
                }
            ],
            "container-title": "PeerJ Computer Science",
            "id": "smith_software_2016",
            "issued": {
                "date-parts": [
                    [
                        2016,
                        9
                    ]
                ]
            },
            "note": "Publisher: PeerJ Inc.",
            "page": "e86",
            "title": "Software citation principles",
            "type": "article-journal",
            "volume": "2"
        },
        {
            "DOI": "10.12688/f1000research.11369.2",
            "URL": "https://f1000research.com/articles/6-588",
            "abstract": "Background : “Open peer review” (OPR), despite being a major pillar of Open Science, has neither a standardized definition nor an agreed schema of its features and implementations. The literature reflects this, with numerous overlapping and contradictory definitions. While for some the term refers to peer review where the identities of both author and reviewer are disclosed to each other, for others it signifies systems where reviewer reports are published alongside articles. For others it signifies both of these conditions, and for yet others it describes systems where not only “invited experts” are able to comment. For still others, it includes a variety of combinations of these and other novel methods. Methods : Recognising the absence of a consensus view on what open peer review is, this article undertakes a systematic review of definitions of “open peer review” or “open review”, to create a corpus of 122 definitions. These definitions are systematically analysed to build a coherent typology of the various innovations in peer review signified by the term, and hence provide the precise technical definition currently lacking. Results : This quantifiable data yields rich information on the range and extent of differing definitions over time and by broad subject area. Quantifying definitions in this way allows us to accurately portray exactly how ambiguously the phrase “open peer review” has been used thus far, for the literature offers 22 distinct configurations of seven traits, effectively meaning that there are 22 different definitions of OPR in the literature reviewed. Conclusions : I propose a pragmatic definition of open peer review as an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science, including making reviewer and author identities open, publishing review reports and enabling greater participation in the peer review process.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Ross-Hellauer",
                    "given": "Tony"
                }
            ],
            "id": "ross-hellauer_what_2017",
            "issued": {
                "date-parts": [
                    [
                        2017,
                        8
                    ]
                ]
            },
            "keyword": "Open Science, open peer review, publishing, research evaluation, scholarly communication",
            "note": "Type: article",
            "number": "6:588",
            "publisher": "F1000Research",
            "title": "What is open peer review? A systematic review",
            "title-short": "What is open peer review?",
            "type": "report"
        },
        {
            "DOI": "10.1037/amp0000879",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=2022-06923-001&site=ehost-live",
            "abstract": "Recent years have seen dramatic changes in research practices in psychological science. In particular, preregistration of study plans before conducting a study has been identified as an important tool to help increase the transparency of science and to improve the robustness of psychological research findings. This article presents the Psychological Research Preregistration-Quantitative (PRP-QUANT) Template produced by a Joint Psychological Societies Preregistration Task Force consisting of the American Psychological Association (APA), the British Psychological Society (BPS), and the German Psychological Society (DGPs), supported by the Center for Open Science (COS) and the Leibniz Institute for Psychology (ZPID). The goal of the Task Force was to provide the psychological community with a consensus template for the preregistration of quantitative research in psychology, one with wide coverage and the ability, if necessary, to adapt to specific journals, disciplines, and researcher needs. This article covers the structure and use of the PRP-QUANT template, while outlining and discussing the benefits of its use for researchers, authors, funders, and other relevant stakeholders. We hope that by introducing this template and by demonstrating the support of preregistration by major academic psychological societies, we will facilitate an increase in preregistration practices and also the further advancement of transparency and knowledge-sharing in the psychological sciences. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Bosnjak",
                    "given": "Michael"
                },
                {
                    "family": "Fiebach",
                    "given": "Christian J."
                },
                {
                    "family": "Mellor",
                    "given": "David"
                },
                {
                    "family": "Mueller",
                    "given": "Stefanie"
                },
                {
                    "family": "O’Connor",
                    "given": "Daryl B."
                },
                {
                    "family": "Oswald",
                    "given": "Frederick L."
                },
                {
                    "family": "Sokol",
                    "given": "Rosemarie I."
                }
            ],
            "container-title": "American Psychologist",
            "id": "bosnjak_template_2021",
            "issued": {
                "date-parts": [
                    [
                        2021,
                        11
                    ]
                ]
            },
            "keyword": "Sciences, American Psychological Association, Quantitative Methods, open science, Experimental Replication, Behavioral Sciences, preregistration, PRP-QUANT Template, replicability, reproducibility",
            "note": "Publisher: American Psychological Association",
            "title": "A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force",
            "title-short": "A template for preregistration of quantitative research in psychology",
            "type": "article-journal"
        },
        {
            "URL": "https://www.psycharchives.org/en/item/088c79cb-237c-4545-a9e2-3616d6cc8453",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Force",
                    "given": "Preregistration Task"
                }
            ],
            "id": "preregistration_task_force_preregistration_2021",
            "issued": {
                "date-parts": [
                    [
                        2021,
                        2
                    ]
                ]
            },
            "title": "Preregistration Standards for Psychology - the Psychological Research Preregistration-Quantitative (aka PRP-QUANT) Template",
            "type": "article-journal"
        },
        {
            "DOI": "10.31235/osf.io/pz9jr",
            "URL": "https://osf.io/preprints/socarxiv/pz9jr/",
            "abstract": "Preregistrations – records made a priori about study designs and analysis plans and placed in open repositories – are thought to strengthen the credibility and transparency of research. Different authors have put forth arguments in favor of introducing this practice in qualitative research and made suggestions for what to include in a qualitative preregistration form. The goal of this study was to gauge and understand what parts of preregistration templates qualitative researchers would find helpful and informative. We used an online Delphi study design consisting of two rounds with feedback reports in between. In total, 48 researchers participated (response rate: 16%). In round 1, panelists considered 14 proposed items relevant to include in the preregistration form, but 2 items had relevance scores below our predefined criterion (68%) with mixed argument and were put forth again. We combined items where possible, leading to 11 revised items. In round 2, panelists agreed on including the two remaining items. Panelists also converged on suggested terminology and elaborations, except for two terms for which they provided clear arguments. The result is an agreement-based form for the preregistration of qualitative studies that consists of 13 items. The form will be made available as a registration option on Open Science Framework (osf.io). We believe it is important to assure that the strength of qualitative research, which is its flexibility to adapt, adjust and respond, is not lost in preregistration. The preregistration should provide a systematic starting point.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Haven",
                    "given": "Tamarinde"
                },
                {
                    "family": "Errington",
                    "given": "Timothy M."
                },
                {
                    "family": "Gleditsch",
                    "given": "Kristian"
                },
                {
                    "dropping-particle": "van",
                    "family": "Grootel",
                    "given": "Leonie"
                },
                {
                    "family": "Jacobs",
                    "given": "Alan M."
                },
                {
                    "family": "Kern",
                    "given": "Florian"
                },
                {
                    "family": "Piñeiro",
                    "given": "Rafael"
                },
                {
                    "family": "Rosenblatt",
                    "given": "Fernando"
                },
                {
                    "family": "Mokkink",
                    "given": "Lidwine"
                }
            ],
            "id": "haven_preregistering_2020",
            "issued": {
                "date-parts": [
                    [
                        2020,
                        7
                    ]
                ]
            },
            "keyword": "Psychology, Methodology, qualitative research, open science, Social and Behavioral Sciences, preregistration, Other Psychology, Other Social and Behavioral Sciences, qualitative inquiry, registries, Sociology, transparency",
            "note": "type: article",
            "publisher": "SocArXiv",
            "title": "Preregistering Qualitative Research: A Delphi Study",
            "title-short": "Preregistering Qualitative Research",
            "type": "report"
        },
        {
            "URL": "https://papers.ssrn.com/abstract=2209188",
            "abstract": "This essay examines the economics of patronage in the production of knowledge and its influence upon the historical formation of key elements in the ethos and organizational structure of publicly funded ’open science.’ The emergence during the late sixteenth and early seventeenth centuries of the idea and practice of ’open science’ was a distinctive and vital organizational aspect of the Scientific Revolution. It represented a break from the previously dominant ethos of secrecy in the pursuit of Nature’s Secrets, to a new set of norms, incentives, and organizational structures that reinforced scientific researchers’ commitments to rapid disclosure of new knowledge. The rise of ’cooperative rivalries’ in the revelation of new knowledge, is seen as a functional response to heightened asymmetric information problems posed for the Renaissance system of court-patronage of the arts and sciences; pre-existing informational asymmetries had been exacerbated by the claims of mathematicians and the increasing practical reliance upon new mathematical techniques in a variety of ’contexts of application.’ Reputational competition among Europe’s noble patrons motivated much of their efforts to attract to their courts the most prestigious natural philosophers, was no less crucial in the workings of that system than was the concern among their would-be clients to raise their peer-based reputational status. In late Renaissance Europe, the feudal legacy of fragmented political authority had resulted in relations between noble patrons and their savantclients that resembled the situation modern economists describe as “common agency contracting in substitutes” - competition among incompletely informed principals for the dedicated services of multiple agents. These conditions tended to result in contract terms (especially with regard to autonomy and financial support) that left agent client members of the nascent scientific communities better positioned to retain larger information rents on their specialized knowledge. This encouraged entry into their emerging disciplines, and enabled them collectively to develop a stronger degree of professional autonomy for their programs of inquiry within the increasingly specialized and formal scientific academies (such the Académie royale des Sciences and the Royal Society) that had attracted the patronage of rival absolutist States of Western Europe during the latter part of the seventeenth century. The institutionalization of ’open science’ that took place within those settings is shown to have continuities with the use by scientists of the earlier humanist academies, and with the logic of regal patronage, rather than being driven by the material requirements of new observational and experimental techniques.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "David",
                    "given": "Paul A."
                }
            ],
            "genre": "{SSRN} {Scholarly} {Paper}",
            "id": "david_historical_2013",
            "issued": {
                "date-parts": [
                    [
                        2013,
                        1
                    ]
                ]
            },
            "keyword": "open science, asymmetric information, common agency contracting, evolution of institutions, invisible colleges, new economics of science, patronage, principal-agent problems, scientific academies, social networks",
            "number": "2209188",
            "publisher": "Social Science Research Network",
            "publisher-place": "Rochester, NY",
            "title": "The Historical Origins of ’Open Science’: An Essay on Patronage, Reputation and Common Agency Contracting in the Scientific Revolution",
            "title-short": "The Historical Origins of ’Open Science’",
            "type": "report"
        },
        {
            "ISSN": "0162-2439",
            "URL": "http://www.jstor.org/stable/689511",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Chubin",
                    "given": "Daryl E."
                }
            ],
            "container-title": "Science, Technology, & Human Values",
            "id": "chubin_open_1985",
            "issue": "2",
            "issued": {
                "date-parts": [
                    [
                        1985
                    ]
                ]
            },
            "note": "Publisher: Sage Publications, Inc.",
            "page": "73-81",
            "title": "Open Science and Closed Science: Tradeoffs in a Democracy",
            "title-short": "Open Science and Closed Science",
            "type": "article-journal",
            "volume": "10"
        },
        {
            "URL": "https://en.wikipedia.org/w/index.php?title=Open_science&oldid=1087027147",
            "abstract": "Open science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional. Open science is transparent and accessible knowledge that is shared and developed through collaborative networks. It encompasses practices such as publishing open research, campaigning for open access, encouraging scientists to practice open-notebook science, broader dissemination and engagement in science and generally making it easier to publish, access and communicate scientific knowledge. Usage of the term varies substantially across disciplines, with a notable prevalence in the STEM disciplines. Open research is often used quasi-synonymously to address the gap that the denotion of \"science\" might have regarding an inclusion of the Arts, Humanities and Social Sciences. The primary focus connecting all disciplines is the widespread uptake of new technologies and tools, and the underlying ecology of the production, dissemination and reception of knowledge from a research-based point-of-view.As Tennant et al. (2020) note, the term open science \"implicitly seems only to regard “scientific” disciplines, whereas open scholarship can be considered to include research from the Arts and Humanities, as well as the different roles and practices that researchers perform as educators and communicators, and an underlying open philosophy of sharing knowledge beyond research communities.\"Open science can be seen as a continuation of, rather than a revolution in, practices begun in the 17th century with the advent of the academic journal, when the societal demand for access to scientific knowledge reached a point at which it became necessary for groups of scientists to share resources with each other. In modern times there is debate about the extent to which scientific information should be shared. The conflict that led to the Open Science movement is between the desire of scientists to have access to shared resources versus the desire of individual entities to profit when other entities partake of their resources. Additionally, the status of open access and resources that are available for its promotion are likely to differ from one field of academic inquiry to another.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        16
                    ]
                ]
            },
            "container-title": "Wikipedia",
            "id": "noauthor_open_2022",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        5
                    ]
                ]
            },
            "note": "Page Version ID: 1087027147",
            "title": "Open science",
            "type": ""
        },
        {
            "author": [
                {
                    "family": "Merton",
                    "given": "R. K."
                }
            ],
            "container-title": "Journal of Legal and Political Sociology",
            "id": "merton_science_1942",
            "issued": {
                "date-parts": [
                    [
                        1942
                    ]
                ]
            },
            "page": "115-126",
            "title": "Science and technology in a democratic order",
            "type": "article-journal",
            "volume": "1"
        },
        {
            "ISBN": "978-1-4786-3425-6",
            "abstract": "As the new subtitle indicates, the book emphasizes the logic of methods to provide the student a solid basis for future methodology changes, enhancing the integrated approach of the previous edition. Among the author’s many goals are for users to: understand research’s contribution to knowledge building as a social process through which findings become accepted as knowledge; acquire the background to read, analyze, and understand research using a variety of approaches as well as the hallmarks necessary to evaluate each method; and realize that the responsibility for ethical research is fundamentally theirs and that value choices are involved, beginning with the choice of research problem. Updates to the new edition include an extensive example of the use of the computer in the literature search and a new chapter on the reflective researcher. The expanded treatment of qualitative research includes the pros and cons of using software in qualitative analysis. Conceptual analysis, an important concept missing from the second edition, has returned by request because of its widely employed logic in both qualitative and quantitative methods. The author has acknowledged the troublesome nature of the concepts internal validity and external validity and has more clearly defined these important foundational concepts as Internal Integrity and External Generality. Useful tools to facilitate learning include additional reading lists, important terms and concepts, tips on effective research methods and hallmarks of methods, application problems and exercises, a glossary, and an appendix on writing a research proposal. A Web site is available with auxiliary learning enhancements and updates.",
            "author": [
                {
                    "family": "Krathwohl",
                    "given": "David R."
                }
            ],
            "id": "krathwohl_methods_2009",
            "issued": {
                "date-parts": [
                    [
                        2009,
                        2
                    ]
                ]
            },
            "keyword": "Education / Research",
            "note": "Google-Books-ID: 0lMbDQAAQBAJ",
            "publisher": "Waveland Press",
            "title": "Methods of Educational and Social Science Research: The Logic of Methods, Third Edition",
            "title-short": "Methods of Educational and Social Science Research",
            "type": "book"
        },
        {
            "ISBN": "978-0-15-100877-3",
            "author": [
                {
                    "family": "Judson",
                    "given": "Horace Freeland"
                }
            ],
            "edition": "First edition",
            "id": "judson_great_2004",
            "issued": {
                "date-parts": [
                    [
                        2004,
                        10
                    ]
                ]
            },
            "publisher": "Harcourt",
            "publisher-place": "Orlando",
            "title": "The Great Betrayal: Fraud in Science",
            "title-short": "The Great Betrayal",
            "type": "book"
        },
        {
            "DOI": "10.1126/science.aac4716",
            "ISSN": "00368075",
            "abstract": "An abstract is presented for an article in the current issue which reports a study involving replications of numerous published psychology studies, with the aim of determining the extent to which those studies’ results were reproducible.",
            "author": [
                {
                    "literal": "Open Science Collaboration"
                }
            ],
            "container-title": "Science",
            "id": "open_science_collaboration_estimating_2015",
            "issue": "6251",
            "issued": {
                "date-parts": [
                    [
                        2015,
                        8
                    ]
                ]
            },
            "keyword": "PSYCHOLOGICAL research, REPRODUCIBLE research",
            "page": "943-943",
            "title": "Estimating the reproducibility of psychological science",
            "type": "article-journal",
            "volume": "349"
        },
        {
            "DOI": "10.1101/066803",
            "URL": "https://www.biorxiv.org/content/10.1101/066803v1",
            "abstract": "Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        21
                    ]
                ]
            },
            "author": [
                {
                    "family": "Patil",
                    "given": "Prasad"
                },
                {
                    "family": "Peng",
                    "given": "Roger D."
                },
                {
                    "family": "Leek",
                    "given": "Jeffrey T."
                }
            ],
            "id": "patil_statistical_2016",
            "issued": {
                "date-parts": [
                    [
                        2016,
                        7
                    ]
                ]
            },
            "note": "Section: New Results Type: article",
            "page": "066803",
            "publisher": "bioRxiv",
            "title": "A statistical definition for reproducibility and replicability",
            "type": "report"
        },
        {
            "DOI": "10.1371/journal.pone.0137864",
            "ISSN": "1932-6203",
            "URL": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0137864",
            "abstract": "Background The efficacy of antidepressant medication has been shown empirically to be overestimated due to publication bias, but this has only been inferred statistically with regard to psychological treatment for depression. We assessed directly the extent of study publication bias in trials examining the efficacy of psychological treatment for depression. Methods and Findings We identified US National Institutes of Health grants awarded to fund randomized clinical trials comparing psychological treatment to control conditions or other treatments in patients diagnosed with major depressive disorder for the period 1972–2008, and we determined whether those grants led to publications. For studies that were not published, data were requested from investigators and included in the meta-analyses. Thirteen (23.6%) of the 55 funded grants that began trials did not result in publications, and two others never started. Among comparisons to control conditions, adding unpublished studies (Hedges’ g = 0.20; CI95% -0.11~0.51; k = 6) to published studies (g = 0.52; 0.37~0.68; k = 20) reduced the psychotherapy effect size point estimate (g = 0.39; 0.08~0.70) by 25%. Moreover, these findings may overestimate the \"true\" effect of psychological treatment for depression as outcome reporting bias could not be examined quantitatively. Conclusion The efficacy of psychological interventions for depression has been overestimated in the published literature, just as it has been for pharmacotherapy. Both are efficacious but not to the extent that the published literature would suggest. Funding agencies and journals should archive both original protocols and raw data from treatment trials to allow the detection and correction of outcome reporting bias. Clinicians, guidelines developers, and decision makers should be aware that the published literature overestimates the effects of the predominant treatments for depression.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        23
                    ]
                ]
            },
            "author": [
                {
                    "family": "Driessen",
                    "given": "Ellen"
                },
                {
                    "family": "Hollon",
                    "given": "Steven D."
                },
                {
                    "family": "Bockting",
                    "given": "Claudi L. H."
                },
                {
                    "family": "Cuijpers",
                    "given": "Pim"
                },
                {
                    "family": "Turner",
                    "given": "Erick H."
                }
            ],
            "container-title": "PLOS ONE",
            "id": "driessen_does_2015",
            "issue": "9",
            "issued": {
                "date-parts": [
                    [
                        2015,
                        9
                    ]
                ]
            },
            "keyword": "Psychotherapy, Depression, Antidepressant drug therapy, Antidepressants, Clinical psychology, Drug therapy, Mental health therapies, Publication ethics",
            "note": "Publisher: Public Library of Science",
            "page": "e0137864",
            "title": "Does Publication Bias Inflate the Apparent Efficacy of Psychological Treatment for Major Depressive Disorder? A Systematic Review and Meta-Analysis of US National Institutes of Health-Funded Trials",
            "title-short": "Does Publication Bias Inflate the Apparent Efficacy of Psychological Treatment for Major Depressive Disorder?",
            "type": "article-journal",
            "volume": "10"
        },
        {
            "DOI": "10.1037/0033-2909.86.3.638",
            "ISSN": "0033-2909",
            "URL": "https://ezproxy.spu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=1979-27602-001&site=ehost-live",
            "abstract": "For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the ’file drawer problem’ is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        5,
                        23
                    ]
                ]
            },
            "author": [
                {
                    "family": "Rosenthal",
                    "given": "Robert"
                }
            ],
            "container-title": "Psychological Bulletin",
            "id": "rosenthal_file_1979",
            "issue": "3",
            "issued": {
                "date-parts": [
                    [
                        1979,
                        5
                    ]
                ]
            },
            "keyword": "Statistical Probability, Experimentation, Type I Errors, Scientific Communication, Statistical Tests, tolerance for null results bias in publication of research with Type I errors & estimation of unpublished domain",
            "note": "Publisher: American Psychological Association",
            "page": "638-641",
            "title": "The file drawer problem and tolerance for null results",
            "type": "article-journal",
            "volume": "86"
        },
        {
            "DOI": "10.1038/s41562-021-01193-7",
            "ISSN": "2397-3374",
            "URL": "https://www.nature.com/articles/s41562-021-01193-7",
            "abstract": "Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        12
                    ]
                ]
            },
            "author": [
                {
                    "family": "Chambers",
                    "given": "Christopher D."
                },
                {
                    "family": "Tzavella",
                    "given": "Loukia"
                }
            ],
            "container-title": "Nature Human Behaviour",
            "id": "chambers_past_2022",
            "issue": "1",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        1
                    ]
                ]
            },
            "keyword": "Culture, Publishing",
            "note": "Number: 1 Publisher: Nature Publishing Group",
            "page": "29-42",
            "title": "The past, present and future of Registered Reports",
            "type": "article-journal",
            "volume": "6"
        },
        {
            "URL": "https://www.apa.org/ethics/code",
            "abstract": "The American Psychological Association’s Ethical Principles of Psychologists and Code of Conduct provides guidance for psychologists in professional, scientific and educational roles. The Ethics Code also outlines standards of professional conduct for APA members and student affiliates.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        15
                    ]
                ]
            },
            "author": [
                {
                    "literal": "American Psychological Association"
                }
            ],
            "id": "american_psychological_association_ethical_2017",
            "issued": {
                "date-parts": [
                    [
                        2017
                    ]
                ]
            },
            "title": "Ethical principles of psychologists and code of conduct",
            "type": ""
        },
        {
            "URL": "https://schoemanna.shinyapps.io/mc_power_med/",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Shoemann",
                    "given": "Alex M."
                },
                {
                    "family": "Boulton",
                    "given": "A. J."
                },
                {
                    "family": "Short",
                    "given": "S. D."
                }
            ],
            "container-title": "Social Psychological and Personality Science",
            "id": "shoemann_determining_2017",
            "issued": {
                "date-parts": [
                    [
                        2017
                    ]
                ]
            },
            "page": "379-386",
            "title": "Determining power and sample size for simple and complex mediation models",
            "type": "article-journal",
            "volume": "8"
        },
        {
            "URL": "https://www.jamovi.org",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "dropping-particle": "jamovi",
                    "family": "project",
                    "given": "The"
                }
            ],
            "id": "the_jamovi_project_about_2021",
            "issued": {
                "date-parts": [
                    [
                        2021
                    ]
                ]
            },
            "title": "About - jamovi",
            "type": ""
        },
        {
            "URL": "https://jasp-stats.org/faq/",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "family": "Team",
                    "given": "JASP"
                }
            ],
            "id": "jasp_team_jasp_2022",
            "issued": {
                "date-parts": [
                    [
                        2022
                    ]
                ]
            },
            "title": "JASP (Version 0.16.3)",
            "type": ""
        },
        {
            "URL": "https://www.zotero.org/",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        16
                    ]
                ]
            },
            "author": [
                {
                    "dropping-particle": "for",
                    "family": "Digital Scholarship",
                    "given": "Corporation"
                }
            ],
            "id": "corporation_for_digital_scholarship_zotero_2022",
            "issued": {
                "date-parts": [
                    [
                        2022
                    ]
                ]
            },
            "title": "Zotero  Your personal research assistant",
            "type": "paper-conference"
        },
        {
            "URL": "https://github.com",
            "abstract": "GitHub is where people build software. More than 83 million people use GitHub to discover, fork, and contribute to over 200 million projects.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        16
                    ]
                ]
            },
            "id": "noauthor_github_2022",
            "issued": {
                "date-parts": [
                    [
                        2022
                    ]
                ]
            },
            "title": "GitHub",
            "type": ""
        },
        {
            "URL": "https://osf.io/",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "family": "OSF"
                }
            ],
            "id": "osf_open_2022",
            "issued": {
                "date-parts": [
                    [
                        2022
                    ]
                ]
            },
            "title": "Open Science Framework",
            "type": ""
        },
        {
            "DOI": "10.1037/0003-066X.38.4.414",
            "ISSN": "0003-066X",
            "URL": "https://ezproxy.spu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&AuthType=ip&db=psyh&AN=1984-02320-001&site=ehost-live",
            "abstract": "Discusses issues related to psychologists’ willingness to share data from research projects supported by tax dollars. Attention is focused on the legal, ethical, and pragmatic reasons given for refusing to share the raw data of one’s federally sponsored research with other psychologists. A proposal to mandate data sharing is proposed, with discussion of the technical and ethical costs and benefits it would likely entail. Such a proposal would be congruent with mission statements contained in grant announcements, the public’s right to know, and a change in attitudes in the field of professional psychology from a Cartesian to a Baconian orientation. (37 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "family": "Ceci",
                    "given": "Stephen J."
                },
                {
                    "family": "Walker",
                    "given": "Elaine"
                }
            ],
            "container-title": "American Psychologist",
            "id": "ceci_private_1983",
            "issue": "4",
            "issued": {
                "date-parts": [
                    [
                        1983,
                        4
                    ]
                ]
            },
            "keyword": "Statistical Data, Experimentation, Data Collection, psychologists, Professional Ethics, Experimental Psychologists, legal & ethical & pragmatic reasons for unwillingness to share data from research projects supported by tax dollars, Legal Processes",
            "note": "Publisher: American Psychological Association",
            "page": "414-423",
            "title": "Private archives and public needs",
            "type": "article-journal",
            "volume": "38"
        },
        {
            "URL": "https://www.apa.org/ethics/code",
            "abstract": "The American Psychological Association’s Ethical Principles of Psychologists and Code of Conduct provides guidance for psychologists in professional, scientific and educational roles. The Ethics Code also outlines standards of professional conduct for APA members and student affiliates.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        17
                    ]
                ]
            },
            "author": [
                {
                    "dropping-particle": "date",
                    "family": "June 1",
                    "given": "Effective"
                },
                {
                    "dropping-particle": "with amendments effective",
                    "family": "June 1",
                    "given": "2003"
                },
                {
                    "literal": "2010"
                },
                {
                    "family": "1",
                    "given": "January"
                },
                {
                    "dropping-particle": "rights",
                    "family": "reserved",
                    "given": "2017 Copyright © 2017 American Psychological Association All"
                }
            ],
            "container-title": "https://www.apa.org",
            "id": "june_1_ethical_nodate",
            "title": "Ethical principles of psychologists and code of conduct",
            "type": ""
        },
        {
            "URL": "https://en.wikipedia.org/w/index.php?title=R_(programming_language)&oldid=1098779048",
            "abstract": "R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software. Users have created packages to augment the functions of the R language. According to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages used in data mining. As of March 2022, R ranks 11th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020.The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface. Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        21
                    ]
                ]
            },
            "container-title": "Wikipedia",
            "id": "noauthor_r_2022",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        7
                    ]
                ]
            },
            "note": "Page Version ID: 1098779048",
            "title": "R (programming language)",
            "type": ""
        },
        {
            "URL": "https://en.wikipedia.org/w/index.php?title=RStudio&oldid=1099226518",
            "abstract": "RStudio is an integrated development environment for R, a programming language for statistical computing and graphics. It is available in two formats: RStudio Desktop is a regular desktop application while RStudio Server runs on a remote server and allows accessing RStudio using a web browser.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        21
                    ]
                ]
            },
            "container-title": "Wikipedia",
            "id": "noauthor_rstudio_2022",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        7
                    ]
                ]
            },
            "note": "Page Version ID: 1099226518",
            "title": "RStudio",
            "type": ""
        },
        {
            "URL": "https://en.wikipedia.org/w/index.php?title=GitHub&oldid=1097996266",
            "abstract": "GitHub, Inc. is a provider of Internet hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. It provides access control and several collaboration features such as bug tracking, feature requests, task management, continuous integration, and wikis for every project. Headquartered in California, it has been a subsidiary of Microsoft since 2018.It is commonly used to host open-source projects. As of June 2022, GitHub reports having over 83 million developers and more than 200 million repositories (including at least 28 million public repositories). It is the largest source code host as of November 2021.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        21
                    ]
                ]
            },
            "container-title": "Wikipedia",
            "id": "noauthor_github_2022-1",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        7
                    ]
                ]
            },
            "note": "Page Version ID: 1097996266",
            "title": "GitHub",
            "type": ""
        },
        {
            "URL": "https://en.wikipedia.org/w/index.php?title=Mendeley&oldid=1095492319",
            "abstract": "Mendeley is a software company based in London, UK, which provides products and services for academic researchers. It is most known for its reference manager which is used to manage and share research papers and generate bibliographies for scholarly articles.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        21
                    ]
                ]
            },
            "container-title": "Wikipedia",
            "id": "noauthor_mendeley_2022",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        6
                    ]
                ]
            },
            "note": "Page Version ID: 1095492319",
            "title": "Mendeley",
            "type": ""
        },
        {
            "DOI": "http://dx.doi.org/10.18590/mjm.2017.vol3.iss3.1",
            "ISSN": "2379-9536",
            "URL": "https://mds.marshall.edu/mjm/vol3/iss3/1",
            "author": [
                {
                    "family": "Shah",
                    "given": "Darshana"
                }
            ],
            "container-title": "Marshall Journal of Medicine",
            "id": "shah_open_2017",
            "issue": "3",
            "issued": {
                "date-parts": [
                    [
                        2017,
                        7
                    ]
                ]
            },
            "page": "1",
            "title": "Open Access Publishing: Pros, Cons, and Current Threats",
            "title-short": "Open Access Publishing",
            "type": "article-journal",
            "volume": "3"
        },
        {
            "DOI": "10.1080/21642850.2021.2012474",
            "ISSN": "null",
            "URL": "https://doi.org/10.1080/21642850.2021.2012474",
            "abstract": "Background Identification of widespread biases present in reported research findings in many scientific disciplines, including psychology, such as failures to replicate and the likely extensive application of questionable research practices, has raised serious concerns over the reliability and trustworthiness of scientific research. This has led to the development of, and advocacy for, “open science” practices, including data, materials, analysis, and output sharing, pre-registration of study predictions and analysis plans, and increased access to published research findings. Implementation of such practices has been enthusiastic in some quarters, but literacy in, and adoption of, these practices has lagged behind among many researchers in the scientific community.Advances In the current article I propose that researchers adopt an open science “mindset”, a comprehensive approach to open science predicated on researchers’ operating under the basic assumption that, wherever possible, open science practices will be a central component of all steps of their research projects. The primary, defining feature of the mindset is a commitment to open science principles in all research projects from inception to dissemination. Other features of the mindset include the assumption that all components of research projects (e.g. pre-registered hypotheses, protocols, materials, analysis plans, data, and output) will be accessible broadly; pro-active selection of open fora to disseminate research components and findings; open and transparent dissemination of reports of the research findings in advance of, and after, formal publication; and active promotion of open science practices through education, modeling, and advocacy.Conclusion The open science mindset is a “farm to fork” approach to open science aimed at promoting comprehensive quality in application of open science, and widening participation in open science practices so that they become the norm in research in health psychology and behavioral medicine going forward.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        22
                    ]
                ]
            },
            "author": [
                {
                    "family": "Hagger",
                    "given": "Martin S."
                }
            ],
            "container-title": "Health Psychology and Behavioral Medicine",
            "id": "hagger_developing_2022",
            "issue": "1",
            "issued": {
                "date-parts": [
                    [
                        2022,
                        12
                    ]
                ]
            },
            "keyword": "data sharing, pre-registration, Open science, replication crisis, research transparency",
            "note": "Publisher: Routledge _eprint: https://doi.org/10.1080/21642850.2021.2012474",
            "page": "1-21",
            "title": "Developing an open science “mindset”",
            "type": "article-journal",
            "volume": "10"
        },
        {
            "DOI": "10.1038/489179a",
            "ISSN": "1476-4687",
            "URL": "https://www.nature.com/articles/489179a",
            "abstract": "Journals that exploit the author-pays model damage scholarly publishing and promote unethical behaviour by scientists, argues Jeffrey Beall.",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        22
                    ]
                ]
            },
            "author": [
                {
                    "family": "Beall",
                    "given": "Jeffrey"
                }
            ],
            "container-title": "Nature",
            "id": "beall_predatory_2012",
            "issue": "7415",
            "issued": {
                "date-parts": [
                    [
                        2012,
                        9
                    ]
                ]
            },
            "keyword": "Publishing, Ethics, Peer review, Research management",
            "note": "Number: 7415 Publisher: Nature Publishing Group",
            "page": "179-179",
            "title": "Predatory publishers are corrupting open access",
            "type": "article-journal",
            "volume": "489"
        },
        {
            "DOI": "10.5811/westjem.2016.6.30836",
            "ISSN": "1936-900X",
            "PMCID": "PMC5017831",
            "PMID": "27625711",
            "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017831/",
            "accessed": {
                "date-parts": [
                    [
                        2022,
                        7,
                        22
                    ]
                ]
            },
            "author": [
                {
                    "family": "Fortney",
                    "given": "Katie"
                },
                {
                    "family": "Murphy",
                    "given": "Linda Suk-Ling"
                }
            ],
            "container-title": "Western Journal of Emergency Medicine",
            "id": "fortney_getting_2016",
            "issue": "5",
            "issued": {
                "date-parts": [
                    [
                        2016,
                        9
                    ]
                ]
            },
            "page": "508-510",
            "title": "Getting Found: Indexing and the Independent Open Access Journal",
            "title-short": "Getting Found",
            "type": "article-journal",
            "volume": "17"
        },
        {
            "DOI": "10.1037/amp0000242",
            "ISSN": "1935-990X",
            "PMCID": "PMC5920518",
            "PMID": "29481105",
            "abstract": "Routine data sharing, defined here as the publication of the primary data and any supporting materials required to interpret the data acquired as part of a research study, is still in its infancy in psychology, as in many domains. Nevertheless, with increased scrutiny on reproducibility and more funder mandates requiring sharing of data, the issues surrounding data sharing are moving beyond whether data sharing is a benefit or a bane to science, to what data should be shared and how. Here, we present an overview of these issues, specifically focusing on the sharing of so-called \"long tail\" data, that is, data generated by individual laboratories as part of largely hypothesis-driven research. We draw on experiences in other domains to discuss attitudes toward data sharing, cost-benefits, best practices and infrastructure. We argue that the publishing of data sets is an integral component of 21st-century scholarship. Moreover, although not all issues around how and what to share have been resolved, a consensus on principles and best practices for effective data sharing and the infrastructure for sharing many types of data are largely in place. (PsycINFO Database Record",
            "author": [
                {
                    "family": "Martone",
                    "given": "Maryann E."
                },
                {
                    "family": "Garcia-Castro",
                    "given": "Alexander"
                },
                {
                    "family": "VandenBos",
                    "given": "Gary R."
                }
            ],
            "container-title": "The American Psychologist",
            "id": "martone_data_2018",
            "issue": "2",
            "issued": {
                "date-parts": [
                    [
                        2018,
                        3
                    ]
                ]
            },
            "keyword": "Humans, Information Dissemination, Psychology, Reproducibility of Results, Research",
            "page": "111-125",
            "title": "Data sharing in psychology",
            "type": "article-journal",
            "volume": "73"
        }
    ],
    "project_biblios": [
        "TRM_refs.bib"
    ]
}